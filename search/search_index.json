{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"adopting/adopting/","text":"Adopting the Cloud-Native Toolkit \u00b6 This section assumes you have completed the fast-start learning, or equivalent, and have some knowledge of the Cloud-Native Toolkit. Now you have some hands on experience of using the toolkit, you want to start using the Toolkit on your own project. To do this you need to make some decisions on how you want to integrate the Cloud-Native Toolkit into your development processes. This section provides guidance on: how to setup the Toolkit within your development environment how to customize the Toolkit to meet your development team needs how to add or replace components of the Toolkit with alternate solutions, such as selecting a different container registry solution how to use the Toolkit for the included development use cases (JavaScript, TypeScript, Spring Boot, etc...) how to support development use cases not provided with the Toolkit best practices for using the Toolkit within a Cloud-Native development process","title":"Adopting the Toolkit"},{"location":"adopting/adopting/#adopting-the-cloud-native-toolkit","text":"This section assumes you have completed the fast-start learning, or equivalent, and have some knowledge of the Cloud-Native Toolkit. Now you have some hands on experience of using the toolkit, you want to start using the Toolkit on your own project. To do this you need to make some decisions on how you want to integrate the Cloud-Native Toolkit into your development processes. This section provides guidance on: how to setup the Toolkit within your development environment how to customize the Toolkit to meet your development team needs how to add or replace components of the Toolkit with alternate solutions, such as selecting a different container registry solution how to use the Toolkit for the included development use cases (JavaScript, TypeScript, Spring Boot, etc...) how to support development use cases not provided with the Toolkit best practices for using the Toolkit within a Cloud-Native development process","title":"Adopting the Cloud-Native Toolkit"},{"location":"adopting/admin/argo-cd-setup/","text":"ArgoCD Setup \u00b6 Complete the steps for setting up the ArgoCD tool to support integration with Artifactory as a Helm repository Overview \u00b6 ArgoCD is the environment's continuous delivery tool . Before you can use it as part of the environment, it must first be configured. Prerequisites \u00b6 Before setting up ArgoCD, set up Artifactory so that ArgoCD can use it as a Helm repository. Configuration \u00b6 Configure ArgoCD to use Artifactory as a Helm repository. Get the Helm repository location \u00b6 The Helm repository is in Artifactory. ArgoCD will need the URL to that repository. In the Artifactory console, select the Home page In the Set Me Up section, click on the generic-local repository In the Set Me Up dialog for the generic-local repository, copy the Deploy URL Register the helm repository with ArgoCD \u00b6 Log in to ArgoCD Open the Settings dialog by clicking the Manage your repositories, projects, settings button on the left menu From the Settings page, select Repositories Press the Connect repo using https button at the top of the page In the New Repository dialog, select helm for the repository type, provide a name to identify the repository, and enter the Artifactory deploy url for the repository url. Press Connect to create the repository. The repository should now appear and the list and can be used for application deployments Conclusion \u00b6 ArgoCD now has the configuration in the environment that it needs to access the helm chart repository in Artifactory.","title":"Argo CD setup"},{"location":"adopting/admin/argo-cd-setup/#argocd-setup","text":"Complete the steps for setting up the ArgoCD tool to support integration with Artifactory as a Helm repository","title":"ArgoCD Setup"},{"location":"adopting/admin/argo-cd-setup/#overview","text":"ArgoCD is the environment's continuous delivery tool . Before you can use it as part of the environment, it must first be configured.","title":"Overview"},{"location":"adopting/admin/argo-cd-setup/#prerequisites","text":"Before setting up ArgoCD, set up Artifactory so that ArgoCD can use it as a Helm repository.","title":"Prerequisites"},{"location":"adopting/admin/argo-cd-setup/#configuration","text":"Configure ArgoCD to use Artifactory as a Helm repository.","title":"Configuration"},{"location":"adopting/admin/argo-cd-setup/#get-the-helm-repository-location","text":"The Helm repository is in Artifactory. ArgoCD will need the URL to that repository. In the Artifactory console, select the Home page In the Set Me Up section, click on the generic-local repository In the Set Me Up dialog for the generic-local repository, copy the Deploy URL","title":"Get the Helm repository location"},{"location":"adopting/admin/argo-cd-setup/#register-the-helm-repository-with-argocd","text":"Log in to ArgoCD Open the Settings dialog by clicking the Manage your repositories, projects, settings button on the left menu From the Settings page, select Repositories Press the Connect repo using https button at the top of the page In the New Repository dialog, select helm for the repository type, provide a name to identify the repository, and enter the Artifactory deploy url for the repository url. Press Connect to create the repository. The repository should now appear and the list and can be used for application deployments","title":"Register the helm repository with ArgoCD"},{"location":"adopting/admin/argo-cd-setup/#conclusion","text":"ArgoCD now has the configuration in the environment that it needs to access the helm chart repository in Artifactory.","title":"Conclusion"},{"location":"adopting/admin/artifactory-setup/","text":"Artifactory Setup \u00b6 Complete the steps for setting up the Artifactory tool Note As of v1.9.0 of the Artifactory module, these steps have been automated and should not be required after a Toolkit install. The instructions are provided here to understand what steps are required to manually integrate the Artifactory instance into the pipelines. Overview \u00b6 Artifactory is the environments's artifact repository manager . Before you can use it as part of the continuous integration pipeline, it first must be configured as part of the environment. Configuration \u00b6 Set up Artifactory in the environment with a Helm repository. Also, set up the environment to access Artifactory. Warning The version of Artifactory that is installed into the cluster is not configured for high availability. It is limited to the storage size of your cluster. It is highly recommended you integrate your pipelines with an enterprise grade Helm repository or artifact management system. Set up Artifactory \u00b6 Create a repository in Artifactory for storing Helm charts. Open the Artifactory dashboard from the Developer Dashboard and login Log into Artifactory using the default userid and password, igc credentials can show these values. The first time you log in the Welcome Page will prompt you to configure Artifactory using the Onboarding Wizard. Press Get Started The first step of the setup wizard is to change the default password. It is recommend you use a password generated by a password manager. 1Password is the tool IBM recommends to its employees. Save the password somewhere safe or in your password manager; you will need to manually update the password in the secret that is stored in the tools namespace/project called artifactory-access . There are more detailed instructions after you have completed the setup wizard. The next step is to define the base url that will be used by the server. From the browser you are currently using copy the URL from the address bar. Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Since the open source release of Artifactory doesn't provide a package for Helm repositories, we will use a generic repository to hold the helm artifacts. Select Generic , then press Next The next page in the wizard is the Onboarding Complete page. Press Finish Allow Anonymous Access to Artifactory \u00b6 Open the Artifactory dashboard from the Developer Dashboard and login Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Check the Allow Anonymous Access check box and press Save Obtain the encrypted password \u00b6 To enable the CI pipelines ( Jenkins , Tekton , etc.) to store the Helm artifacts in the Artifactory repository, we need to obtain Artifactory's encrypted password and save it where the pipeline can access the password -- in the artifactory-access Kubernetes secret in the tools namespace in the Development Tools environment. In the Artifactory console, press the Welcome, admin menu button in the top right corner of the console and select Edit profile In the User Profile: admin page, enter you Artifactory password and press * Unlock Below, in the Authentication Settings section, is the Encrypted Password field. Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it Now we have obtained the Encrypt key and had kept a copy of the Password we changed on the initial setup wizard. We need to update the secrets that stores these inside the cluster. Update the secret - OpenShift 4 \u00b6 In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . (You do not need to base encode these values the OpenShift console does this for you.) Update the values that you retrieved for the encrypt key and updated the admin password Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value Update the ARTIFACTORY_PASSWORD value with thew new admin password Click Save in OpenShift console View the secret in the console and confirm that the visible value of ARTIFACTORY_ENCRYPT matches the encrypted password shown in the Artifactory console On the command line, run igc credentials and verify that the Artifactory details have been updated Update the secret - Kubernetes and OpenShift 3 \u00b6 To update the secret in Kubernetes or Openshift 3, navigate to the secret called artifactory-access in the tools namespace and update the following values. You will need encode these value in base64 key before editing the secret. Kubernetes secrets store data in base64 format. So to store Artifactory's encrypted password in a secret, it needs to be encoded into base64. Any base64 encoding tool will work. For example: Go to the Base64 Encode website Paste the encrypted password into the online tool and press the Encode button In the field below, copy the encoded value into you clipboard or store it somewhere safe Find the ARTIFACTORY_ENCRYPT value (under data ) and paste in the Artifactory base64 key into the field Find the ARTIFACTORY_PASSWORD value (under data ) and paste in the Rerun the Pipeline \u00b6 Previously, when you deployed you first app, the pipeline's Package Helm Chart stage didn't store the chart because the ARTIFACTORY_ENCRYPT property was not set. Now that it has been set, rerun your pipeline and check that the Helm chart for your app is stored correctly in Artifactory with matching semantic version information. In the Artifactory console, select the Artifacts page and expand the generic-local repository You will see a folder named after your resource group containing the tar file for a Helm chart. Conclusion \u00b6 Artifactory is now set up in your environment and CI pipelines can store Helm charts in Artifactory repository. Artifactory can now be used as a Helm repository for CD tools like ArgoCD and IBM Cloud Pak for Multi-Cloud Management.","title":"Artifactory setup"},{"location":"adopting/admin/artifactory-setup/#artifactory-setup","text":"Complete the steps for setting up the Artifactory tool Note As of v1.9.0 of the Artifactory module, these steps have been automated and should not be required after a Toolkit install. The instructions are provided here to understand what steps are required to manually integrate the Artifactory instance into the pipelines.","title":"Artifactory Setup"},{"location":"adopting/admin/artifactory-setup/#overview","text":"Artifactory is the environments's artifact repository manager . Before you can use it as part of the continuous integration pipeline, it first must be configured as part of the environment.","title":"Overview"},{"location":"adopting/admin/artifactory-setup/#configuration","text":"Set up Artifactory in the environment with a Helm repository. Also, set up the environment to access Artifactory. Warning The version of Artifactory that is installed into the cluster is not configured for high availability. It is limited to the storage size of your cluster. It is highly recommended you integrate your pipelines with an enterprise grade Helm repository or artifact management system.","title":"Configuration"},{"location":"adopting/admin/artifactory-setup/#set-up-artifactory","text":"Create a repository in Artifactory for storing Helm charts. Open the Artifactory dashboard from the Developer Dashboard and login Log into Artifactory using the default userid and password, igc credentials can show these values. The first time you log in the Welcome Page will prompt you to configure Artifactory using the Onboarding Wizard. Press Get Started The first step of the setup wizard is to change the default password. It is recommend you use a password generated by a password manager. 1Password is the tool IBM recommends to its employees. Save the password somewhere safe or in your password manager; you will need to manually update the password in the secret that is stored in the tools namespace/project called artifactory-access . There are more detailed instructions after you have completed the setup wizard. The next step is to define the base url that will be used by the server. From the browser you are currently using copy the URL from the address bar. Paste the URL into the Select Base URL form and remove any trailing context roots, similar to the one in this view. The next page in the wizard is the Configure a Proxy Server page. This is to setup a proxy for external resources. You can click Next to skip this step. The next page in the wizard is the Create Repositories page. Since the open source release of Artifactory doesn't provide a package for Helm repositories, we will use a generic repository to hold the helm artifacts. Select Generic , then press Next The next page in the wizard is the Onboarding Complete page. Press Finish","title":"Set up Artifactory"},{"location":"adopting/admin/artifactory-setup/#allow-anonymous-access-to-artifactory","text":"Open the Artifactory dashboard from the Developer Dashboard and login Click on the Settings tab on the left menu (the one with the gear icon), and then select Security Check the Allow Anonymous Access check box and press Save","title":"Allow Anonymous Access to Artifactory"},{"location":"adopting/admin/artifactory-setup/#obtain-the-encrypted-password","text":"To enable the CI pipelines ( Jenkins , Tekton , etc.) to store the Helm artifacts in the Artifactory repository, we need to obtain Artifactory's encrypted password and save it where the pipeline can access the password -- in the artifactory-access Kubernetes secret in the tools namespace in the Development Tools environment. In the Artifactory console, press the Welcome, admin menu button in the top right corner of the console and select Edit profile In the User Profile: admin page, enter you Artifactory password and press * Unlock Below, in the Authentication Settings section, is the Encrypted Password field. Press the Eye icon to view the encrypted password and press the Cut & Paste icon to copy it Now we have obtained the Encrypt key and had kept a copy of the Password we changed on the initial setup wizard. We need to update the secrets that stores these inside the cluster.","title":"Obtain the encrypted password"},{"location":"adopting/admin/artifactory-setup/#update-the-secret-openshift-4","text":"In the OpenShift 4 console, go to Administrator > Workloads > Secrets . At the top, select the tools project and filter for artifactory . Select Edit Secret on artifactory-access . (You do not need to base encode these values the OpenShift console does this for you.) Update the values that you retrieved for the encrypt key and updated the admin password Add a key/value for ARTIFACTORY_ENCRYPT and set the value to your encrypt key value Update the ARTIFACTORY_PASSWORD value with thew new admin password Click Save in OpenShift console View the secret in the console and confirm that the visible value of ARTIFACTORY_ENCRYPT matches the encrypted password shown in the Artifactory console On the command line, run igc credentials and verify that the Artifactory details have been updated","title":"Update the secret - OpenShift 4"},{"location":"adopting/admin/artifactory-setup/#update-the-secret-kubernetes-and-openshift-3","text":"To update the secret in Kubernetes or Openshift 3, navigate to the secret called artifactory-access in the tools namespace and update the following values. You will need encode these value in base64 key before editing the secret. Kubernetes secrets store data in base64 format. So to store Artifactory's encrypted password in a secret, it needs to be encoded into base64. Any base64 encoding tool will work. For example: Go to the Base64 Encode website Paste the encrypted password into the online tool and press the Encode button In the field below, copy the encoded value into you clipboard or store it somewhere safe Find the ARTIFACTORY_ENCRYPT value (under data ) and paste in the Artifactory base64 key into the field Find the ARTIFACTORY_PASSWORD value (under data ) and paste in the","title":"Update the secret - Kubernetes and OpenShift 3"},{"location":"adopting/admin/artifactory-setup/#rerun-the-pipeline","text":"Previously, when you deployed you first app, the pipeline's Package Helm Chart stage didn't store the chart because the ARTIFACTORY_ENCRYPT property was not set. Now that it has been set, rerun your pipeline and check that the Helm chart for your app is stored correctly in Artifactory with matching semantic version information. In the Artifactory console, select the Artifacts page and expand the generic-local repository You will see a folder named after your resource group containing the tar file for a Helm chart.","title":"Rerun the Pipeline"},{"location":"adopting/admin/artifactory-setup/#conclusion","text":"Artifactory is now set up in your environment and CI pipelines can store Helm charts in Artifactory repository. Artifactory can now be used as a Helm repository for CD tools like ArgoCD and IBM Cloud Pak for Multi-Cloud Management.","title":"Conclusion"},{"location":"adopting/admin/cluster-config/","text":"Cluster Configuration \u00b6 Within the created Kubernetes or OpenShift cluster there are a number secrets and config maps that are either provided by the IBM Cloud public environment and utilized by the Terraform scripts or are created during the Terraform provisioning process. Many of these secrets and config maps are also used by the pipeline scripts, so you may need to update them as you change passwords. Namespaces \u00b6 The Iteration Zero scripts create four namespaces that are used by the components deployed into the cluster: tools , dev , test , and staging . The actual names used for the namespaces are provided in Terraform variables with the defaults being those listed. The variables are then passed into the namespaces module in stage1-namespaces.tf . Provided resources \u00b6 TLS secret \u00b6 When the cluster is created, a secret containing the TLS certs for the ingress subdomain is provided in the default namespace. The the name of the secret is based off of the cluster name with some rules applied to limit the length and replace disallowed characters. Note To avoid issues with the naming conventions for the secret, the Iteration Zero scripts look for the secret that has a tls.key value: kubectl get secrets -o jsonpath='{.items[?(@.data.tls\\.key != \"\")].metadata.name}' . Ideally this would be identified using a label and a selector... During the Iteration Zero process, the TLS secret is copied into each of the four namespaces created and used by the Iteration Zero processes. Pull secrets \u00b6 Pull secrets for the IBM Cloud Image Registry are generated in the cluster as part of the Iteration Zero process to prepare the namespaces. The secrets are initially created in the default namespace with the following names: default-icr-io default-{region}-icr-io During the namespace preparation process, the pull secrets are copied into the different namespaces with default- prefix dropped. E.g. icr-io {region}-icr-io The pull secrets are also added to the default service account in each of the namespaces. As a result, it is not necessary to directly reference the pull secret as long as the pod runs under the default service account. Created resources \u00b6 The following resources are all created during the Iteration Zero provisioning process. These resources are used generally to expose the config of the installed tools but specifically used by the CI pipeline ( Jenkins , Tekton , etc.) to interact with the deployed tools. The resources are bound as optional environment variables in the containers used within the pipeline so if a particular tool has not been installed the associated environment variables won't be set. For example: envFrom : - configMapRef : name : pactbroker-config optional : true - configMapRef : name : sonarqube-config optional : true - secretRef : name : sonarqube-access optional : true IBM Cloud config \u00b6 The Iteration Zero script creates a config map and secret named ibmcloud-config and ibmcloud-access in the default namespace that contains the relevant configuration values for the cluster within the IBM Cloud account. The config map and secret are copied into each of the Iteration Zero namespaces as the namespaces are created. ibmcloud-config config map \u00b6 The following values are collected: CLUSTER_TYPE - the type of cluster (kubernetes or openshift) APIURL - the api url used to connect to the IBM Cloud environment SERVER_URL - the server url used to connect to the cluster, particularly for OpenShift RESOURCE_GROUP - the IBM Cloud resource group where the cluster has been installed REGISTRY_URL - the url to the image registry REGISTRY_NAMESPACE - the namespace within the image registry where images will be stored REGION - the IBM Cloud region where where the cluster has been installed CLUSTER_NAME - the name of the cluster INGRESS_SUBDOMAIN - the subdomain for the cluster to use in building ingress urls TLS_SECRET_NAME - the name of the secret that contains the TLS certificate information ibmcloud-access secret \u00b6 The following values are collected: APIKEY - the IBM Cloud apikey used to access the environment ArgoCD config \u00b6 argocd-config config map \u00b6 ARGOCD_URL - the url of the ArgoCD ingress argocd-access secret \u00b6 ARGOCD_PASSWORD - the password for the argocd user ARGOCD_USER - the user id of the argocd user Artifactory config \u00b6 artifactory-config config map \u00b6 ARTIFACTORY_URL - the url for the Artifactory ingress artifactory-access config map \u00b6 ARTIFACTORY_USER - the user name of the admin user ARTIFACTORY_PASSWORD - the password for the admin user ARTIFACTORY_ENCRPT - the encrypted password for the admin user. This value is initially blank and must be updated after the value is generated in the UI ARTIFACTORY_ADMIN_ACCESS_USER - the admin access user ARTIFACTORY_ADMIN_ACCESS_PASSWORD - the admin access password Jenkins config \u00b6 jenkins-config config map \u00b6 JENKINS_HOST - the host name of the Jenkins ingress JENKINS_URL - the url of the Jenkins ingress jenkins-access secret \u00b6 api_token - the Jenkins api token host - the host name of the Jenkins ingress url - the url of the Jenkins ingress username - the Jenkins user name password - the Jenkins password PactBroker config \u00b6 pactbroker-config config map \u00b6 PACTBROKER_URL - the url of the Pact Broker ingress SonarQube config \u00b6 sonarqube-config config map \u00b6 SONARQUBE_URL - the url of the SonarQube ingress sonarqube-access secret \u00b6 SONARQUBE_USER - the user name of the SonarQube user SONARQUBE_PASSWORD - the password of the SonarQube user","title":"cluster configuration"},{"location":"adopting/admin/cluster-config/#cluster-configuration","text":"Within the created Kubernetes or OpenShift cluster there are a number secrets and config maps that are either provided by the IBM Cloud public environment and utilized by the Terraform scripts or are created during the Terraform provisioning process. Many of these secrets and config maps are also used by the pipeline scripts, so you may need to update them as you change passwords.","title":"Cluster Configuration"},{"location":"adopting/admin/cluster-config/#namespaces","text":"The Iteration Zero scripts create four namespaces that are used by the components deployed into the cluster: tools , dev , test , and staging . The actual names used for the namespaces are provided in Terraform variables with the defaults being those listed. The variables are then passed into the namespaces module in stage1-namespaces.tf .","title":"Namespaces"},{"location":"adopting/admin/cluster-config/#provided-resources","text":"","title":"Provided resources"},{"location":"adopting/admin/cluster-config/#tls-secret","text":"When the cluster is created, a secret containing the TLS certs for the ingress subdomain is provided in the default namespace. The the name of the secret is based off of the cluster name with some rules applied to limit the length and replace disallowed characters. Note To avoid issues with the naming conventions for the secret, the Iteration Zero scripts look for the secret that has a tls.key value: kubectl get secrets -o jsonpath='{.items[?(@.data.tls\\.key != \"\")].metadata.name}' . Ideally this would be identified using a label and a selector... During the Iteration Zero process, the TLS secret is copied into each of the four namespaces created and used by the Iteration Zero processes.","title":"TLS secret"},{"location":"adopting/admin/cluster-config/#pull-secrets","text":"Pull secrets for the IBM Cloud Image Registry are generated in the cluster as part of the Iteration Zero process to prepare the namespaces. The secrets are initially created in the default namespace with the following names: default-icr-io default-{region}-icr-io During the namespace preparation process, the pull secrets are copied into the different namespaces with default- prefix dropped. E.g. icr-io {region}-icr-io The pull secrets are also added to the default service account in each of the namespaces. As a result, it is not necessary to directly reference the pull secret as long as the pod runs under the default service account.","title":"Pull secrets"},{"location":"adopting/admin/cluster-config/#created-resources","text":"The following resources are all created during the Iteration Zero provisioning process. These resources are used generally to expose the config of the installed tools but specifically used by the CI pipeline ( Jenkins , Tekton , etc.) to interact with the deployed tools. The resources are bound as optional environment variables in the containers used within the pipeline so if a particular tool has not been installed the associated environment variables won't be set. For example: envFrom : - configMapRef : name : pactbroker-config optional : true - configMapRef : name : sonarqube-config optional : true - secretRef : name : sonarqube-access optional : true","title":"Created resources"},{"location":"adopting/admin/cluster-config/#ibm-cloud-config","text":"The Iteration Zero script creates a config map and secret named ibmcloud-config and ibmcloud-access in the default namespace that contains the relevant configuration values for the cluster within the IBM Cloud account. The config map and secret are copied into each of the Iteration Zero namespaces as the namespaces are created.","title":"IBM Cloud config"},{"location":"adopting/admin/cluster-config/#argocd-config","text":"","title":"ArgoCD config"},{"location":"adopting/admin/cluster-config/#artifactory-config","text":"","title":"Artifactory config"},{"location":"adopting/admin/cluster-config/#jenkins-config","text":"","title":"Jenkins config"},{"location":"adopting/admin/cluster-config/#pactbroker-config","text":"","title":"PactBroker config"},{"location":"adopting/admin/cluster-config/#sonarqube-config","text":"","title":"SonarQube config"},{"location":"adopting/admin/sysdig-setup/","text":"Sysdig Setup \u00b6 Complete the steps for setting up the Sysdig tool Overview \u00b6 Sysdig is the environment's monitoring tool](./tools/sysdig). Before you can use it as part of the environment, you must finish setting up the new instance. Setup \u00b6 Finish setting up the new Sysdig instance. If this is your first time opening the Sysdig dashboard, Sysdig will open a platform dialog and an Onboarding wizard. Select the platform On the Welcome to Sysdig Monitor panel click Next Choose Kubernetes | GKE | OpenShift as the installation method The agent is already installed in the cluster, so click Go to next step! to access the dashboard Configure the monitoring agent The first page shows the hosts (i.e. Kubernetes nodes) and containers that Sysdig found, as well as the integrations it has selected to monitor them Notice there are different types of integrations for different types of runtimes such as Java, servers such as Tomcat, and even infrastructure integrations for monitoring the Kubernetes cluster itself and its containerd engines. Press Next The second page shows the predefined metrics, dashboards, and alerts that Sysdig will start using to monitor the apps in your cluster Press Complete Onboarding Conclusion \u00b6 The environment's Sysdig instance is now setup and ready for developers to use.","title":"Sysdig setup"},{"location":"adopting/admin/sysdig-setup/#sysdig-setup","text":"Complete the steps for setting up the Sysdig tool","title":"Sysdig Setup"},{"location":"adopting/admin/sysdig-setup/#overview","text":"Sysdig is the environment's monitoring tool](./tools/sysdig). Before you can use it as part of the environment, you must finish setting up the new instance.","title":"Overview"},{"location":"adopting/admin/sysdig-setup/#setup","text":"Finish setting up the new Sysdig instance. If this is your first time opening the Sysdig dashboard, Sysdig will open a platform dialog and an Onboarding wizard. Select the platform On the Welcome to Sysdig Monitor panel click Next Choose Kubernetes | GKE | OpenShift as the installation method The agent is already installed in the cluster, so click Go to next step! to access the dashboard Configure the monitoring agent The first page shows the hosts (i.e. Kubernetes nodes) and containers that Sysdig found, as well as the integrations it has selected to monitor them Notice there are different types of integrations for different types of runtimes such as Java, servers such as Tomcat, and even infrastructure integrations for monitoring the Kubernetes cluster itself and its containerd engines. Press Next The second page shows the predefined metrics, dashboards, and alerts that Sysdig will start using to monitor the apps in your cluster Press Complete Onboarding","title":"Setup"},{"location":"adopting/admin/sysdig-setup/#conclusion","text":"The environment's Sysdig instance is now setup and ready for developers to use.","title":"Conclusion"},{"location":"adopting/best-practices/best-practices/","text":"Adopting the Cloud-Native Toolkit \u00b6 To get the most benefit from using the Toolkit, your development process should be based on an agile methodology designed for Cloud-Native development, which encompasses the entire software lifecycle, from initial concept, through development into production and operational running, such as the IBM Garage Methodology . The following pages show some of the practices recommended as part of a modern, cloud-native development environment: DevOps - automated pipeline to build, test, integrate and deliver applications and services Infrastructure as Code - manage infrastructure through version controlled configuration files backed by automation technology You development tooling needs to support the development team and adopted methodology and processes. The Cloud-Native Toolkit provides a good starting point for development tooling, but there are a number of decisions to make when creating a production setup of Toolkit Best practices for setting up the toolkit on IBM Cloud","title":"Best Practices"},{"location":"adopting/best-practices/best-practices/#adopting-the-cloud-native-toolkit","text":"To get the most benefit from using the Toolkit, your development process should be based on an agile methodology designed for Cloud-Native development, which encompasses the entire software lifecycle, from initial concept, through development into production and operational running, such as the IBM Garage Methodology . The following pages show some of the practices recommended as part of a modern, cloud-native development environment: DevOps - automated pipeline to build, test, integrate and deliver applications and services Infrastructure as Code - manage infrastructure through version controlled configuration files backed by automation technology You development tooling needs to support the development team and adopted methodology and processes. The Cloud-Native Toolkit provides a good starting point for development tooling, but there are a number of decisions to make when creating a production setup of Toolkit Best practices for setting up the toolkit on IBM Cloud","title":"Adopting the Cloud-Native Toolkit"},{"location":"adopting/best-practices/devops/","text":"Dev-Ops Concepts \u00b6 This short video introduces the concepts of DevOps with Red Hat OpenShift: Continuous Delivery \u00b6 In IBM Garage Method, one of the Develop practices is continuous delivery . A preferred model for implementing continuous delivery is GitOps, where the desired state of the operational environment is defined in a source control repository (namely Git). What is continuous delivery \u00b6 Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: Continuous delivery deploys an application when a user manually triggers deployment Continuous deployment deploys an application automatically when it is ready Typically, continuous deployment is an evolution of a continuous delivery process. An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. Once these tests have been automated in a way that reliably verifies the application components then the deployment can be automated. Additionally, for continuous delivery it important to employ other best practices around moving and managing changes in an environment: blue-green deployments, shadow deployments, and feature toggles to name a few. Until these practices are in place and verified, it is best to stick with continuous delivery. As with most cloud-native practices, the move from continuous deployment to continuous delivery would not be done in a \"big bang\" but incrementally and as different application components are ready. What is GitOps \u00b6 GitOps is the operational pattern of using source code repositories (namely Git) as the source of truth for defining the configuration that makes up the desired state of the operational environment. Git repositories are used to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: Git provides a versioned history of changes, listing what was changed and who made the change Change releases can be managed from a pull request, allowing multiple people to make changes but a select few to approve the changes Git provides access control mechanisms to limit who can change and view the configuration Git enables changes to be rolled back quickly if there is an issue with a new release Git supports multiple models for change management: Branches, Forks, GitFlow, etc Hosted git providers (like GitHub) provide a rich API that allows the different operations to be automated, if desired CI/CD integration \u00b6 For the full end-to-end build and delivery process, both the CI and CD pipelines are used. When working in a containerized environment such as Kubernetes or Red Hat OpenShift, the responsibilities between the two processes are clearly defined: The CI pipeline is responsible for building validating and packaging the \"raw materials\" (source code, deployment configuration, etc) into versioned, deployable artifacts (container images, helm charts, published artifacts, etc) The CD pipeline is responsible for applying the deployable artifacts into a particular target environment A change made to one of the source repositories triggers the CI process. The CI process builds, validates, and packages those changes into deployable artifacts that are stored in the image registry and artifact repository(ies). The last step of the CI process updates the GitOps repository with information about the updated artifacts. At a minimum this step stores updates the version number to the newly released versions of the artifacts but depending on the environment this step might also update the deployment configuration. Note It is also possible to trigger a process when a new image is available in the image registry or a new artifact is available to the artifact management system. In this case, the CI process could be split into two parts: 1) create the container image and artifacts, and 2) update the GitOps repo with the available artifacts. Changes to the GitOps repository trigger the CD pipeline to run In the CD pipeline, the configuration describing the desired state as defined in the GitOps repository is reconciled with the actual state of the environment and resources are created, updated, or destroyed as appropriate. Tools to support Continuous Delivery \u00b6 The practice of (CD) can be accomplished in different ways and with different tools. It is possible and certainly valid to use the same tool for both CI and CD (e.g. Tekton or Jenkins) with caution you enforce a clear separation between the two processes. Typically, that would result in two distinct pipelines to respond to changes that happen within the two different Git repositories - source repo and gitops repo. Another class of tools is available that are particularly suited for Continuous Delivery and GitOps. The following is by no means an exhaustive list but it does provide some of the common tools used for CD in a cloud-native environment: ArgoCD Flux IBM Multicloud Manager","title":"DevOps"},{"location":"adopting/best-practices/devops/#dev-ops-concepts","text":"This short video introduces the concepts of DevOps with Red Hat OpenShift:","title":"Dev-Ops Concepts"},{"location":"adopting/best-practices/devops/#continuous-delivery","text":"In IBM Garage Method, one of the Develop practices is continuous delivery . A preferred model for implementing continuous delivery is GitOps, where the desired state of the operational environment is defined in a source control repository (namely Git).","title":"Continuous Delivery"},{"location":"adopting/best-practices/devops/#what-is-continuous-delivery","text":"Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: Continuous delivery deploys an application when a user manually triggers deployment Continuous deployment deploys an application automatically when it is ready Typically, continuous deployment is an evolution of a continuous delivery process. An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. Once these tests have been automated in a way that reliably verifies the application components then the deployment can be automated. Additionally, for continuous delivery it important to employ other best practices around moving and managing changes in an environment: blue-green deployments, shadow deployments, and feature toggles to name a few. Until these practices are in place and verified, it is best to stick with continuous delivery. As with most cloud-native practices, the move from continuous deployment to continuous delivery would not be done in a \"big bang\" but incrementally and as different application components are ready.","title":"What is continuous delivery"},{"location":"adopting/best-practices/devops/#what-is-gitops","text":"GitOps is the operational pattern of using source code repositories (namely Git) as the source of truth for defining the configuration that makes up the desired state of the operational environment. Git repositories are used to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: Git provides a versioned history of changes, listing what was changed and who made the change Change releases can be managed from a pull request, allowing multiple people to make changes but a select few to approve the changes Git provides access control mechanisms to limit who can change and view the configuration Git enables changes to be rolled back quickly if there is an issue with a new release Git supports multiple models for change management: Branches, Forks, GitFlow, etc Hosted git providers (like GitHub) provide a rich API that allows the different operations to be automated, if desired","title":"What is GitOps"},{"location":"adopting/best-practices/devops/#cicd-integration","text":"For the full end-to-end build and delivery process, both the CI and CD pipelines are used. When working in a containerized environment such as Kubernetes or Red Hat OpenShift, the responsibilities between the two processes are clearly defined: The CI pipeline is responsible for building validating and packaging the \"raw materials\" (source code, deployment configuration, etc) into versioned, deployable artifacts (container images, helm charts, published artifacts, etc) The CD pipeline is responsible for applying the deployable artifacts into a particular target environment A change made to one of the source repositories triggers the CI process. The CI process builds, validates, and packages those changes into deployable artifacts that are stored in the image registry and artifact repository(ies). The last step of the CI process updates the GitOps repository with information about the updated artifacts. At a minimum this step stores updates the version number to the newly released versions of the artifacts but depending on the environment this step might also update the deployment configuration. Note It is also possible to trigger a process when a new image is available in the image registry or a new artifact is available to the artifact management system. In this case, the CI process could be split into two parts: 1) create the container image and artifacts, and 2) update the GitOps repo with the available artifacts. Changes to the GitOps repository trigger the CD pipeline to run In the CD pipeline, the configuration describing the desired state as defined in the GitOps repository is reconciled with the actual state of the environment and resources are created, updated, or destroyed as appropriate.","title":"CI/CD integration"},{"location":"adopting/best-practices/devops/#tools-to-support-continuous-delivery","text":"The practice of (CD) can be accomplished in different ways and with different tools. It is possible and certainly valid to use the same tool for both CI and CD (e.g. Tekton or Jenkins) with caution you enforce a clear separation between the two processes. Typically, that would result in two distinct pipelines to respond to changes that happen within the two different Git repositories - source repo and gitops repo. Another class of tools is available that are particularly suited for Continuous Delivery and GitOps. The following is by no means an exhaustive list but it does provide some of the common tools used for CD in a cloud-native environment: ArgoCD Flux IBM Multicloud Manager","title":"Tools to support Continuous Delivery"},{"location":"adopting/best-practices/infrastructure-as-code/","text":"Infrastructure as Code \u00b6 Infrastructure as Code (IaC) uses a high-level descriptive coding language to automate the provisioning of IT infrastructure. This automation eliminates the need for developers to manually provision and manage servers, operating systems, database connections, storage, and other infrastructure elements every time they want to develop, test, or deploy a software application. In an era when it\u2019s not uncommon for an enterprise to deploy hundreds of applications into production every day\u2014and when infrastructure is constantly being spun up, torn down, and scaled up and down in response to developer and user demands\u2014it\u2019s essential for an organization to automate infrastructure in order to control costs, reduce risks, and respond with speed to new business opportunities and competitive threats. IaC makes this automation possible. IaC is also an essential DevOps practice, indispensable to a competitively paced software delivery lifecycle. It enables DevOps teams rapidly create and version infrastructure in the same way they version source code and to track these versions so as to avoid inconsistency among IT environments that can lead to serious issues during deployment. Sai Vennam takes a closer look at IaC in the following video, \u201cWhat is Infrastructure as Code?\u201d: Source: https://www.ibm.com/cloud/learn/infrastructure-as-code","title":"Infrastructure as Code"},{"location":"adopting/best-practices/infrastructure-as-code/#infrastructure-as-code","text":"Infrastructure as Code (IaC) uses a high-level descriptive coding language to automate the provisioning of IT infrastructure. This automation eliminates the need for developers to manually provision and manage servers, operating systems, database connections, storage, and other infrastructure elements every time they want to develop, test, or deploy a software application. In an era when it\u2019s not uncommon for an enterprise to deploy hundreds of applications into production every day\u2014and when infrastructure is constantly being spun up, torn down, and scaled up and down in response to developer and user demands\u2014it\u2019s essential for an organization to automate infrastructure in order to control costs, reduce risks, and respond with speed to new business opportunities and competitive threats. IaC makes this automation possible. IaC is also an essential DevOps practice, indispensable to a competitively paced software delivery lifecycle. It enables DevOps teams rapidly create and version infrastructure in the same way they version source code and to track these versions so as to avoid inconsistency among IT environments that can lead to serious issues during deployment. Sai Vennam takes a closer look at IaC in the following video, \u201cWhat is Infrastructure as Code?\u201d: Source: https://www.ibm.com/cloud/learn/infrastructure-as-code","title":"Infrastructure as Code"},{"location":"adopting/best-practices/registry/","text":"Container Registry strategy \u00b6 By default an OpenShift cluster will use its internal registry. This is fine when working within cluster. Some additional permissions may be needed to allow images to be accessible across projects/namespaces. An Image Registry is a repository of versioned container images. It is perhaps a subset of the larger Artifact Management topic but has special considerations. A specific protocol has been defined around building, pushing, tagging and pulling container images to and from an Image Repository. Typically, the continuous integration process is responsible for verifying and building the application source into an image and pushing it into the registry. At deployment time, the deployment descriptor (e.g. kubernetes resource definition) references the image at its location within the image registry and the container platform pulls the image and manages the running image in the cluster. Tools like skopeo can also be used within the process to copy images from one registry to another. There are a number of options available for the Image Registry, both running in-cluster and outside of the cluster. Red Hat OpenShift even provides an image registry as part of the platform. While an intermediate image registry might be used during the CI process, in an enterprise environment it is ideal to have a centrally managed image registry from which vulnerability scans, certifications, and backups can be performed. Some of the available options include: IBM Cloud Image Registry Artifactory Nexus Red Hat OpenShift image streams","title":"Container Registry"},{"location":"adopting/best-practices/registry/#container-registry-strategy","text":"By default an OpenShift cluster will use its internal registry. This is fine when working within cluster. Some additional permissions may be needed to allow images to be accessible across projects/namespaces. An Image Registry is a repository of versioned container images. It is perhaps a subset of the larger Artifact Management topic but has special considerations. A specific protocol has been defined around building, pushing, tagging and pulling container images to and from an Image Repository. Typically, the continuous integration process is responsible for verifying and building the application source into an image and pushing it into the registry. At deployment time, the deployment descriptor (e.g. kubernetes resource definition) references the image at its location within the image registry and the container platform pulls the image and manages the running image in the cluster. Tools like skopeo can also be used within the process to copy images from one registry to another. There are a number of options available for the Image Registry, both running in-cluster and outside of the cluster. Red Hat OpenShift even provides an image registry as part of the platform. While an intermediate image registry might be used during the CI process, in an enterprise environment it is ideal to have a centrally managed image registry from which vulnerability scans, certifications, and backups can be performed. Some of the available options include: IBM Cloud Image Registry Artifactory Nexus Red Hat OpenShift image streams","title":"Container Registry strategy"},{"location":"adopting/best-practices/secret-management/","text":"Secret management \u00b6 Deploying an application into containers involves both the application logic and the associated configuration. The application logic is packaged into a container image so that it can be deployed but in order to make the container image portable across different environments, the application configuration should be managed separately and applied to the application container image at deployment time. Fortunately, container platforms like Red Hat OpenShift and Kubernetes provides a mechanism to easily provide the configuration at deployment time: ConfigMaps and Secrets. Both ConfigMaps and Secrets work in the same way to represent information in key value pairs and allow that information to be attached to a running container in a number of different ways. Unlike ConfigMaps, Secrets are intended to hold sensitive information (like passwords) and have additional access control facilities to limit who can read and use that information. With a GitOps approach to continuous delivery , the application container image and the associated configuration are represented in the Git repository together. When the desired state defined in Git is applied to an environment, the relevant Kubernetes resources like Deployments, ConfigMaps, and Secrets are generated from the provided Git configuration. A common issue when doing GitOps is how to handle sensitive information that should not be stored in the Git repository (e.g. passwords, keys, etc). There are two different approaches to how to handle this issue: Inject the values from another source into kubernetes Secret(s) at deployment time Inject the values from another source in the pod at startup time via an InitContainer The \"other source\" in this case would be a key management system that centralizes the storage and management of sensitive information. There are a number of key management systems available to manage the secret values: Key Protect Hyper Protect Hashicorp Vault Use the key management system at deployment time \u00b6 CD with ArgoCD covers how to use ArgoCD to do GitOps, including how to manage sensitive information in a key management system. Use the key management system at pod startup time \u00b6 !!Todo Coming soon","title":"Secret management"},{"location":"adopting/best-practices/secret-management/#secret-management","text":"Deploying an application into containers involves both the application logic and the associated configuration. The application logic is packaged into a container image so that it can be deployed but in order to make the container image portable across different environments, the application configuration should be managed separately and applied to the application container image at deployment time. Fortunately, container platforms like Red Hat OpenShift and Kubernetes provides a mechanism to easily provide the configuration at deployment time: ConfigMaps and Secrets. Both ConfigMaps and Secrets work in the same way to represent information in key value pairs and allow that information to be attached to a running container in a number of different ways. Unlike ConfigMaps, Secrets are intended to hold sensitive information (like passwords) and have additional access control facilities to limit who can read and use that information. With a GitOps approach to continuous delivery , the application container image and the associated configuration are represented in the Git repository together. When the desired state defined in Git is applied to an environment, the relevant Kubernetes resources like Deployments, ConfigMaps, and Secrets are generated from the provided Git configuration. A common issue when doing GitOps is how to handle sensitive information that should not be stored in the Git repository (e.g. passwords, keys, etc). There are two different approaches to how to handle this issue: Inject the values from another source into kubernetes Secret(s) at deployment time Inject the values from another source in the pod at startup time via an InitContainer The \"other source\" in this case would be a key management system that centralizes the storage and management of sensitive information. There are a number of key management systems available to manage the secret values: Key Protect Hyper Protect Hashicorp Vault","title":"Secret management"},{"location":"adopting/best-practices/secret-management/#use-the-key-management-system-at-deployment-time","text":"CD with ArgoCD covers how to use ArgoCD to do GitOps, including how to manage sensitive information in a key management system.","title":"Use the key management system at deployment time"},{"location":"adopting/best-practices/secret-management/#use-the-key-management-system-at-pod-startup-time","text":"!!Todo Coming soon","title":"Use the key management system at pod startup time"},{"location":"adopting/best-practices/test/","text":"TEST \u00b6 Test Doc","title":"Test"},{"location":"adopting/best-practices/test/#test","text":"Test Doc","title":"TEST"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/","text":"Configure Account \u00b6 Set up the account so the environment can be installed Note An account manager performs the steps on this page. See Plan Installation > Roles for the overview of the roles involved. Info The video in Resource Access Management > Configuration Process shows how to perform the steps in this process. Configure IBM Cloud account \u00b6 The account must provide a few resources that will be needed to install and use the environment: A public/private pair of VLANs A resource group A pair of access groups for the admin and users These are the steps an account manager should perform to configure the account. Upgrade the image registry \u00b6 First, we'll upgrade the service plan for the image registry so that is has more capacity. Set the registry plan ibmcloud cr plan-upgrade standard Now we can store more container images. Prepare to run scripts \u00b6 Second, we'll use some scripts in the steps below to help create access groups. Here, we'll download the scripts and prepare to run them. (If you want to use the console to manually configure the access groups, you can skip this step.) Clone the Git repository with the scripts. (This repo also has the scripts for installing the environment.) Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero The scripts need you to log into IBM Cloud first. In the terminal you'll use to run the scripts, log in to IBM Cloud. Log in to the IBM Cloud CLI ibmcloud login -a cloud.ibm.com -r <region> Note The steps below need to be repeated for each new environment: Each environment can be in a different data center. For each new environment, select the data center to host it and ensure it has a pair of public/private VLANs for the environment to use. Two environments in the same data center can share a pair of VLANs, or each can be given a separate pair of VLANs. Each environment needs its own resource group and pair of access groups for administrators and users. Each environment will need its own cluster, whether it's created by an account manager or an environment administrator. Data center \u00b6 Third, decide which IBM Cloud location will host the environment. That will be specified with two settings: Region -- A geography such as us-south or eu-gb Zone -- A data center in the region such as dal10 or lon02 Public and private VLANs \u00b6 Fourth, create or provide a pair of public and private VLANs for the selected region and zone. These VLANs will implement the public and private networks in the Kubernetes or OpenShift cluster. Note If your account already has a pair of VLANs for your desired region and zone, you can use those. Use the Toolkit CLI's igc vlan command to select two existing VLANs and generate the properties to use for the installation scripts These links help explain how to find the VLANs an account has, create more, and how a cluster uses them to implement its network. Use the console to manage VLANs: List existing VLANs: Resources > Classic Infrastructure > IP Management > VLANs Create a VLAN: Catalog > Services > Networking > VLAN Read the docs on using VLANs: Getting started with VLANs Understanding network basics of classic clusters Overview of classic networking in IBM Cloud Kubernetes Service Resource group \u00b6 Fifth, create or provide a resource group . This resource group will control access to the environment's cluster and service instances. This resource group should typically be named after the development team, its project, or the application it is implementing. Warning The resource group name should be 24 characters or fewer, and should conform to Kubernetes resource naming conventions --the name should be all lowercase letters, digits, and the separators should be dashes. (The Cloud-Native Toolkit installation scripts will name the cluster <resource-group>-cluster , and a cluster name is limited to 32 characters.) We give our resource groups names like mooc-team-one , garage-dev-tools , gct-showcase , etc. Create the resource group Kubernetes service API key \u00b6 Sixth, to create clusters in the resource group, the account will need API keys for the container service to create resources in the classic infrastructure . A separate API key is needed for each region and resource group. An account manager should use the account's functional ID to set the API key(s) for the new resource group. Create an API key for the resource group and the data center's region: Log into the IBM Cloud CLI using the functional ID key created by the account owner ibmcloud login --apikey @key_file_name Perform these steps to set the API key: Setting up the API key to enable access to the infrastructure portfolio ibmcloud ks api-key reset --region <region> The list of existing API keys shows the new key named containers-kubernetes-key ; the description specifies the resource group and region Access group for environment administrators \u00b6 Seventh, create an access group to grant the necessary permissions for installing a environment. Do this by running a script, or by using the console to manually perform the steps in the script. Also, add the environment administrator(s) (who is the user who will run the scripts to create the environment) to this group. To create the access group for the environment administrators: Create a new access group , name it something like <resource_group>-ADMIN (all capital letters) Run the script ./terraform/scripts/acp-admin.sh , which adds the necessary policies to the access group Add the environment administrator(s) to the group The script adds policies that allow the user to add resources to the resource group: Permission to create clusters Permission to manage the IBM Cloud Container Registry (used as the environment's image registry ) Permission to create service instances Access group for environment users \u00b6 Eighth, create an access group to enable users (e.g. developers, data scientists, etc.) to access the resources in the environment. This can be done later, after the environment is created, either by running a script or using the console. Also, add the users who will use the environment (e.g. developers, etc.) to this group. To create the access group for the environment users: Create a new access group , name it something like <resource_group>-USER (all capital letters) Run the script ./terraform/scripts/acp-user.sh , which adds the necessary policies to the access group Add the users to the group The script adds policies that allow the user to use resources to the resource group: Access to the resource group Access to the cluster Access to the image registry Access to each of the services in the resource group Cluster for the environment (optional) \u00b6 Ninth, if the environment administrator will install the environment including creating a new cluster, then skip this step. However, if the environment administrator will install the environment into an existing cluster, then the account manager must create the cluster for the environment administrator. Create the cluster that the environment will be installed into. Create either a Kubernetes cluster or Red Hat OpenShift cluster as needed. A typical cluster size is single zone, 3 nodes, each 8 vCPUs 32GB RAM. To configure RBAC security in the cluster: Run the script ./terraform/scripts/rbac.sh , which configures RBAC inside the cluster Configuration settings \u00b6 The account manager needs to pass the following values about the account configuration to the environment administrator: The region for the environment The resource group for the environment If the cluster already exists: Cluster type (Kubernetes or Red Hat OpenShift) Cluster name If the environment administrator will create the cluster: For the public/private VLAN pair: the region, data center, and VLAN IDs Conclusion \u00b6 The account manager has now configured the account so that the environment administrator can install the environment, and has passed the configuration settings to the environment administrator.","title":"Access control"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#configure-account","text":"Set up the account so the environment can be installed Note An account manager performs the steps on this page. See Plan Installation > Roles for the overview of the roles involved. Info The video in Resource Access Management > Configuration Process shows how to perform the steps in this process.","title":"Configure Account"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#configure-ibm-cloud-account","text":"The account must provide a few resources that will be needed to install and use the environment: A public/private pair of VLANs A resource group A pair of access groups for the admin and users These are the steps an account manager should perform to configure the account.","title":"Configure IBM Cloud account"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#upgrade-the-image-registry","text":"First, we'll upgrade the service plan for the image registry so that is has more capacity. Set the registry plan ibmcloud cr plan-upgrade standard Now we can store more container images.","title":"Upgrade the image registry"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#prepare-to-run-scripts","text":"Second, we'll use some scripts in the steps below to help create access groups. Here, we'll download the scripts and prepare to run them. (If you want to use the console to manually configure the access groups, you can skip this step.) Clone the Git repository with the scripts. (This repo also has the scripts for installing the environment.) Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero The scripts need you to log into IBM Cloud first. In the terminal you'll use to run the scripts, log in to IBM Cloud. Log in to the IBM Cloud CLI ibmcloud login -a cloud.ibm.com -r <region> Note The steps below need to be repeated for each new environment: Each environment can be in a different data center. For each new environment, select the data center to host it and ensure it has a pair of public/private VLANs for the environment to use. Two environments in the same data center can share a pair of VLANs, or each can be given a separate pair of VLANs. Each environment needs its own resource group and pair of access groups for administrators and users. Each environment will need its own cluster, whether it's created by an account manager or an environment administrator.","title":"Prepare to run scripts"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#data-center","text":"Third, decide which IBM Cloud location will host the environment. That will be specified with two settings: Region -- A geography such as us-south or eu-gb Zone -- A data center in the region such as dal10 or lon02","title":"Data center"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#public-and-private-vlans","text":"Fourth, create or provide a pair of public and private VLANs for the selected region and zone. These VLANs will implement the public and private networks in the Kubernetes or OpenShift cluster. Note If your account already has a pair of VLANs for your desired region and zone, you can use those. Use the Toolkit CLI's igc vlan command to select two existing VLANs and generate the properties to use for the installation scripts These links help explain how to find the VLANs an account has, create more, and how a cluster uses them to implement its network. Use the console to manage VLANs: List existing VLANs: Resources > Classic Infrastructure > IP Management > VLANs Create a VLAN: Catalog > Services > Networking > VLAN Read the docs on using VLANs: Getting started with VLANs Understanding network basics of classic clusters Overview of classic networking in IBM Cloud Kubernetes Service","title":"Public and private VLANs"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#resource-group","text":"Fifth, create or provide a resource group . This resource group will control access to the environment's cluster and service instances. This resource group should typically be named after the development team, its project, or the application it is implementing. Warning The resource group name should be 24 characters or fewer, and should conform to Kubernetes resource naming conventions --the name should be all lowercase letters, digits, and the separators should be dashes. (The Cloud-Native Toolkit installation scripts will name the cluster <resource-group>-cluster , and a cluster name is limited to 32 characters.) We give our resource groups names like mooc-team-one , garage-dev-tools , gct-showcase , etc. Create the resource group","title":"Resource group"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#kubernetes-service-api-key","text":"Sixth, to create clusters in the resource group, the account will need API keys for the container service to create resources in the classic infrastructure . A separate API key is needed for each region and resource group. An account manager should use the account's functional ID to set the API key(s) for the new resource group. Create an API key for the resource group and the data center's region: Log into the IBM Cloud CLI using the functional ID key created by the account owner ibmcloud login --apikey @key_file_name Perform these steps to set the API key: Setting up the API key to enable access to the infrastructure portfolio ibmcloud ks api-key reset --region <region> The list of existing API keys shows the new key named containers-kubernetes-key ; the description specifies the resource group and region","title":"Kubernetes service API key"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#access-group-for-environment-administrators","text":"Seventh, create an access group to grant the necessary permissions for installing a environment. Do this by running a script, or by using the console to manually perform the steps in the script. Also, add the environment administrator(s) (who is the user who will run the scripts to create the environment) to this group. To create the access group for the environment administrators: Create a new access group , name it something like <resource_group>-ADMIN (all capital letters) Run the script ./terraform/scripts/acp-admin.sh , which adds the necessary policies to the access group Add the environment administrator(s) to the group The script adds policies that allow the user to add resources to the resource group: Permission to create clusters Permission to manage the IBM Cloud Container Registry (used as the environment's image registry ) Permission to create service instances","title":"Access group for environment administrators"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#access-group-for-environment-users","text":"Eighth, create an access group to enable users (e.g. developers, data scientists, etc.) to access the resources in the environment. This can be done later, after the environment is created, either by running a script or using the console. Also, add the users who will use the environment (e.g. developers, etc.) to this group. To create the access group for the environment users: Create a new access group , name it something like <resource_group>-USER (all capital letters) Run the script ./terraform/scripts/acp-user.sh , which adds the necessary policies to the access group Add the users to the group The script adds policies that allow the user to use resources to the resource group: Access to the resource group Access to the cluster Access to the image registry Access to each of the services in the resource group","title":"Access group for environment users"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#cluster-for-the-environment-optional","text":"Ninth, if the environment administrator will install the environment including creating a new cluster, then skip this step. However, if the environment administrator will install the environment into an existing cluster, then the account manager must create the cluster for the environment administrator. Create the cluster that the environment will be installed into. Create either a Kubernetes cluster or Red Hat OpenShift cluster as needed. A typical cluster size is single zone, 3 nodes, each 8 vCPUs 32GB RAM. To configure RBAC security in the cluster: Run the script ./terraform/scripts/rbac.sh , which configures RBAC inside the cluster","title":"Cluster for the environment (optional)"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#configuration-settings","text":"The account manager needs to pass the following values about the account configuration to the environment administrator: The region for the environment The resource group for the environment If the cluster already exists: Cluster type (Kubernetes or Red Hat OpenShift) Cluster name If the environment administrator will create the cluster: For the public/private VLAN pair: the region, data center, and VLAN IDs","title":"Configuration settings"},{"location":"adopting/best-practices/ibm-cloud-account/configure-account/#conclusion","text":"The account manager has now configured the account so that the environment administrator can install the environment, and has passed the configuration settings to the environment administrator.","title":"Conclusion"},{"location":"adopting/best-practices/ibm-cloud-account/ibm-cloud-setup/","text":"Setting up an IBM Cloud Account \u00b6 During the fast-start learning section the environment setup was a default install. However, when setting up an environment for production use, there are a number of considerations that need to be made to create the environment that best meets the needs of your development and operations teams while providing the right level of access controls and permissions to resources within the environment. The pages linked below take you through a set of recommendations for setting up a Cloud-Native Toolkit environment on the IBM Cloud: Plan the installation configure your cloud account and roles","title":"IBM Cloud setup"},{"location":"adopting/best-practices/ibm-cloud-account/ibm-cloud-setup/#setting-up-an-ibm-cloud-account","text":"During the fast-start learning section the environment setup was a default install. However, when setting up an environment for production use, there are a number of considerations that need to be made to create the environment that best meets the needs of your development and operations teams while providing the right level of access controls and permissions to resources within the environment. The pages linked below take you through a set of recommendations for setting up a Cloud-Native Toolkit environment on the IBM Cloud: Plan the installation configure your cloud account and roles","title":"Setting up an IBM Cloud Account"},{"location":"adopting/best-practices/ibm-cloud-account/monitoring-logs/","text":"Monitoring and Log Management on IBM Cloud \u00b6 The IBM Cloud offers consolidated application monitoring and logging services. Monitoring \u00b6 In IBM Garage Method, one of the Operate practices is to automate application monitoring . Sysdig automates application monitoring, enabling an operator to view stats and collect metrics about a Kubernetes cluster and its deployments. The environment includes an IBM Cloud Monitoring with Sysdig service instance configured with a Sysdig agent installed in the environment's cluster. Simply by deploying your application into the environment, Sysdig monitors it, just open the Sysdig web UI from the IBM Cloud dashboard to browse your application's status. Log Management \u00b6 Imagine your application isn't working right in production even though the environment is fine. What information would you want in your logs to help you figure out what's wrong with your application? Build logging messages for that information into your application. Given that your application is logging, as are lots of other applications and services in your cloud environment, these logs need to be managed and made accessible. LogDNA adds log management capabilities to a Kubernetes cluster and its deployments. The environment includes an IBM Log Analysis with LogDNA service instance configured with a LogDNA agent installed in the environment's cluster. Simply by deploying your application into the environment, LogDNA collects the logs, just open the LogDNA web UI from the IBM Cloud dashboard to browse your application's logs.","title":"Monitoring and Logs"},{"location":"adopting/best-practices/ibm-cloud-account/monitoring-logs/#monitoring-and-log-management-on-ibm-cloud","text":"The IBM Cloud offers consolidated application monitoring and logging services.","title":"Monitoring and Log Management on IBM Cloud"},{"location":"adopting/best-practices/ibm-cloud-account/monitoring-logs/#monitoring","text":"In IBM Garage Method, one of the Operate practices is to automate application monitoring . Sysdig automates application monitoring, enabling an operator to view stats and collect metrics about a Kubernetes cluster and its deployments. The environment includes an IBM Cloud Monitoring with Sysdig service instance configured with a Sysdig agent installed in the environment's cluster. Simply by deploying your application into the environment, Sysdig monitors it, just open the Sysdig web UI from the IBM Cloud dashboard to browse your application's status.","title":"Monitoring"},{"location":"adopting/best-practices/ibm-cloud-account/monitoring-logs/#log-management","text":"Imagine your application isn't working right in production even though the environment is fine. What information would you want in your logs to help you figure out what's wrong with your application? Build logging messages for that information into your application. Given that your application is logging, as are lots of other applications and services in your cloud environment, these logs need to be managed and made accessible. LogDNA adds log management capabilities to a Kubernetes cluster and its deployments. The environment includes an IBM Log Analysis with LogDNA service instance configured with a LogDNA agent installed in the environment's cluster. Simply by deploying your application into the environment, LogDNA collects the logs, just open the LogDNA web UI from the IBM Cloud dashboard to browse your application's logs.","title":"Log Management"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/","text":"Plan Installation \u00b6 Overall process for installing a environment Background \u00b6 To understand how the process is performed, keep these concepts in mind. Roles \u00b6 An environment is installed and used by users acting in four roles: Account owner : The user who owns the account Account managers : Account owner or other users with account management permissions Environment administrators : Users in the account with permissions to create services in the environment's resource group Environment users : Users in the account with permissions to use existing services in the environment's resource group (e.g. developers, data scientists, etc.) The account owner must create the access group for account managers (see below). The account owner will: Create an ACCT-MGR-IAM-ADMIN access group using the acp-mgr script Add a functional ID, configured using the acp-iaas script, with API keys for the account managers Then, as described in Configure Account , the account managers can set up the resource groups and access groups needed to install and use the environments. For each environment, the account managers will: Create a resource group Create an access group named <resource_group>-ADMIN using the script acp-admin Create an access group named <resource_group>-USER using the script acp-user This diagram from [Resource Access Management]((../../resources/ibm-cloud/access-control.md#access-group-example){: target=_blank} shows the relationship of these access groups to a pair of development environments: The steps below help the account owner start configuring the account in this manner. Set up account managers \u00b6 The account owner may want to delegate the responsibilities for configuring the account for all of the development teams that want environments. Those users who are delegated to will need account management permissions. An easy way to manage who has these permissions is to create an access group with those policies and add those users to the group. An account only needs one of these account manager access groups, which will be used to support all environments in the account. In a new account, the account owner is the only user. Even after inviting other users, the account owner is initially the only user with the account management permissions needed to grant those permissions to other users. Therefore it is the account owner who must create the access group for account managers. !!!Note: The video in Resource Access Management > Configuration Process shows how to perform the steps in this process. Prepare to run scripts \u00b6 We'll use some scripts in the steps below to help create access groups. Here, we'll download the scripts and prepare to run them. (If you want to use the console to manually configure the access groups, you can skip this step.) Clone the Git repository with the scripts. (This repo also has the scripts for installing the environment.) Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero The scripts need you to log into IBM Cloud first. In the terminal you'll use to run the scripts, log in to IBM Cloud. Log in to the IBM Cloud CLI ibmcloud login -a cloud.ibm.com -r <region> Access group for account managers \u00b6 The account owner must create an access group to grant the necessary permissions for managing the account. Do this by running a script, or by using the console to manually perform the steps in the script. Also, add the account manager(s) to this group. Note IBM Cloud has multiple account management services , in addition to IAM: Billing, License and entitlement, Support center, etc. An easy way to grant access to these individually is to create an access group for administering each: ACCT-MGR-BILLING-ADMIN, ACCT-MGR-LICENSE-ADMIN, ACCT-MGR-SUPPORT-ADMIN. The group created below only has IAM capabilities, so a good name for it is ACCT-MGR-IAM-ADMIN. To create the access group for the account managers: Create a new access group , name it something like ACCT-MGR-IAM-ADMIN (all capital letters) (or name it after your account) Run the script ./terraform/scripts/acp-mgr.sh , which adds the necessary policies to the access group Add the account managers to the group The script adds policies that allow the user to: Create resource groups Invite users to the account Create access groups The script also adds the same policies that an environment administrator has. But whereas an environment administrator can only manage the resources in one resource group, an account manager can manage the resources in all resource groups. This gives the account manager the ability in any resource group to: Create clusters Manage the IBM Cloud Container Registry (used as the environment's image registry ) Create service instances Functional ID for infrastructure permissions \u00b6 Account managers need the permissions to create and manage IaaS resources required by a environment. Permissions for classic infrastructure (fka SoftLayer) cannot be added to an access group, only to a user. Rather than add these permissions to each account manager, create a functional ID and grant it the infrastructure permissions. The functional ID will own the API keys that the Kubernetes service needs to create clusters. As account managers are added to and removed from the account, the functional ID will always remain and always have the necessary infrastructure permissions. Set up the functional ID in the account: Create the functional ID, sign it up for an IBM Cloud account, and invite the user to this account Run the script ./terraform/scripts/acp-iaas.sh for the functional ID's user, which adds the necessary permissions to the user The script adds the classic infrastructure permissions needed to create and manage clusters: Create VLANs Create Kubernetes Service clusters (e.g. create virtual servers, storage, and networking) Manage Kubernetes Service clusters (e.g. add nodes) The script also adds the IAM permissions to: Run the command to reset the API key that the Kubernetes service will use By granting these infrastructure permissions to the functional ID and using it to create API keys, the account managers and environment administrators can create Kubernetes and OpenShift clusters without needing infrastructure permissions. Next, each account manager will need to use the functional ID to reset the API key that the Kubernetes service will use. By using the functional ID, the API key will be owned by the functional ID instead of by the account manager. The account managers need a way to log in as the functional ID without all of them sharing the ID's password or a single API key. Thus each account manager needs their own API key for the functional ID's account. Create API keys for the functional ID: Log into IBM Cloud as the functional ID (not as the account owner) Switch to this account that you're configuring For each account manager, create an API key , downloading each key to a file Give each account manager their API key file for the functional ID Users should not share these API key files with each other. When a user is no longer an account manager, remove them from the access group and delete their API key. To make a user an account manager, add them to the access group and create an API key for them. Process \u00b6 The overall process: Configure account -- An account manager configures the account so that environments can be installed Install environment -- An environment administrator runs the Cloud-Native Toolkit scripts to create a environment Configure environment -- The environment administrator finishes setting up the newly installed environment Once the environment is set up, the environment users can start using it to develop applications. 1. Configure account \u00b6 First, before installing an environment, an account manager needs to configure the IBM Cloud account for the environment. See Configure Account for detailed instructions, which accomplish this: Upgrade the image registry Prepare to run scripts Select a region and zone Select a pair of public/private VLANs Create a resource group Set the Kubernetes service API key Create an access group for environment administrators Create an access group for environment users Optionally, create a cluster for the environment 2. Install environment \u00b6 Once the account manager has configured the account for installing an environment, the account manager and the environment administrator need to jointly decide how the environment administrator is going to install the environment. There are two options: Install including creating a new cluster : The environment administrator who performs this install needs permissions to create clusters as well as service instances. The script will run as this user to create the cluster, create the service instances, and install the CI/CD tools in the cluster. Install into an existing cluster : The account manager creates the cluster and then grants the environment administrator access to it. The environment administrator who performs this install needs permissions to create service instances but not to create clusters. The script will run as the environment administrator to create the service instances and install the CI/CD tools in the existing cluster. Note A third option is to install an environment in Red Hat CodeReady Containers . For this option, you're not installing in IBM Cloud, so you don't have to configure your IBM Cloud account. However, you do have to install CRC. 3. Configure environment \u00b6 After installing the environment, before giving the users access to the environment, the environment administrator needs to finish configuring it. See Configure Environment for detailed instructions, which accomplish this: Configure RBAC security in the cluster Test opening the Developer Dashboard Complete setup of LogDNA Complete setup of SysDig Test the pipeline by deploying a first app Complete setup of Artifactory Complete setup of Argo CD Rerun pipeline and confirm that the app's Helm chart is added to Artifactory Set up a GitOps repo to validate ArgoCD setup and configuration Conclusion \u00b6 Having configured the account, installed the environment, and configured the environment, the administrators have now completed an end-to-end installation of a environment. It is ready for a development team to begin using for application development.","title":"Installation planning"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#plan-installation","text":"Overall process for installing a environment","title":"Plan Installation"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#background","text":"To understand how the process is performed, keep these concepts in mind.","title":"Background"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#roles","text":"An environment is installed and used by users acting in four roles: Account owner : The user who owns the account Account managers : Account owner or other users with account management permissions Environment administrators : Users in the account with permissions to create services in the environment's resource group Environment users : Users in the account with permissions to use existing services in the environment's resource group (e.g. developers, data scientists, etc.) The account owner must create the access group for account managers (see below). The account owner will: Create an ACCT-MGR-IAM-ADMIN access group using the acp-mgr script Add a functional ID, configured using the acp-iaas script, with API keys for the account managers Then, as described in Configure Account , the account managers can set up the resource groups and access groups needed to install and use the environments. For each environment, the account managers will: Create a resource group Create an access group named <resource_group>-ADMIN using the script acp-admin Create an access group named <resource_group>-USER using the script acp-user This diagram from [Resource Access Management]((../../resources/ibm-cloud/access-control.md#access-group-example){: target=_blank} shows the relationship of these access groups to a pair of development environments: The steps below help the account owner start configuring the account in this manner.","title":"Roles"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#set-up-account-managers","text":"The account owner may want to delegate the responsibilities for configuring the account for all of the development teams that want environments. Those users who are delegated to will need account management permissions. An easy way to manage who has these permissions is to create an access group with those policies and add those users to the group. An account only needs one of these account manager access groups, which will be used to support all environments in the account. In a new account, the account owner is the only user. Even after inviting other users, the account owner is initially the only user with the account management permissions needed to grant those permissions to other users. Therefore it is the account owner who must create the access group for account managers. !!!Note: The video in Resource Access Management > Configuration Process shows how to perform the steps in this process.","title":"Set up account managers"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#prepare-to-run-scripts","text":"We'll use some scripts in the steps below to help create access groups. Here, we'll download the scripts and prepare to run them. (If you want to use the console to manually configure the access groups, you can skip this step.) Clone the Git repository with the scripts. (This repo also has the scripts for installing the environment.) Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero The scripts need you to log into IBM Cloud first. In the terminal you'll use to run the scripts, log in to IBM Cloud. Log in to the IBM Cloud CLI ibmcloud login -a cloud.ibm.com -r <region>","title":"Prepare to run scripts"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#access-group-for-account-managers","text":"The account owner must create an access group to grant the necessary permissions for managing the account. Do this by running a script, or by using the console to manually perform the steps in the script. Also, add the account manager(s) to this group. Note IBM Cloud has multiple account management services , in addition to IAM: Billing, License and entitlement, Support center, etc. An easy way to grant access to these individually is to create an access group for administering each: ACCT-MGR-BILLING-ADMIN, ACCT-MGR-LICENSE-ADMIN, ACCT-MGR-SUPPORT-ADMIN. The group created below only has IAM capabilities, so a good name for it is ACCT-MGR-IAM-ADMIN. To create the access group for the account managers: Create a new access group , name it something like ACCT-MGR-IAM-ADMIN (all capital letters) (or name it after your account) Run the script ./terraform/scripts/acp-mgr.sh , which adds the necessary policies to the access group Add the account managers to the group The script adds policies that allow the user to: Create resource groups Invite users to the account Create access groups The script also adds the same policies that an environment administrator has. But whereas an environment administrator can only manage the resources in one resource group, an account manager can manage the resources in all resource groups. This gives the account manager the ability in any resource group to: Create clusters Manage the IBM Cloud Container Registry (used as the environment's image registry ) Create service instances","title":"Access group for account managers"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#functional-id-for-infrastructure-permissions","text":"Account managers need the permissions to create and manage IaaS resources required by a environment. Permissions for classic infrastructure (fka SoftLayer) cannot be added to an access group, only to a user. Rather than add these permissions to each account manager, create a functional ID and grant it the infrastructure permissions. The functional ID will own the API keys that the Kubernetes service needs to create clusters. As account managers are added to and removed from the account, the functional ID will always remain and always have the necessary infrastructure permissions. Set up the functional ID in the account: Create the functional ID, sign it up for an IBM Cloud account, and invite the user to this account Run the script ./terraform/scripts/acp-iaas.sh for the functional ID's user, which adds the necessary permissions to the user The script adds the classic infrastructure permissions needed to create and manage clusters: Create VLANs Create Kubernetes Service clusters (e.g. create virtual servers, storage, and networking) Manage Kubernetes Service clusters (e.g. add nodes) The script also adds the IAM permissions to: Run the command to reset the API key that the Kubernetes service will use By granting these infrastructure permissions to the functional ID and using it to create API keys, the account managers and environment administrators can create Kubernetes and OpenShift clusters without needing infrastructure permissions. Next, each account manager will need to use the functional ID to reset the API key that the Kubernetes service will use. By using the functional ID, the API key will be owned by the functional ID instead of by the account manager. The account managers need a way to log in as the functional ID without all of them sharing the ID's password or a single API key. Thus each account manager needs their own API key for the functional ID's account. Create API keys for the functional ID: Log into IBM Cloud as the functional ID (not as the account owner) Switch to this account that you're configuring For each account manager, create an API key , downloading each key to a file Give each account manager their API key file for the functional ID Users should not share these API key files with each other. When a user is no longer an account manager, remove them from the access group and delete their API key. To make a user an account manager, add them to the access group and create an API key for them.","title":"Functional ID for infrastructure permissions"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#process","text":"The overall process: Configure account -- An account manager configures the account so that environments can be installed Install environment -- An environment administrator runs the Cloud-Native Toolkit scripts to create a environment Configure environment -- The environment administrator finishes setting up the newly installed environment Once the environment is set up, the environment users can start using it to develop applications.","title":"Process"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#1-configure-account","text":"First, before installing an environment, an account manager needs to configure the IBM Cloud account for the environment. See Configure Account for detailed instructions, which accomplish this: Upgrade the image registry Prepare to run scripts Select a region and zone Select a pair of public/private VLANs Create a resource group Set the Kubernetes service API key Create an access group for environment administrators Create an access group for environment users Optionally, create a cluster for the environment","title":"1. Configure account"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#2-install-environment","text":"Once the account manager has configured the account for installing an environment, the account manager and the environment administrator need to jointly decide how the environment administrator is going to install the environment. There are two options: Install including creating a new cluster : The environment administrator who performs this install needs permissions to create clusters as well as service instances. The script will run as this user to create the cluster, create the service instances, and install the CI/CD tools in the cluster. Install into an existing cluster : The account manager creates the cluster and then grants the environment administrator access to it. The environment administrator who performs this install needs permissions to create service instances but not to create clusters. The script will run as the environment administrator to create the service instances and install the CI/CD tools in the existing cluster. Note A third option is to install an environment in Red Hat CodeReady Containers . For this option, you're not installing in IBM Cloud, so you don't have to configure your IBM Cloud account. However, you do have to install CRC.","title":"2. Install environment"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#3-configure-environment","text":"After installing the environment, before giving the users access to the environment, the environment administrator needs to finish configuring it. See Configure Environment for detailed instructions, which accomplish this: Configure RBAC security in the cluster Test opening the Developer Dashboard Complete setup of LogDNA Complete setup of SysDig Test the pipeline by deploying a first app Complete setup of Artifactory Complete setup of Argo CD Rerun pipeline and confirm that the app's Helm chart is added to Artifactory Set up a GitOps repo to validate ArgoCD setup and configuration","title":"3. Configure environment"},{"location":"adopting/best-practices/ibm-cloud-account/plan-installation/#conclusion","text":"Having configured the account, installed the environment, and configured the environment, the administrators have now completed an end-to-end installation of a environment. It is ready for a development team to begin using for application development.","title":"Conclusion"},{"location":"adopting/customize/customize/","text":"Customizing the Toolkit installation \u00b6 One of the ways of customizing the toolkit has already been covered in the Iteration Zero script installation section. Once the toolkit has been installed you may want to further customize the toolkit. Some of the areas you may want to customize are: add new starter kits to fit your development needs modify or add new pipelines and pipeline tasks replace or add new tools to your development environment Customize the dashboard","title":"Customization overview"},{"location":"adopting/customize/customize/#customizing-the-toolkit-installation","text":"One of the ways of customizing the toolkit has already been covered in the Iteration Zero script installation section. Once the toolkit has been installed you may want to further customize the toolkit. Some of the areas you may want to customize are: add new starter kits to fit your development needs modify or add new pipelines and pipeline tasks replace or add new tools to your development environment Customize the dashboard","title":"Customizing the Toolkit installation"},{"location":"adopting/customize/config-dashboard/dashboard/","text":"Configure Dashboard \u00b6 Customize the Developer Dashboard and the OpenShift console Customizing the Dashboard \u00b6 After the Dashboard has been installed into your development cluster, you can customize it to you team's needs. You can change the Title, Prefix, and Cloud Console links by adding the following environment variables to your deployment yaml. Customize the Dashboard's title from its default of \"IBM Cloud Garage\" The necessary set of environment variables isn't defined by default. You need to edit the YAML for the developer-dashboard deployment in the tools namespace to insert this set of variables. Edit the YAML for the /tools/deployments/developer-dashboard resource. In the spec.template.spec.containers section, in the resources for the container named developer-dashboard , add a new env resource to this container that defines these environment variables: env : - name : DASHBOARD_TITLE value : GSI Labs Sandbox - name : DASHBOARD_PREFIX value : IBM - name : CLOUD_TITLE value : Azure Console - name : CLOUD_URL value : https://azure.microsoft.com/en-us/ - name : LINKS_URL value : http://<url>/data/links.json Then fill in the values you want to use, such as the name of your team and company Note The CLOUD_TITLE , CLOUD_URL , and LINKS_URL aren't needed when the platform is IBM Cloud You can also tailor the list of content that is displayed in the Activation tab and the Starter Kits tab by creating your own version of the links.json JSON file and host that somewhere accessible to you cluster's network. Adding tools \u00b6 You can add additional tools to the Developer Dashboard and to the Tools menu on the OpenShift console. The tools in the cluster are added automatically, but the tools outside the cluster must be added manually. You can also add tools for the Cloud Paks that you install in the environment, for CodeReady Workspaces (if you've installed that in the environment), etc. Adding tools to the Dashboard \u00b6 Use the Cloud Native Toolkit CLI to add tools to the dashboard. Use this syntax to add a tool: igc tool-config --name <name of tool> --url <url of tool> These are tools that every Environment has but that are hosted outside of the cluster. To add these to the Dashboard, run these commands and provide the URLs: igc tool-config --name ir --url { url image registry } igc tool-config --name logdna --url { url to LogDNA instance } igc tool-config --name sysdig --url { url to Sysdig instance } igc tool-config --name github --url { url to git org } If your Environment includes the Cloud Paks with these tools, add them to your Dashboard: igc tool-config --name ta --url { url to the Transformation Advisor } igc tool-config --name mcm --url { url to IBM CP4MCM } igc tool-config --name integration --url { url to CP4I instance } If you've installed CodeReady Workspaces in your Environment, add it to your Dashboard: igc tool-config --name codeready --url { url to the CRW instance } This table lists the tools that can be displayed. Todo Verify this list is still correct Tool Name Name Parameter Description Pre-Configured GitLab gitlab IBM Cloud GitLab instance for the region you are using Yes Eclipse Che che Link to Eclipse Che instance No Jenkins jenkins If IKS configured by default Yes Pipeline pipeline If OCP configured by default Yes ArgoCD argocd Link to ArgoCD instance in cluster Yes Artifactory artifactory Link to Artifactory instance in cluster Yes SonarQube sonarqube Link to SonarQube instance in cluster Yes Pact pact Link to Pact Broker instance in cluster Yes Tekton tekton Link to Tekton Dashboard in cluster No Transformation Advisor ta Link to Transformation Advisor tool in cluster No Swagger Editor apieditor Link to Swagger Editor instance in cluster Yes CodeReady Workspaces codeready Link to CodeReady Workspaces instance in cluster No GitHub github Link to teams GitHub organization Yes Cloud Pak for Integration integration Link to main console for Cloud Pak for Integration No Cloud Pak for Multi Cloud Manager mcm Link to main console for Cloud Pak for Multi Cloud Manager No Cloud Pak for Data data Link to main console for Cloud Pak for Data No Cloud Pak for Automation automation Link to main console for Cloud Pak for Automation No Grafana grafana Link to Grafana in cluster No Prometheus prometheus Link to Prometheus in cluster No LogDNA logDNA Link to LogDNA service instance No Sysdig Sysdig Link to Sysdig service instance No Image Registry ir Link to Image Registry No Jaeger jaeger Link to Jaeger in cluster Yes Adding Tools to the OpenShift Console \u00b6 If the environment includes an OpenShift cluster, the Environment adds a Tools menu to the OpenShift console. The tools in the cluster are automatically added, but you need to add the tools outside of the cluster to specify their URLs. You can also extend the Tools menu to provide fast links to common tools you the development team will require. These tools links are common across the cluster. Edit the file called tools.yaml in the terraform/scripts folder. This file contains the Custom Resource Definitions (CRDs) required to configure the menu items. Add custom links for github , logdna , and sysdig , and save the file. Run the terraform/scripts/config-console-tools script to apply the settings in tools.yaml . To do so: Make sure you are logged into your cluster from the command line and run the script, specifying your cluster's ingress subdomain. To find the ingress subdomain, go to the cluster overview page in the IBM Cloud console; it's something like resource-group-NNN-XXX.region.containers.appdomain.cloud . ./config-console-tools { cluster ingress subdomain } Optionally, you can extend the list of tools to include links to other tools. For example, here are two links to the Cloud Pak for Multicloud Management and the Cloud Pak for Integration. --- apiVersion : console.openshift.io/v1 kind : ConsoleLink metadata : name : toolkit-mcm spec : applicationMenu : imageURL : https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/mcm section : Cloud Native Toolkit href : https://icp-console.gsi-learning-ocp43-7ec5d722a0ab3f463fdc90eeb94dbc70-0000.us-east.containers.appdomain.cloud/ location : ApplicationMenu text : Multi Cloud Manager --- apiVersion : console.openshift.io/v1 kind : ConsoleLink metadata : name : toolkit-integration spec : applicationMenu : imageURL : https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/integration section : Cloud Native Toolkit href : https://navigator-integration.gsi-ocp311-integration-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.us-east.containers.appdomain.cloud/ location : ApplicationMenu text : Integration Obtaining links to external tools \u00b6 To add external tools to the Dashboard and/or Tools menu, you need to know the links to those tools. Here's how to find those links. Image registry \u00b6 To get the URL for the image registry: In the IBM Cloud console, navigate to Kubernetes > Registry or OpenShift > Registry On the Registry page, select the Images tab That URL for the Images tab (or any of the Registry tabs) is the one to add to the tools lists LogDNA dashboard \u00b6 Get the URL for the LogDNA web UI in your environment (as explained in IBM Log Analysis with LogDNA: Viewing logs ) In the IBM Cloud dashboard, navigate to Observability > Logging Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. In the logging instance, the URL in the View LogDNA button is the one to add to the tools lists Sysdig dashboard \u00b6 Get the URL for the Sysdig web UI for your environment (as explained in Step 4: Launch the web UI ) In the IBM Cloud dashboard, navigate to Observability > Monitoring Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig In the monitoring instance, the URL in the View Sysdig button is the one to add to the tools lists","title":"Customizing the dashboard"},{"location":"adopting/customize/config-dashboard/dashboard/#configure-dashboard","text":"Customize the Developer Dashboard and the OpenShift console","title":"Configure Dashboard"},{"location":"adopting/customize/config-dashboard/dashboard/#customizing-the-dashboard","text":"After the Dashboard has been installed into your development cluster, you can customize it to you team's needs. You can change the Title, Prefix, and Cloud Console links by adding the following environment variables to your deployment yaml. Customize the Dashboard's title from its default of \"IBM Cloud Garage\" The necessary set of environment variables isn't defined by default. You need to edit the YAML for the developer-dashboard deployment in the tools namespace to insert this set of variables. Edit the YAML for the /tools/deployments/developer-dashboard resource. In the spec.template.spec.containers section, in the resources for the container named developer-dashboard , add a new env resource to this container that defines these environment variables: env : - name : DASHBOARD_TITLE value : GSI Labs Sandbox - name : DASHBOARD_PREFIX value : IBM - name : CLOUD_TITLE value : Azure Console - name : CLOUD_URL value : https://azure.microsoft.com/en-us/ - name : LINKS_URL value : http://<url>/data/links.json Then fill in the values you want to use, such as the name of your team and company Note The CLOUD_TITLE , CLOUD_URL , and LINKS_URL aren't needed when the platform is IBM Cloud You can also tailor the list of content that is displayed in the Activation tab and the Starter Kits tab by creating your own version of the links.json JSON file and host that somewhere accessible to you cluster's network.","title":"Customizing the Dashboard"},{"location":"adopting/customize/config-dashboard/dashboard/#adding-tools","text":"You can add additional tools to the Developer Dashboard and to the Tools menu on the OpenShift console. The tools in the cluster are added automatically, but the tools outside the cluster must be added manually. You can also add tools for the Cloud Paks that you install in the environment, for CodeReady Workspaces (if you've installed that in the environment), etc.","title":"Adding tools"},{"location":"adopting/customize/config-dashboard/dashboard/#adding-tools-to-the-dashboard","text":"Use the Cloud Native Toolkit CLI to add tools to the dashboard. Use this syntax to add a tool: igc tool-config --name <name of tool> --url <url of tool> These are tools that every Environment has but that are hosted outside of the cluster. To add these to the Dashboard, run these commands and provide the URLs: igc tool-config --name ir --url { url image registry } igc tool-config --name logdna --url { url to LogDNA instance } igc tool-config --name sysdig --url { url to Sysdig instance } igc tool-config --name github --url { url to git org } If your Environment includes the Cloud Paks with these tools, add them to your Dashboard: igc tool-config --name ta --url { url to the Transformation Advisor } igc tool-config --name mcm --url { url to IBM CP4MCM } igc tool-config --name integration --url { url to CP4I instance } If you've installed CodeReady Workspaces in your Environment, add it to your Dashboard: igc tool-config --name codeready --url { url to the CRW instance } This table lists the tools that can be displayed. Todo Verify this list is still correct Tool Name Name Parameter Description Pre-Configured GitLab gitlab IBM Cloud GitLab instance for the region you are using Yes Eclipse Che che Link to Eclipse Che instance No Jenkins jenkins If IKS configured by default Yes Pipeline pipeline If OCP configured by default Yes ArgoCD argocd Link to ArgoCD instance in cluster Yes Artifactory artifactory Link to Artifactory instance in cluster Yes SonarQube sonarqube Link to SonarQube instance in cluster Yes Pact pact Link to Pact Broker instance in cluster Yes Tekton tekton Link to Tekton Dashboard in cluster No Transformation Advisor ta Link to Transformation Advisor tool in cluster No Swagger Editor apieditor Link to Swagger Editor instance in cluster Yes CodeReady Workspaces codeready Link to CodeReady Workspaces instance in cluster No GitHub github Link to teams GitHub organization Yes Cloud Pak for Integration integration Link to main console for Cloud Pak for Integration No Cloud Pak for Multi Cloud Manager mcm Link to main console for Cloud Pak for Multi Cloud Manager No Cloud Pak for Data data Link to main console for Cloud Pak for Data No Cloud Pak for Automation automation Link to main console for Cloud Pak for Automation No Grafana grafana Link to Grafana in cluster No Prometheus prometheus Link to Prometheus in cluster No LogDNA logDNA Link to LogDNA service instance No Sysdig Sysdig Link to Sysdig service instance No Image Registry ir Link to Image Registry No Jaeger jaeger Link to Jaeger in cluster Yes","title":"Adding tools to the Dashboard"},{"location":"adopting/customize/config-dashboard/dashboard/#adding-tools-to-the-openshift-console","text":"If the environment includes an OpenShift cluster, the Environment adds a Tools menu to the OpenShift console. The tools in the cluster are automatically added, but you need to add the tools outside of the cluster to specify their URLs. You can also extend the Tools menu to provide fast links to common tools you the development team will require. These tools links are common across the cluster. Edit the file called tools.yaml in the terraform/scripts folder. This file contains the Custom Resource Definitions (CRDs) required to configure the menu items. Add custom links for github , logdna , and sysdig , and save the file. Run the terraform/scripts/config-console-tools script to apply the settings in tools.yaml . To do so: Make sure you are logged into your cluster from the command line and run the script, specifying your cluster's ingress subdomain. To find the ingress subdomain, go to the cluster overview page in the IBM Cloud console; it's something like resource-group-NNN-XXX.region.containers.appdomain.cloud . ./config-console-tools { cluster ingress subdomain } Optionally, you can extend the list of tools to include links to other tools. For example, here are two links to the Cloud Pak for Multicloud Management and the Cloud Pak for Integration. --- apiVersion : console.openshift.io/v1 kind : ConsoleLink metadata : name : toolkit-mcm spec : applicationMenu : imageURL : https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/mcm section : Cloud Native Toolkit href : https://icp-console.gsi-learning-ocp43-7ec5d722a0ab3f463fdc90eeb94dbc70-0000.us-east.containers.appdomain.cloud/ location : ApplicationMenu text : Multi Cloud Manager --- apiVersion : console.openshift.io/v1 kind : ConsoleLink metadata : name : toolkit-integration spec : applicationMenu : imageURL : https://dashboard-tools.#CLUSTER_INGRESS_URL/tools/icon/integration section : Cloud Native Toolkit href : https://navigator-integration.gsi-ocp311-integration-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.us-east.containers.appdomain.cloud/ location : ApplicationMenu text : Integration","title":"Adding Tools to the OpenShift Console"},{"location":"adopting/customize/config-dashboard/dashboard/#obtaining-links-to-external-tools","text":"To add external tools to the Dashboard and/or Tools menu, you need to know the links to those tools. Here's how to find those links.","title":"Obtaining links to external tools"},{"location":"adopting/customize/config-dashboard/dashboard/#image-registry","text":"To get the URL for the image registry: In the IBM Cloud console, navigate to Kubernetes > Registry or OpenShift > Registry On the Registry page, select the Images tab That URL for the Images tab (or any of the Registry tabs) is the one to add to the tools lists","title":"Image registry"},{"location":"adopting/customize/config-dashboard/dashboard/#logdna-dashboard","text":"Get the URL for the LogDNA web UI in your environment (as explained in IBM Log Analysis with LogDNA: Viewing logs ) In the IBM Cloud dashboard, navigate to Observability > Logging Find the logging instance named after your environment's cluster, such as showcase-dev-iks-logdna . To help find it, you can filter by your resource group. In the logging instance, the URL in the View LogDNA button is the one to add to the tools lists","title":"LogDNA dashboard"},{"location":"adopting/customize/config-dashboard/dashboard/#sysdig-dashboard","text":"Get the URL for the Sysdig web UI for your environment (as explained in Step 4: Launch the web UI ) In the IBM Cloud dashboard, navigate to Observability > Monitoring Find the monitoring instance named after your environment's cluster, such as showcase-dev-iks-sysdig In the monitoring instance, the URL in the View Sysdig button is the one to add to the tools lists","title":"Sysdig dashboard"},{"location":"adopting/setup/air-gapped-setup/","text":"Installing the toolkit in an air-gapped environment \u00b6 Todo Create this content, possible content to include: install assets + usage assets needed locally local Git server (Gogs / Gitea) + migrate required repositories local container repository + populate required images local operator catalog + populate required entries fix up dashboard / console links / pipeline to point to local resources how to handle TLS certificates (corporate provided / self-signed) local language specific resources (e.g. npm packages)","title":"Air-gapped setup"},{"location":"adopting/setup/air-gapped-setup/#installing-the-toolkit-in-an-air-gapped-environment","text":"Todo Create this content, possible content to include: install assets + usage assets needed locally local Git server (Gogs / Gitea) + migrate required repositories local container repository + populate required images local operator catalog + populate required entries fix up dashboard / console links / pipeline to point to local resources how to handle TLS certificates (corporate provided / self-signed) local language specific resources (e.g. npm packages)","title":"Installing the toolkit in an air-gapped environment"},{"location":"adopting/setup/configureEnvironment/","text":"Configure Environment \u00b6 Configure the toolkit development environment after installation Post installation Tools setup \u00b6 The following post installation setup is required. To get the developers enabled quickly, make sure you have completed at least post installation tasks. The customization is optional and down to development team needs. Configure the RBAC rules in the development cluster. This restricts who can change the parts of the cluster where the tools are installed. Run the RBAC script ./terraform/scripts/rbac.sh {ACCESS-GROUP} , where {ACCESS-GROUP} is the name of the user group (i.e. {resource_group}-USER ). Perform the steps in Configure Dashboard to add tiles and menu items for the tools that are external to the cluster: the Image Registry, GitHub, LogDNA, Sysdig, etc. If running on IBM Cloud: Check you log data if flowing into LogDNA Complete the setup of Sysdig and check the monitoring data is flowing Managing development assets is an important part of any Software Development Life Cycle (SDLC), The open source version of Artifactory has been installed into the cluster, This enables the full end to end process SDLC to be demonstrated. This version requires some manual configuration after its installation. These these instructions can be found here Artifactory Setup Complete the Argo CD Setup , this configures ArgoCD to use Artifactory as a Helm Repository Test opening the Developer Dashboard Run either or all of the CLI options to load the dashboard oc dashboard | kubectl dashboard | igc dashboard Test the pipeline by deploying a first app Set up a GitOps repo to validate ArgoCD setup and configuration Generated new passwords for SonarQube and update the secret in the tool namespace Test all the installed tools with new passwords Test end to end flow for an application and validate the content in each tool","title":"Post Install steps"},{"location":"adopting/setup/configureEnvironment/#configure-environment","text":"Configure the toolkit development environment after installation","title":"Configure Environment"},{"location":"adopting/setup/configureEnvironment/#post-installation-tools-setup","text":"The following post installation setup is required. To get the developers enabled quickly, make sure you have completed at least post installation tasks. The customization is optional and down to development team needs. Configure the RBAC rules in the development cluster. This restricts who can change the parts of the cluster where the tools are installed. Run the RBAC script ./terraform/scripts/rbac.sh {ACCESS-GROUP} , where {ACCESS-GROUP} is the name of the user group (i.e. {resource_group}-USER ). Perform the steps in Configure Dashboard to add tiles and menu items for the tools that are external to the cluster: the Image Registry, GitHub, LogDNA, Sysdig, etc. If running on IBM Cloud: Check you log data if flowing into LogDNA Complete the setup of Sysdig and check the monitoring data is flowing Managing development assets is an important part of any Software Development Life Cycle (SDLC), The open source version of Artifactory has been installed into the cluster, This enables the full end to end process SDLC to be demonstrated. This version requires some manual configuration after its installation. These these instructions can be found here Artifactory Setup Complete the Argo CD Setup , this configures ArgoCD to use Artifactory as a Helm Repository Test opening the Developer Dashboard Run either or all of the CLI options to load the dashboard oc dashboard | kubectl dashboard | igc dashboard Test the pipeline by deploying a first app Set up a GitOps repo to validate ArgoCD setup and configuration Generated new passwords for SonarQube and update the secret in the tool namespace Test all the installed tools with new passwords Test end to end flow for an application and validate the content in each tool","title":"Post installation Tools setup"},{"location":"adopting/setup/ibmcloud-iz-cluster/","text":"Provision an IBM Cloud cluster using Iteration Zero scripts \u00b6 Information This installation method will provision the cluster if it doesn't already exist Install a managed cluster and the Cloud Native Toolkit using Iteration Zero \u00b6 A. Download the Iteration Zero scripts \u00b6 Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git B. Configure the credentials \u00b6 In a terminal, change to the ibm-garage-iteration-zero cloned directory cd ibm-garage-iteration-zero Copy the credentials.template file to a file named credentials.properties cp credentials.template credentials.properties Note credentials.properties is already listed in .gitignore to prevent the private credentials from being committed to the git repository Update the value for the ibmcloud.api.key property in credentials.properties with your IBM Cloud CLI API key Note The API key should have been set up during prepare account . C. Configure the environment variables \u00b6 The settings for creating the Cloud-Native Toolkit on IBM Cloud are set in the environment-ibmcloud.tfvars file in the ./terraform/settings directory of the ibm-garage-iteration-zero repository. There are a number of values that can be applied in the file, some required and some optional. Consult with the following table to determine which values should be used: VPC Infrastructure Variable Required? Description eg. Value cluster_type yes The type of cluster into which the toolkit will be installed kubernetes , ocp3 , ocp4 or ocp44 cluster_exists yes Flag indicating if the cluster already exists. ( false means the cluster should be provisioned) false vpc_cluster yes Flag indicating that the cluster has been built on VPC infrastructure. Defaults to true true name_prefix no The prefix that should be applied for any resources that are provisioned. Defaults to {resource_group_name} dev-one cluster_name no The name of the cluster (If cluster_exists is set to true then this name should match an existing cluster). Defaults to {prefix_name}-cluster or {resource_group_name}-cluster dev-team-one-iks-117-vpc resource_group_name yes Existing resource group in the account where the cluster has been created dev-team-one region yes The region where the cluster has been/will be provisioned us-east , eu-gb , etc vpc_zone_names no A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if cluster_exists is set to false and vpc_cluster is set to true us-south-1 or us-east-1,us-east-2 provision_logdna no Flag indicating that a new instance of LogDNA should be provisioned. Defaults to false true or false logdna_name no The name of the LogDNA instance (If provision_logdna is set to false this value is used by the scripts to bind the existing LogDNA instance to the cluster) cntk-showcase-logdna logdna_region no The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster region . us-east provision_sysdig no Flag indicating that a new instance of Sysdig should be provisioned. Defaults to false true or false sysdig_name no The name of the Sysdig instance (If provision_sysdig is set to false this value is used by the scripts to bind the existing Sysdig instance to the cluster) cntk-showcase-sysdig sysdig_region no The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster region . us-east cos_name no The name of the Cloud Object Storage instance that will be used with the OCP cluster. registry_type no The type of the Container Registry that will be used with the cluster. Valid values are icr , ocp , other , or none . registry_namespace no The namespace that should be used in the IBM Container Registry. Defaults to {resource_group_name} dev-team-one-registry-2020 registry_host no The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" quay.io registry_user no The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" {username} registry_password no The password needed to access the image registry. This value is required if the registry_type is set to \"other\". {password} source_control_type no The type of source control system (github, gitlab, or none) github source_control_url no The url to the source control system https://github.com Update environment-ibmcloud.tfvars with the appropriate values for your installation. Particularly, be sure to set the following values in order to provision a VPC cluster: cluster_exists to false vpc_cluster to true Classic Infrastructure Variable Required? Description eg. Value cluster_type yes The type of cluster into which the toolkit will be installed kubernetes , ocp3 , ocp4 or ocp44 cluster_exists yes Flag indicating if the cluster already exists. ( false means the cluster should be provisioned) false vpc_cluster yes Flag indicating that the cluster has been built on VPC infrastructure. Defaults to true false name_prefix no The prefix that should be applied for any resources that are provisioned. Defaults to {resource_group_name} dev-one cluster_name no The name of the cluster (If cluster_exists is set to true then this name should match an existing cluster). Defaults to {prefix_name}-cluster or {resource_group_name}-cluster dev-team-one-iks-117-vpc resource_group_name yes Existing resource group in the account where the cluster has been created dev-team-one region yes The region where the cluster has been/will be provisioned us-east , eu-gb , etc provision_logdna no Flag indicating that a new instance of LogDNA should be provisioned. Defaults to false true or false logdna_name no The name of the LogDNA instance (If provision_logdna is set to false this value is used by the scripts to bind the existing LogDNA instance to the cluster) cntk-showcase-logdna logdna_region no The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster region . us-east provision_sysdig no Flag indicating that a new instance of Sysdig should be provisioned. Defaults to false true or false sysdig_name no The name of the Sysdig instance (If provision_sysdig is set to false this value is used by the scripts to bind the existing Sysdig instance to the cluster) cntk-showcase-sysdig sysdig_region no The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster region . us-east cos_name no The name of the Cloud Object Storage instance that will be used with the OCP cluster. registry_type no The type of the Container Registry that will be used with the cluster. Valid values are icr , ocp , other , or none . registry_namespace no The namespace that should be used in the IBM Container Registry. Defaults to {resource_group_name} dev-team-one-registry-2020 registry_host no The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" quay.io registry_user no The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" {username} registry_password no The password needed to access the image registry. This value is required if the registry_type is set to \"other\". {password} source_control_type no The type of source control system (github, gitlab, or none) github source_control_url no The url to the source control system https://github.com Configure the VLAN settings \u00b6 The vlan.tfvars file in terraform/settings contains properties that define the classic infrastructure configuration in order to provision a new cluster. Typical values look like this: shell script vlan_datacenter=\"dal10\" public_vlan_id=\"2366011\" private_vlan_id=\"2366012\" You must set all of these specifically for your cluster. Use the values provided by the account manager. vlan_datacenter -- The zone in the region in which the cluster worker nodes will be provisioned public_vlan_id -- The public VLAN that the cluster will use private_vlan_id -- The private VLAN that the cluster will use Optional: Generate the VLAN properties \u00b6 The IGC CLI can be used to generate these settings, to make the configuration as simple as possible. If your account has numerous VLANs and you want your cluster to use specific ones, then skip this step and provide the values by hand. This tool is for users who don't know what these required settings should be and want a simple way to gather reasonable defaults for their particular account. Log in using the Target the region where the cluster will be provisioned with the shell script ibmcloud target -r {region} Run the VLAN command shell script igc vlan Copy the output values from the CLI Command into your vlan.tfvars files D. (Optional) Customize the installed components \u00b6 The terraform/stages directory contains the default set of stages that define the modules that will be applied to the environment. The stages can be customized to change the makeup of the environment that is provisioned by either removing or adding stages from/to the terraform/stages directory. Note The stages occasionally have dependencies on other stages (e.g. most all depend on the cluster module, many depend on the namespace module, etc.) so be aware of those dependencies as you start making changes. Dependencies are reflected in the module.{stage name} references in the stage variable list. The terraform/stages/catalog directory contains some optional stages that are prep-configured and can be dropped into the terraform/stages directory. Other modules are available from the Garage Terraform Modules catalog and can be added as stages to the directory as well. Since this is Terraform, any other Terraform scripts and modules can be added to the terraform/stages directory as desired. E. Run the installation \u00b6 Open a terminal to the ibm-garage-iteration-zero directory Launch a Developer Tools Docker container from which the Terraform scripts will be run ./launch.sh This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts and exec shell into the running container. When the container starts it mounts the filesystem's ./terraform/ directory as /home/devops/src/ and loads the values from the credentials.properties file as environment variables. Apply the Terraform by running the provided runTerraform.sh script ./runTerraform.sh This script collects the values provided in the environment-ibmcloud.tfvars and the stages defined in the terraform/stages to build the Terraform workspace. Along the way it will prompt for a couple pieces of information. Type of installation: cluster This prompt can be skipped by providing --cluster as an argument to ./runTerraform.sh Handling of an old workspace (if applicable): keep or delete If you executed the script previously for the current cluster configuration and the workspace directory still exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if you want to use the state from the previous run as a starting point to either add or remove configuration. Delete the workspace if you want to start with a clean install of the Toolkit. This prompt can be skipped by providing --delete or --keep as an argument to ./runTerraform.sh Verify the installation configuration The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. This prompt can be skipped by providing --auto-approve as an argument to ./runTerraform.sh Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer) and the rest of the process takes about 30 minutes. Troubleshooting \u00b6 If you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state. If you find that some of the services have failed to create in the time allocated, try the following with Iteration zero: Manually delete the service instances in your resource group Re-run the runTerraform.sh script with the --delete argument to clean up the state ./runTerraform.sh --delete","title":"Install IBM Cloud cluster Iteration Zero"},{"location":"adopting/setup/ibmcloud-iz-cluster/#provision-an-ibm-cloud-cluster-using-iteration-zero-scripts","text":"Information This installation method will provision the cluster if it doesn't already exist","title":"Provision an IBM Cloud cluster using Iteration Zero scripts"},{"location":"adopting/setup/ibmcloud-iz-cluster/#install-a-managed-cluster-and-the-cloud-native-toolkit-using-iteration-zero","text":"","title":"Install a managed cluster and the Cloud Native Toolkit using Iteration Zero"},{"location":"adopting/setup/ibmcloud-iz-cluster/#a-download-the-iteration-zero-scripts","text":"Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git","title":"A. Download the Iteration Zero scripts"},{"location":"adopting/setup/ibmcloud-iz-cluster/#b-configure-the-credentials","text":"In a terminal, change to the ibm-garage-iteration-zero cloned directory cd ibm-garage-iteration-zero Copy the credentials.template file to a file named credentials.properties cp credentials.template credentials.properties Note credentials.properties is already listed in .gitignore to prevent the private credentials from being committed to the git repository Update the value for the ibmcloud.api.key property in credentials.properties with your IBM Cloud CLI API key Note The API key should have been set up during prepare account .","title":"B. Configure the credentials"},{"location":"adopting/setup/ibmcloud-iz-cluster/#c-configure-the-environment-variables","text":"The settings for creating the Cloud-Native Toolkit on IBM Cloud are set in the environment-ibmcloud.tfvars file in the ./terraform/settings directory of the ibm-garage-iteration-zero repository. There are a number of values that can be applied in the file, some required and some optional. Consult with the following table to determine which values should be used: VPC Infrastructure Variable Required? Description eg. Value cluster_type yes The type of cluster into which the toolkit will be installed kubernetes , ocp3 , ocp4 or ocp44 cluster_exists yes Flag indicating if the cluster already exists. ( false means the cluster should be provisioned) false vpc_cluster yes Flag indicating that the cluster has been built on VPC infrastructure. Defaults to true true name_prefix no The prefix that should be applied for any resources that are provisioned. Defaults to {resource_group_name} dev-one cluster_name no The name of the cluster (If cluster_exists is set to true then this name should match an existing cluster). Defaults to {prefix_name}-cluster or {resource_group_name}-cluster dev-team-one-iks-117-vpc resource_group_name yes Existing resource group in the account where the cluster has been created dev-team-one region yes The region where the cluster has been/will be provisioned us-east , eu-gb , etc vpc_zone_names no A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if cluster_exists is set to false and vpc_cluster is set to true us-south-1 or us-east-1,us-east-2 provision_logdna no Flag indicating that a new instance of LogDNA should be provisioned. Defaults to false true or false logdna_name no The name of the LogDNA instance (If provision_logdna is set to false this value is used by the scripts to bind the existing LogDNA instance to the cluster) cntk-showcase-logdna logdna_region no The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster region . us-east provision_sysdig no Flag indicating that a new instance of Sysdig should be provisioned. Defaults to false true or false sysdig_name no The name of the Sysdig instance (If provision_sysdig is set to false this value is used by the scripts to bind the existing Sysdig instance to the cluster) cntk-showcase-sysdig sysdig_region no The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster region . us-east cos_name no The name of the Cloud Object Storage instance that will be used with the OCP cluster. registry_type no The type of the Container Registry that will be used with the cluster. Valid values are icr , ocp , other , or none . registry_namespace no The namespace that should be used in the IBM Container Registry. Defaults to {resource_group_name} dev-team-one-registry-2020 registry_host no The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" quay.io registry_user no The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" {username} registry_password no The password needed to access the image registry. This value is required if the registry_type is set to \"other\". {password} source_control_type no The type of source control system (github, gitlab, or none) github source_control_url no The url to the source control system https://github.com Update environment-ibmcloud.tfvars with the appropriate values for your installation. Particularly, be sure to set the following values in order to provision a VPC cluster: cluster_exists to false vpc_cluster to true Classic Infrastructure Variable Required? Description eg. Value cluster_type yes The type of cluster into which the toolkit will be installed kubernetes , ocp3 , ocp4 or ocp44 cluster_exists yes Flag indicating if the cluster already exists. ( false means the cluster should be provisioned) false vpc_cluster yes Flag indicating that the cluster has been built on VPC infrastructure. Defaults to true false name_prefix no The prefix that should be applied for any resources that are provisioned. Defaults to {resource_group_name} dev-one cluster_name no The name of the cluster (If cluster_exists is set to true then this name should match an existing cluster). Defaults to {prefix_name}-cluster or {resource_group_name}-cluster dev-team-one-iks-117-vpc resource_group_name yes Existing resource group in the account where the cluster has been created dev-team-one region yes The region where the cluster has been/will be provisioned us-east , eu-gb , etc provision_logdna no Flag indicating that a new instance of LogDNA should be provisioned. Defaults to false true or false logdna_name no The name of the LogDNA instance (If provision_logdna is set to false this value is used by the scripts to bind the existing LogDNA instance to the cluster) cntk-showcase-logdna logdna_region no The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster region . us-east provision_sysdig no Flag indicating that a new instance of Sysdig should be provisioned. Defaults to false true or false sysdig_name no The name of the Sysdig instance (If provision_sysdig is set to false this value is used by the scripts to bind the existing Sysdig instance to the cluster) cntk-showcase-sysdig sysdig_region no The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster region . us-east cos_name no The name of the Cloud Object Storage instance that will be used with the OCP cluster. registry_type no The type of the Container Registry that will be used with the cluster. Valid values are icr , ocp , other , or none . registry_namespace no The namespace that should be used in the IBM Container Registry. Defaults to {resource_group_name} dev-team-one-registry-2020 registry_host no The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" quay.io registry_user no The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" {username} registry_password no The password needed to access the image registry. This value is required if the registry_type is set to \"other\". {password} source_control_type no The type of source control system (github, gitlab, or none) github source_control_url no The url to the source control system https://github.com","title":"C. Configure the environment variables"},{"location":"adopting/setup/ibmcloud-iz-cluster/#d-optional-customize-the-installed-components","text":"The terraform/stages directory contains the default set of stages that define the modules that will be applied to the environment. The stages can be customized to change the makeup of the environment that is provisioned by either removing or adding stages from/to the terraform/stages directory. Note The stages occasionally have dependencies on other stages (e.g. most all depend on the cluster module, many depend on the namespace module, etc.) so be aware of those dependencies as you start making changes. Dependencies are reflected in the module.{stage name} references in the stage variable list. The terraform/stages/catalog directory contains some optional stages that are prep-configured and can be dropped into the terraform/stages directory. Other modules are available from the Garage Terraform Modules catalog and can be added as stages to the directory as well. Since this is Terraform, any other Terraform scripts and modules can be added to the terraform/stages directory as desired.","title":"D. (Optional) Customize the installed components"},{"location":"adopting/setup/ibmcloud-iz-cluster/#e-run-the-installation","text":"Open a terminal to the ibm-garage-iteration-zero directory Launch a Developer Tools Docker container from which the Terraform scripts will be run ./launch.sh This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts and exec shell into the running container. When the container starts it mounts the filesystem's ./terraform/ directory as /home/devops/src/ and loads the values from the credentials.properties file as environment variables. Apply the Terraform by running the provided runTerraform.sh script ./runTerraform.sh This script collects the values provided in the environment-ibmcloud.tfvars and the stages defined in the terraform/stages to build the Terraform workspace. Along the way it will prompt for a couple pieces of information. Type of installation: cluster This prompt can be skipped by providing --cluster as an argument to ./runTerraform.sh Handling of an old workspace (if applicable): keep or delete If you executed the script previously for the current cluster configuration and the workspace directory still exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if you want to use the state from the previous run as a starting point to either add or remove configuration. Delete the workspace if you want to start with a clean install of the Toolkit. This prompt can be skipped by providing --delete or --keep as an argument to ./runTerraform.sh Verify the installation configuration The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. This prompt can be skipped by providing --auto-approve as an argument to ./runTerraform.sh Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer) and the rest of the process takes about 30 minutes.","title":"E. Run the installation"},{"location":"adopting/setup/ibmcloud-iz-cluster/#troubleshooting","text":"If you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state. If you find that some of the services have failed to create in the time allocated, try the following with Iteration zero: Manually delete the service instances in your resource group Re-run the runTerraform.sh script with the --delete argument to clean up the state ./runTerraform.sh --delete","title":"Troubleshooting"},{"location":"adopting/setup/ibmcloud-setup/","text":"Prepare an IBM Cloud account \u00b6 IBM CloudPrepare the account for an installation of Cloud-Native Toolkit environment Note: If you already have a cluster then you can jump to install the toolkit . If you will be provisioning a cluster outside of IBM Cloud then you can jump to provision the cluster . 1. Set up account managers access group \u00b6 The account owner creates this access group for account managers and adds the functional ID for managing API keys. See IBM Cloud account management for the overview of the roles involved. An account only needs one of these account manager access groups, which will be used to support all environments in the account. In a new account, the account owner is the only user. Even after inviting other users, the account owner is initially the only user with the account management permissions needed to grant those permissions to other users. Therefore, it is the account owner who must create the access group for account managers. Note The video in Resource Access Management > Configuration Process shows how to perform the steps in this process. A. Prepare to run scripts \u00b6 We'll use some scripts in the steps below to help create access groups. Before using them, the scripts need to be downloaded and the environment needs to be prepared before running them. i. Clone the Iteration Zero git repository \u00b6 The ibm-garage-iteration-zero repository contains a number of scripts that support the administrative tasks. It also has contains the Terraform scripts for installing the Cloud-Native Toolkit environment.) git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git cd ibm-garage-iteration-zero ii. Log in to IBM Cloud using the IBM Cloud CLI \u00b6 ibmcloud login -a cloud.ibm.com -r <region> B. Create the access group for account managers \u00b6 The account owner must create an access group to grant the necessary permissions for managing the account. A script has been provided to automate the configuration of the policies. Note IBM Cloud has multiple account management services , in addition to IAM: Billing, License and entitlement, Support center, etc. An easy way to grant access to these individually is to create an access group for administering each: ACCT-MGR-BILLING-ADMIN , ACCT-MGR-LICENSE-ADMIN , ACCT-MGR-SUPPORT-ADMIN . The group created below only has IAM capabilities, so we suggest ACCT-MGR-IAM-ADMIN for its name. To create the access group for the account managers: Create a new access group and name it something like ACCT-MGR-IAM-ADMIN or name it after your account (use all capital letters) Run ./terraform/scripts/acp-mgr.sh from the ibm-garage-iteration-zero repository to add the necessary policies to the access group ./terraform/scripts/acp-mgr.sh ACCT-MGR-IAM-ADMIN Add the users to the access group The ./terraform/scripts/acp-mgr.sh script adds policies that allow the user to: Create resource groups Invite users to the account Create access groups and manage access group membership Create clusters across all resources groups in the account Create service instances across all resource groups in the account Manage the IBM Cloud Container Registry (used as the environment's image registry) 2. Configure the account \u00b6 The account must provide a few resources that will be needed to install and use the Cloud-Native Toolkit environment: Upgrade the image registry Create a resource group Create a pair of access groups for the admin and users A. Prepare to run scripts \u00b6 We'll use some scripts in the steps below to help create access groups. Before using them, the scripts need to be downloaded and the environment needs to be prepared before running them. i. Clone the Iteration Zero git repository \u00b6 The ibm-garage-iteration-zero repository contains a number of scripts that support the administrative tasks. It also has contains the Terraform scripts for installing the Cloud-Native Toolkit environment.) git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git cd ibm-garage-iteration-zero ii. Log in to IBM Cloud using the IBM Cloud CLI \u00b6 ibmcloud login -a cloud.ibm.com -r <region> B. Upgrade the image registry \u00b6 Upgrade the service plan for the image registry so that is has unlimited capacity for images. ibmcloud cr plan-upgrade standard Note The steps below need to be repeated for each new environment: Each environment needs its own resource group and pair of access groups for administrators and users. Each environment will need its own cluster, whether it's created by an account manager or an environment administrator. C. Create the resource group \u00b6 Create or provide a resource group . The resource group should be given a name that clearly identifies the purpose and usage of the collection of resources, such as the development team, the project, the environment, and/or the application(s) implemented. For example, we give our resource groups names like mooc-team-one , garage-dev-tools , cntk-showcase , etc. Warning The resource group name should be 24 characters or fewer, and should conform to Kubernetes resource naming conventions - the name should be all lowercase letters, digits, and the separators should be dashes. (Unless otherwise specified, the installation scripts will name the cluster <resource-group>-cluster and a cluster name is limited to 32 characters.) D. Create an IBM Cloud API Key \u00b6 An API Key is used to authenticate to the IBM Cloud APIs. It is specific to a user within a particular account, meaning you need a different API key for each account you will be accessing. Note: If you will be using classic infrastructure and/or multiple account mangers follow these instructions instead to create a functional ID for the account. Create an API key and download each key to a file. Be sure to include a descriptive name of the APIKey. E. Attach the IBM Cloud API Key to the resource group \u00b6 To create resources in the resource group, the account will need API keys for the container service to create resources in the classic infrastructure . The API key is needed for each region and resource group. Use the API key from the previous step for these steps: Log into the IBM Cloud CLI using the functional ID key created by the account owner ibmcloud login --apikey @key_file_name Reset the API key for a given region for the container service: Setting up the API key to enable access to the infrastructure portfolio ibmcloud ks api-key reset --region <region> The list of existing API keys shows the new key named containers-kubernetes-key ; the description specifies the resource group and region F. Create access groups for the resource group \u00b6 For each resource group, create an access group for environment administrators and environment users . The access group can be configured manually or with a script provided in the ibm-cloud-iteration-zero repository. i. Create the access group for environment administrators \u00b6 Create a new access group named something like <resource_group>-ADMIN (use all capital letters) Run the script ./terraform/scripts/acp-admin.sh from the ibm-garage-iteration-zero repository to add the necessary policies to the access group ./terraform/scripts/acp-admin.sh { ACCESS_GROUP } { resource group } Add the environment administrator(s) to the group The script adds policies that allow the user to add resources to the resource group: Permission to create clusters Permission to manage the IBM Cloud Container Registry (used as the environment's image registry ) Permission to create service instances ii. Create the access group for environment users \u00b6 Create a new access group named something like <resource_group>-USER (use all capital letters) Run the script ./terraform/scripts/acp-user.sh from the ibm-garage-iteration-zero repository to add the necessary policies to the access group ./terraform/scripts/acp-user.sh { ACCESS_GROUP } { resource group } Add the users to the group The script adds policies that allow the user to use resources to the resource group: Access to the resource group Access to the cluster Access to the image registry Access to each of the services in the resource group 3. Create the Private Catalog \u00b6 A. Create the catalog \u00b6 Log in to the IBM Cloud Console Click Manage->Catalogs from the top menu Click on Create Catalog In the Create a catalog dialog, provide the following values: name: the name of the catalog, for example Team Catalog description: (optional) a brief description of the purpose of the catalog products: select Start with no products resource group: click Update to change the default resource group for the catalog Click Create to complete the catalog creation B. Register the Cloud-Native Toolkit tiles in the catalog \u00b6 Note The following instructions depend on the jq command, which is installed as part of the Cloud-Native Toolkit CLI Download create-catalog-offering.sh from the latest Iteration Zero release and make the file executable LATEST_RELEASE = $( curl -sL https://api.github.com/repos/cloud-native-toolkit/ibm-garage-iteration-zero/releases/latest | jq -r '.tag_name' ) curl -OL \"https://github.com/cloud-native-toolkit/ibm-garage-iteration-zero/releases/download/ ${ LATEST_RELEASE } /create-catalog-offering.sh\" chmod +x create-catalog-offering.sh Run the create-catalog-offering.sh scripts passing in the API Key and the name of the catalog that you created in the previous step ./create-catalog-offering.sh { API_KEY } \"Team Catalog\" 4. (Optional) Set up Classic Infrastructure \u00b6 Virtual Private Cloud infrastructure is recommended for use with clusters as it easier to manage with IAM and provides a superior environment. However, there are some situations where classic infrastructure is still required, particularly when using Cloud Paks due to some current storage limitations with the VPC infrastructure. When classic infrastructure will be used then the following additional configuration is required. For a more in depth look at the differences between Classic and Virtual Private Cloud infrastructure, please refer the the IBM Cloud documentation A. Create a functional ID for Classic Infrastructure permissions \u00b6 Account managers need the permissions to create and manage IaaS resources required by a environment. Permissions for classic infrastructure (formerly known as SoftLayer) cannot be added to an access group, only to a user. Rather than add these permissions to each account manager, create a functional ID and grant it the infrastructure permissions. The functional ID will own the API keys that the Kubernetes service needs to create clusters. As account managers are added to and removed from the account, the functional ID will always remain and always have the necessary infrastructure permissions. i. Set up the functional ID in the account \u00b6 Create the email account for the functional ID, sign it up for an IBM Cloud account, and invite the functional ID to this account. Run the script ./terraform/scripts/acp-iaas.sh from the ibm-garage-iteration-zero repository to grant the necessary permissions to the functional ID user. ./terraform/scripts/acp-iaas.sh { functional-id } The script adds the classic infrastructure permissions needed to create and manage clusters: Create VLANs Create Kubernetes Service clusters (e.g. create virtual servers, storage, and networking) Manage Kubernetes Service clusters (e.g. add nodes) The script also adds the IAM permissions to: Run the command to reset the API key that the Kubernetes service will use By granting these infrastructure permissions to the functional ID and using it to create API keys, the account managers and environment administrators can create Kubernetes and OpenShift clusters without needing infrastructure permissions. ii. Create API keys for the functional ID \u00b6 Each account manager will need to use an API Key owned by functional ID's to have the necessary permissions to provision the classic infrastructure for the cluster. For security reasons, each account manager needs their own API key for the functional ID's account. Log into IBM Cloud as the functional ID (not as the account owner) and switch to the appropriate account. For each account manager, create an API key and download each key to a file. Be sure to include the account manager's name in the name/description of the APIKey. Give each account manager their API key file for the functional ID Users should not share these API key files with each other. When a user is no longer an account manager, remove them from the access group and delete their API key. B. Public and private VLANs \u00b6 Create or provide a pair of public and private VLANs for the selected region and zone. These VLANs will implement the public and private networks in the Kubernetes or OpenShift cluster. Use following to find the available IBM Cloud locations and the available data centers in each region, such as dal10 or lon02 Note If your account already has a pair of VLANs for your desired region and zone, you can use those. Create and manage vlans \u00b6 Visit the VLANs page to list and manage the VLANs. Select Order VLAN to create a new public or private VLAN for use with the cluster. List the available vlans \u00b6 Use the IGC CLI 's igc vlan command to select two existing VLANs and generate the properties to use for the installation scripts igc vlan These links help explain how to find the VLANs an account has, create more, and how a cluster uses them to implement its network: Getting started with VLANs Understanding network basics of classic clusters Overview of classic networking in IBM Cloud Kubernetes Service List and create VLANs: Resources > Classic Infrastructure > IP Management > VLANs Next steps \u00b6 With these steps completed, the account manager will have configured the account so that the environment administrator can install the environment. As you move into the next step of installing into the environment that has just been prepared, be sure to record the following information: IBM Cloud API key Resource group name Region Data center (classic infrastructure only) Private VLAN id (classic infrastructure only) Public VLAN id (classic infrastructure only) Please now select which installation method to use: Catalog tile Iteration Zero scripts","title":"Configuring IBM Cloud based setup"},{"location":"adopting/setup/ibmcloud-setup/#prepare-an-ibm-cloud-account","text":"IBM CloudPrepare the account for an installation of Cloud-Native Toolkit environment Note: If you already have a cluster then you can jump to install the toolkit . If you will be provisioning a cluster outside of IBM Cloud then you can jump to provision the cluster .","title":"Prepare an IBM Cloud account"},{"location":"adopting/setup/ibmcloud-setup/#1-set-up-account-managers-access-group","text":"The account owner creates this access group for account managers and adds the functional ID for managing API keys. See IBM Cloud account management for the overview of the roles involved. An account only needs one of these account manager access groups, which will be used to support all environments in the account. In a new account, the account owner is the only user. Even after inviting other users, the account owner is initially the only user with the account management permissions needed to grant those permissions to other users. Therefore, it is the account owner who must create the access group for account managers. Note The video in Resource Access Management > Configuration Process shows how to perform the steps in this process.","title":"1. Set up account managers access group"},{"location":"adopting/setup/ibmcloud-setup/#a-prepare-to-run-scripts","text":"We'll use some scripts in the steps below to help create access groups. Before using them, the scripts need to be downloaded and the environment needs to be prepared before running them.","title":"A. Prepare to run scripts"},{"location":"adopting/setup/ibmcloud-setup/#b-create-the-access-group-for-account-managers","text":"The account owner must create an access group to grant the necessary permissions for managing the account. A script has been provided to automate the configuration of the policies. Note IBM Cloud has multiple account management services , in addition to IAM: Billing, License and entitlement, Support center, etc. An easy way to grant access to these individually is to create an access group for administering each: ACCT-MGR-BILLING-ADMIN , ACCT-MGR-LICENSE-ADMIN , ACCT-MGR-SUPPORT-ADMIN . The group created below only has IAM capabilities, so we suggest ACCT-MGR-IAM-ADMIN for its name. To create the access group for the account managers: Create a new access group and name it something like ACCT-MGR-IAM-ADMIN or name it after your account (use all capital letters) Run ./terraform/scripts/acp-mgr.sh from the ibm-garage-iteration-zero repository to add the necessary policies to the access group ./terraform/scripts/acp-mgr.sh ACCT-MGR-IAM-ADMIN Add the users to the access group The ./terraform/scripts/acp-mgr.sh script adds policies that allow the user to: Create resource groups Invite users to the account Create access groups and manage access group membership Create clusters across all resources groups in the account Create service instances across all resource groups in the account Manage the IBM Cloud Container Registry (used as the environment's image registry)","title":"B. Create the access group for account managers"},{"location":"adopting/setup/ibmcloud-setup/#2-configure-the-account","text":"The account must provide a few resources that will be needed to install and use the Cloud-Native Toolkit environment: Upgrade the image registry Create a resource group Create a pair of access groups for the admin and users","title":"2. Configure the account"},{"location":"adopting/setup/ibmcloud-setup/#a-prepare-to-run-scripts_1","text":"We'll use some scripts in the steps below to help create access groups. Before using them, the scripts need to be downloaded and the environment needs to be prepared before running them.","title":"A. Prepare to run scripts"},{"location":"adopting/setup/ibmcloud-setup/#b-upgrade-the-image-registry","text":"Upgrade the service plan for the image registry so that is has unlimited capacity for images. ibmcloud cr plan-upgrade standard Note The steps below need to be repeated for each new environment: Each environment needs its own resource group and pair of access groups for administrators and users. Each environment will need its own cluster, whether it's created by an account manager or an environment administrator.","title":"B. Upgrade the image registry"},{"location":"adopting/setup/ibmcloud-setup/#c-create-the-resource-group","text":"Create or provide a resource group . The resource group should be given a name that clearly identifies the purpose and usage of the collection of resources, such as the development team, the project, the environment, and/or the application(s) implemented. For example, we give our resource groups names like mooc-team-one , garage-dev-tools , cntk-showcase , etc. Warning The resource group name should be 24 characters or fewer, and should conform to Kubernetes resource naming conventions - the name should be all lowercase letters, digits, and the separators should be dashes. (Unless otherwise specified, the installation scripts will name the cluster <resource-group>-cluster and a cluster name is limited to 32 characters.)","title":"C. Create the resource group"},{"location":"adopting/setup/ibmcloud-setup/#d-create-an-ibm-cloud-api-key","text":"An API Key is used to authenticate to the IBM Cloud APIs. It is specific to a user within a particular account, meaning you need a different API key for each account you will be accessing. Note: If you will be using classic infrastructure and/or multiple account mangers follow these instructions instead to create a functional ID for the account. Create an API key and download each key to a file. Be sure to include a descriptive name of the APIKey.","title":"D. Create an IBM Cloud API Key"},{"location":"adopting/setup/ibmcloud-setup/#e-attach-the-ibm-cloud-api-key-to-the-resource-group","text":"To create resources in the resource group, the account will need API keys for the container service to create resources in the classic infrastructure . The API key is needed for each region and resource group. Use the API key from the previous step for these steps: Log into the IBM Cloud CLI using the functional ID key created by the account owner ibmcloud login --apikey @key_file_name Reset the API key for a given region for the container service: Setting up the API key to enable access to the infrastructure portfolio ibmcloud ks api-key reset --region <region> The list of existing API keys shows the new key named containers-kubernetes-key ; the description specifies the resource group and region","title":"E. Attach the IBM Cloud API Key to the resource group"},{"location":"adopting/setup/ibmcloud-setup/#f-create-access-groups-for-the-resource-group","text":"For each resource group, create an access group for environment administrators and environment users . The access group can be configured manually or with a script provided in the ibm-cloud-iteration-zero repository.","title":"F. Create access groups for the resource group"},{"location":"adopting/setup/ibmcloud-setup/#3-create-the-private-catalog","text":"","title":"3. Create the Private Catalog"},{"location":"adopting/setup/ibmcloud-setup/#a-create-the-catalog","text":"Log in to the IBM Cloud Console Click Manage->Catalogs from the top menu Click on Create Catalog In the Create a catalog dialog, provide the following values: name: the name of the catalog, for example Team Catalog description: (optional) a brief description of the purpose of the catalog products: select Start with no products resource group: click Update to change the default resource group for the catalog Click Create to complete the catalog creation","title":"A. Create the catalog"},{"location":"adopting/setup/ibmcloud-setup/#b-register-the-cloud-native-toolkit-tiles-in-the-catalog","text":"Note The following instructions depend on the jq command, which is installed as part of the Cloud-Native Toolkit CLI Download create-catalog-offering.sh from the latest Iteration Zero release and make the file executable LATEST_RELEASE = $( curl -sL https://api.github.com/repos/cloud-native-toolkit/ibm-garage-iteration-zero/releases/latest | jq -r '.tag_name' ) curl -OL \"https://github.com/cloud-native-toolkit/ibm-garage-iteration-zero/releases/download/ ${ LATEST_RELEASE } /create-catalog-offering.sh\" chmod +x create-catalog-offering.sh Run the create-catalog-offering.sh scripts passing in the API Key and the name of the catalog that you created in the previous step ./create-catalog-offering.sh { API_KEY } \"Team Catalog\"","title":"B. Register the Cloud-Native Toolkit tiles in the catalog"},{"location":"adopting/setup/ibmcloud-setup/#4-optional-set-up-classic-infrastructure","text":"Virtual Private Cloud infrastructure is recommended for use with clusters as it easier to manage with IAM and provides a superior environment. However, there are some situations where classic infrastructure is still required, particularly when using Cloud Paks due to some current storage limitations with the VPC infrastructure. When classic infrastructure will be used then the following additional configuration is required. For a more in depth look at the differences between Classic and Virtual Private Cloud infrastructure, please refer the the IBM Cloud documentation","title":"4. (Optional) Set up Classic Infrastructure"},{"location":"adopting/setup/ibmcloud-setup/#a-create-a-functional-id-for-classic-infrastructure-permissions","text":"Account managers need the permissions to create and manage IaaS resources required by a environment. Permissions for classic infrastructure (formerly known as SoftLayer) cannot be added to an access group, only to a user. Rather than add these permissions to each account manager, create a functional ID and grant it the infrastructure permissions. The functional ID will own the API keys that the Kubernetes service needs to create clusters. As account managers are added to and removed from the account, the functional ID will always remain and always have the necessary infrastructure permissions.","title":"A. Create a functional ID for Classic Infrastructure permissions"},{"location":"adopting/setup/ibmcloud-setup/#b-public-and-private-vlans","text":"Create or provide a pair of public and private VLANs for the selected region and zone. These VLANs will implement the public and private networks in the Kubernetes or OpenShift cluster. Use following to find the available IBM Cloud locations and the available data centers in each region, such as dal10 or lon02 Note If your account already has a pair of VLANs for your desired region and zone, you can use those.","title":"B. Public and private VLANs"},{"location":"adopting/setup/ibmcloud-setup/#next-steps","text":"With these steps completed, the account manager will have configured the account so that the environment administrator can install the environment. As you move into the next step of installing into the environment that has just been prepared, be sure to record the following information: IBM Cloud API key Resource group name Region Data center (classic infrastructure only) Private VLAN id (classic infrastructure only) Public VLAN id (classic infrastructure only) Please now select which installation method to use: Catalog tile Iteration Zero scripts","title":"Next steps"},{"location":"adopting/setup/ibmcloud-tile-cluster/","text":"Provision an IBM Cloud cluster using private catalog \u00b6 This method will install a managed cluster and the Cloud-Native Toolkit on IBM Cloud using the private catalog tiles installed in the previous step. Note These steps assume the private catalog has been created and populated with the Cloud-Native Toolkit tiles during the prepare the account steps. Select the preferred environment for the cluster to run in, VPC or Classic infrastructure. Virtual Private Cloud Log in to the IBM Cloud Console. Select Catalog from the top menu. From the side menu, select your catalog from the drop-down list (e.g. Team Catalog ). ( IBM Cloud catalog should be selected initially.) Click Private on the side menu to see the private catalog entries Click on the 220. Cloud-Native VPC cluster tile Enter values for the variables list provided. Variable Description eg. Value ibmcloud_api_key The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation {guid API key from Console} resource_group_name The existing resource group in the account where the cluster will be created dev-team-one region The region where the cluster will be provisioned. us-east , eu-gb , etc cluster_name The name of the cluster that will be provisioned. dev-team-one-iks-117-vpc vpc_zone_names A comma-separated list of the VPC zones that should be used for worker nodes. us-south-1 or us-east-1,us-east-2 cluster_type The type of cluster into which the toolkit will be installed. The default is OpenShift 4.5 . kubernetes , ocp3 , ocp4 , ocp44 , or ocp45 flavor The flavor of machine that should be provisioned for each worker. Defaults to mx2.4x32 . mx2.4x32 cluster_worker_count The number of worker nodes that should be provisioned for each zone. Defaults to 3 3 cluster_provision_cos Flag indicating that a new Object Storage instance should be provisioned. Defaults to true true or false cos_name The name of the Object Storage instance (If cluster_provision_cos is set to true this value is required cntk-showcase-cos Check the box to accept the Apache 2 license for the tile. Click Install to start the install process This will kick off the installation of the Cloud-Native Toolkit using an IBM Cloud Private Catalog Tile. The progress can be reviewed from the Schematics entry Classic Infrastructure Log in to the IBM Cloud Console. Select Catalog from the top menu. From the side menu, select your catalog from the drop-down list (e.g. Team Catalog ). ( IBM Cloud catalog should be selected initially.) Click Private on the side menu to see the private catalog entries Click on the 221. Cloud-Native Classic cluster tile Enter values for the variables list provided. Variable Description eg. Value ibmcloud_api_key The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation {guid API key from Console} resource_group_name The existing resource group in the account where the cluster will be created dev-team-one region The region where the cluster will be provisioned. us-east , eu-gb , etc cluster_name The name of the cluster that will be provisioned. dev-team-one-iks-117-vpc private_vlan_id The id of an existing private VLAN. public_vlan_id The id of an existing public VLAN. vlan_datacenter The VLAN datacenter where the cluster will be provisioned. cluster_type The type of cluster into which the toolkit will be installed. The default is OpenShift 4.5 . kubernetes , ocp3 , ocp4 , ocp44 , or ocp45 flavor The flavor of machine that should be provisioned for each worker. Defaults to m3c.4x32 . m3c.4x32 cluster_worker_count The number of worker nodes that should be provisioned for each zone. Defaults to 3 3 Check the box to accept the Apache 2 license for the tile. Click Install to start the install process This will kick off the installation of the Cloud-Native Toolkit using an IBM Cloud Private Catalog Tile. The progress can be reviewed from the Schematics entry Troubleshooting \u00b6 If you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state. If you find that some of the services have failed to create in the time allocated, try the following with Iteration zero: Manually delete the service instances in your resource group Re-run the runTerraform.sh script with the --delete argument to clean up the state ./runTerraform.sh --delete","title":"Install IBM Cloud cluster from catalog"},{"location":"adopting/setup/ibmcloud-tile-cluster/#provision-an-ibm-cloud-cluster-using-private-catalog","text":"This method will install a managed cluster and the Cloud-Native Toolkit on IBM Cloud using the private catalog tiles installed in the previous step. Note These steps assume the private catalog has been created and populated with the Cloud-Native Toolkit tiles during the prepare the account steps. Select the preferred environment for the cluster to run in, VPC or Classic infrastructure. Virtual Private Cloud Log in to the IBM Cloud Console. Select Catalog from the top menu. From the side menu, select your catalog from the drop-down list (e.g. Team Catalog ). ( IBM Cloud catalog should be selected initially.) Click Private on the side menu to see the private catalog entries Click on the 220. Cloud-Native VPC cluster tile Enter values for the variables list provided. Variable Description eg. Value ibmcloud_api_key The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation {guid API key from Console} resource_group_name The existing resource group in the account where the cluster will be created dev-team-one region The region where the cluster will be provisioned. us-east , eu-gb , etc cluster_name The name of the cluster that will be provisioned. dev-team-one-iks-117-vpc vpc_zone_names A comma-separated list of the VPC zones that should be used for worker nodes. us-south-1 or us-east-1,us-east-2 cluster_type The type of cluster into which the toolkit will be installed. The default is OpenShift 4.5 . kubernetes , ocp3 , ocp4 , ocp44 , or ocp45 flavor The flavor of machine that should be provisioned for each worker. Defaults to mx2.4x32 . mx2.4x32 cluster_worker_count The number of worker nodes that should be provisioned for each zone. Defaults to 3 3 cluster_provision_cos Flag indicating that a new Object Storage instance should be provisioned. Defaults to true true or false cos_name The name of the Object Storage instance (If cluster_provision_cos is set to true this value is required cntk-showcase-cos Check the box to accept the Apache 2 license for the tile. Click Install to start the install process This will kick off the installation of the Cloud-Native Toolkit using an IBM Cloud Private Catalog Tile. The progress can be reviewed from the Schematics entry Classic Infrastructure Log in to the IBM Cloud Console. Select Catalog from the top menu. From the side menu, select your catalog from the drop-down list (e.g. Team Catalog ). ( IBM Cloud catalog should be selected initially.) Click Private on the side menu to see the private catalog entries Click on the 221. Cloud-Native Classic cluster tile Enter values for the variables list provided. Variable Description eg. Value ibmcloud_api_key The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation {guid API key from Console} resource_group_name The existing resource group in the account where the cluster will be created dev-team-one region The region where the cluster will be provisioned. us-east , eu-gb , etc cluster_name The name of the cluster that will be provisioned. dev-team-one-iks-117-vpc private_vlan_id The id of an existing private VLAN. public_vlan_id The id of an existing public VLAN. vlan_datacenter The VLAN datacenter where the cluster will be provisioned. cluster_type The type of cluster into which the toolkit will be installed. The default is OpenShift 4.5 . kubernetes , ocp3 , ocp4 , ocp44 , or ocp45 flavor The flavor of machine that should be provisioned for each worker. Defaults to m3c.4x32 . m3c.4x32 cluster_worker_count The number of worker nodes that should be provisioned for each zone. Defaults to 3 3 Check the box to accept the Apache 2 license for the tile. Click Install to start the install process This will kick off the installation of the Cloud-Native Toolkit using an IBM Cloud Private Catalog Tile. The progress can be reviewed from the Schematics entry","title":"Provision an IBM Cloud cluster using private catalog"},{"location":"adopting/setup/ibmcloud-tile-cluster/#troubleshooting","text":"If you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state. If you find that some of the services have failed to create in the time allocated, try the following with Iteration zero: Manually delete the service instances in your resource group Re-run the runTerraform.sh script with the --delete argument to clean up the state ./runTerraform.sh --delete","title":"Troubleshooting"},{"location":"adopting/setup/install-crc/","text":"Install into Red Hat CodeReady Containers \u00b6 Warning You may not be able to run CodeReady Containers (CRC) if you use a VPN solution to access a corporate network. CRC needs to be able to manage the network configuration of you laptop or workstation, but some VPN applications lock the network configuration, so may block CRC operation. You may be able to run CRC in a virtual machine on your system to overcome this issue. Prerequisites \u00b6 Prepare to install developer tools into Red Hat CodeReady Containers on your laptop The IBM Garage for Cloud Developer Tools facilitate development and deployment of cloud-native applications. They can be hosted in any Kubernetes or OpenShift cluster, including the Red Hat CodeReady Containers local OpenShift environment. These instructions help you install Red Hat CodeReady Containers and explain how to configure and run the Terraform infrastructure-as-code (IasC) scripts to install the Developer Tools into that CodeReady Containers install. Information Red Hat CodeReady Containers (CRC) is based on OpenShift and the current installation of the Cloud-Native Toolkit only installs the RedHat Pipelines (Tekton) operator, Jenkins is not installed. Read the Tekton Pipelines and Tasks Guide to understand how to deploy your app into CRC using Tekton. The following prerequisites are required to support installing CodeReady Containers: CodeReady Containers Minimum system requirements A Red Hat account is required The following prerequisites are required before following the setup instructions: Install the Prerequisites listed before continuing Install CRC \u00b6 Download CodeReady Containers (CRC) and install it Install and configure CRC as described in Install on Laptop Remember to take a copy of Pull Secret CRC executable : Copy the crc binary to your $PATH: From the directory where you unzipped the download: cp crc /usr/local/bin Follow these steps to complete the installation: Setup : Run the following command from a terminal session: crc setup Add memory : By default, the CRC VM is set to only use 8 GB of RAM. The more RAM you can give it, the better. To set the CRC VM to 10 GB of RAM, do this: ``shell crc config set memory 10240 ``` Start the local CRC Cluster Run the crc start command in a terminal window: crc start During the start process, you will be prompted for your pull secret. Copy and paste it into the terminal window. Wait about 5 minutes for the VM initialization to complete. When complete, the kubeadmin password will be displayed. Make note of this password because you will need it to log into the console. Open the Cluster Admin Console crc console Open a web browser and go to the OpenShift console Login with user id kubeadmin and the password that was displayed after the crc start had completed. Other useful links: Getting started with Red Hat CodeReady Containers, Section 1.5 Installing CodeReady Containers . Download Toolkit Iteration Zero \u00b6 Obtain the Iteration Zerp scripts that will install the Toolkit into CodeReady Containers Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero API keys \u00b6 Configure the keys the CLI uses to authenticate API keys are not needed to connect to CRC, but the file must still exist. Inside the iteration-zero-ibmcloud folder, copy credentials.template to a file named credentials.properties cd iteration-zero-ibmcloud cp credentials.template credentials.properties Edit the credentials.properties file and set the ibmcloud.api.key property to the admin password displayed in the terminal when CRC was started. Configuration \u00b6 Configure the properties describing the environment The settings for installing the Developer Tools go in a single property file in the ./terraform/settings directory: environment.tfvars -- Properties for installing the Developer Tools Environment variables \u00b6 Use the environment.tfvars properties to configure the installation for the Development Tools. Set the following properties so they match below, all the other properties will be ignored: # The type of cluster that will be created/used (kubernetes, openshift, or crc) cluster_type=\"crc\" # Flag indicating if we are using an existing cluster or creating a new one cluster_exists=\"true\" # Enter any value for the resource group name resource_group_name=\"crc-resource-group\" # Flag indicating if we are using an existing postgres server or creating a new one postgres_server_exists=\"false\" Run Iteration Zero \u00b6 Run Terraform to install the tools into the CRC environment Having configured the credentials.properties , environment.tfvars properties files, we are now ready to kick off the installation. Launch a Developer Tools Docker container . Run the following command to run the Docker container: ./launch.sh For more information on the Developer Tools Image see the following guide: This will install the Cloud Garage Tools Docker image and exec shell into the running container. The container will mount the filesystem's ./terraform/ directory as /home/devops/src/ . Once the Docker container has started and the script has exec shelled into it, you will see an IBM Garage banner. This will help you identify you are running inside the Docker image that has just mounted your file system. The supplied Terraform scripts are ready to run using the settings in the properties files. You optionally can extend or modify the scripts and tailor them for your project's specific needs. From inside the terminal/container run this script: ./runTerraform.sh This script will setup the Developer Tools in the CRC environment. The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. The existing cluster's contents will be cleaned up to prepare for the terraform process. Any resources in the tools , dev , test , and staging namespaces/projects will be deleted. Installing the tools into an existing cluster takes about 20 minutes. Success You should now have your environment fully provisioned and configured. Enjoy! Finish \u00b6 Once the Terraform scripts have finished, you can see the resources that the scripts created. To see this: Open the OpenShift web console. You should see: Namespaces: tools , dev , test , and staging Deployments in the tools namespace: catalyst-dashboard , jenkins , etc. Possible issues \u00b6 If you find that that the Terraform provisioning has failed, try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state.","title":"Install CodeReady Containers"},{"location":"adopting/setup/install-crc/#install-into-red-hat-codeready-containers","text":"Warning You may not be able to run CodeReady Containers (CRC) if you use a VPN solution to access a corporate network. CRC needs to be able to manage the network configuration of you laptop or workstation, but some VPN applications lock the network configuration, so may block CRC operation. You may be able to run CRC in a virtual machine on your system to overcome this issue.","title":"Install into Red Hat CodeReady Containers"},{"location":"adopting/setup/install-crc/#prerequisites","text":"Prepare to install developer tools into Red Hat CodeReady Containers on your laptop The IBM Garage for Cloud Developer Tools facilitate development and deployment of cloud-native applications. They can be hosted in any Kubernetes or OpenShift cluster, including the Red Hat CodeReady Containers local OpenShift environment. These instructions help you install Red Hat CodeReady Containers and explain how to configure and run the Terraform infrastructure-as-code (IasC) scripts to install the Developer Tools into that CodeReady Containers install. Information Red Hat CodeReady Containers (CRC) is based on OpenShift and the current installation of the Cloud-Native Toolkit only installs the RedHat Pipelines (Tekton) operator, Jenkins is not installed. Read the Tekton Pipelines and Tasks Guide to understand how to deploy your app into CRC using Tekton. The following prerequisites are required to support installing CodeReady Containers: CodeReady Containers Minimum system requirements A Red Hat account is required The following prerequisites are required before following the setup instructions: Install the Prerequisites listed before continuing","title":"Prerequisites"},{"location":"adopting/setup/install-crc/#install-crc","text":"Download CodeReady Containers (CRC) and install it Install and configure CRC as described in Install on Laptop Remember to take a copy of Pull Secret CRC executable : Copy the crc binary to your $PATH: From the directory where you unzipped the download: cp crc /usr/local/bin Follow these steps to complete the installation: Setup : Run the following command from a terminal session: crc setup Add memory : By default, the CRC VM is set to only use 8 GB of RAM. The more RAM you can give it, the better. To set the CRC VM to 10 GB of RAM, do this: ``shell crc config set memory 10240 ``` Start the local CRC Cluster Run the crc start command in a terminal window: crc start During the start process, you will be prompted for your pull secret. Copy and paste it into the terminal window. Wait about 5 minutes for the VM initialization to complete. When complete, the kubeadmin password will be displayed. Make note of this password because you will need it to log into the console. Open the Cluster Admin Console crc console Open a web browser and go to the OpenShift console Login with user id kubeadmin and the password that was displayed after the crc start had completed. Other useful links: Getting started with Red Hat CodeReady Containers, Section 1.5 Installing CodeReady Containers .","title":"Install CRC"},{"location":"adopting/setup/install-crc/#download-toolkit-iteration-zero","text":"Obtain the Iteration Zerp scripts that will install the Toolkit into CodeReady Containers Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git Switch to the cloned directory cd ibm-garage-iteration-zero","title":"Download Toolkit Iteration Zero"},{"location":"adopting/setup/install-crc/#api-keys","text":"Configure the keys the CLI uses to authenticate API keys are not needed to connect to CRC, but the file must still exist. Inside the iteration-zero-ibmcloud folder, copy credentials.template to a file named credentials.properties cd iteration-zero-ibmcloud cp credentials.template credentials.properties Edit the credentials.properties file and set the ibmcloud.api.key property to the admin password displayed in the terminal when CRC was started.","title":"API keys"},{"location":"adopting/setup/install-crc/#configuration","text":"Configure the properties describing the environment The settings for installing the Developer Tools go in a single property file in the ./terraform/settings directory: environment.tfvars -- Properties for installing the Developer Tools","title":"Configuration"},{"location":"adopting/setup/install-crc/#environment-variables","text":"Use the environment.tfvars properties to configure the installation for the Development Tools. Set the following properties so they match below, all the other properties will be ignored: # The type of cluster that will be created/used (kubernetes, openshift, or crc) cluster_type=\"crc\" # Flag indicating if we are using an existing cluster or creating a new one cluster_exists=\"true\" # Enter any value for the resource group name resource_group_name=\"crc-resource-group\" # Flag indicating if we are using an existing postgres server or creating a new one postgres_server_exists=\"false\"","title":"Environment variables"},{"location":"adopting/setup/install-crc/#run-iteration-zero","text":"Run Terraform to install the tools into the CRC environment Having configured the credentials.properties , environment.tfvars properties files, we are now ready to kick off the installation. Launch a Developer Tools Docker container . Run the following command to run the Docker container: ./launch.sh For more information on the Developer Tools Image see the following guide: This will install the Cloud Garage Tools Docker image and exec shell into the running container. The container will mount the filesystem's ./terraform/ directory as /home/devops/src/ . Once the Docker container has started and the script has exec shelled into it, you will see an IBM Garage banner. This will help you identify you are running inside the Docker image that has just mounted your file system. The supplied Terraform scripts are ready to run using the settings in the properties files. You optionally can extend or modify the scripts and tailor them for your project's specific needs. From inside the terminal/container run this script: ./runTerraform.sh This script will setup the Developer Tools in the CRC environment. The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. The existing cluster's contents will be cleaned up to prepare for the terraform process. Any resources in the tools , dev , test , and staging namespaces/projects will be deleted. Installing the tools into an existing cluster takes about 20 minutes. Success You should now have your environment fully provisioned and configured. Enjoy!","title":"Run Iteration Zero"},{"location":"adopting/setup/install-crc/#finish","text":"Once the Terraform scripts have finished, you can see the resources that the scripts created. To see this: Open the OpenShift web console. You should see: Namespaces: tools , dev , test , and staging Deployments in the tools namespace: catalyst-dashboard , jenkins , etc.","title":"Finish"},{"location":"adopting/setup/install-crc/#possible-issues","text":"If you find that that the Terraform provisioning has failed, try re-running the runTerraform.sh script again. The state will be saved and Terraform will try and apply the configuration to match the desired end state.","title":"Possible issues"},{"location":"adopting/setup/installing/","text":"Installing the Cloud-Native Toolkit \u00b6 Initially the Fast-start installation method is the recommended way to install the Cloud-Native Toolkit, as this is largely an automatic install with minimal choices required that creates a default configuration suitable for learning about cloud native development. When you want to adopt cloud native development within an enterprise development team, then you may want more control over the installation and configuration of the toolkit. This section outlines more advanced options available for installing the toolkit. IBM Cloud Private Catalog : This provides a mechanism to easily create and delete instances of the Cloud-Native Toolkit on clusters within the IBM Cloud using the catalog, as you do with other service on the IBM Cloud. Iteration-Zero : This is a command line based tool that gives you full control over customizing how the toolkit is installed and what components you want deployed as part of the toolkit. Iteration-Zero is used by the other install methods, but you can also drive it directly from the command line. When you want to install your development cluster and the Cloud-Native Toolkit, there are a number of choices you need to make, including: what kubernetes distribution do you want your development cluster to use? how you want to apply access control to development resources? can you use public repositories, registries and source control systems or do you need to host your own? what monitoring and logging tools will you use? do you want to use the default install of the toolkit or customize it to use a different tools? where will you locate the development environment, on a public or private cloud? These topics are discussed in the adopting best practices section. The recommended development cluster for the Cloud-Native Toolkit is using RedHat OpenShift on IBM Cloud using the Virtual Private Cloud Infrastructure. If you will be using the IBM Cloud to host your development cluster, then proceed to the next section to configure your IBM Cloud account . If you plan to deploy to a different cloud infrastructure, you should proceed to the provision a cluster section.","title":"Installation options"},{"location":"adopting/setup/installing/#installing-the-cloud-native-toolkit","text":"Initially the Fast-start installation method is the recommended way to install the Cloud-Native Toolkit, as this is largely an automatic install with minimal choices required that creates a default configuration suitable for learning about cloud native development. When you want to adopt cloud native development within an enterprise development team, then you may want more control over the installation and configuration of the toolkit. This section outlines more advanced options available for installing the toolkit. IBM Cloud Private Catalog : This provides a mechanism to easily create and delete instances of the Cloud-Native Toolkit on clusters within the IBM Cloud using the catalog, as you do with other service on the IBM Cloud. Iteration-Zero : This is a command line based tool that gives you full control over customizing how the toolkit is installed and what components you want deployed as part of the toolkit. Iteration-Zero is used by the other install methods, but you can also drive it directly from the command line. When you want to install your development cluster and the Cloud-Native Toolkit, there are a number of choices you need to make, including: what kubernetes distribution do you want your development cluster to use? how you want to apply access control to development resources? can you use public repositories, registries and source control systems or do you need to host your own? what monitoring and logging tools will you use? do you want to use the default install of the toolkit or customize it to use a different tools? where will you locate the development environment, on a public or private cloud? These topics are discussed in the adopting best practices section. The recommended development cluster for the Cloud-Native Toolkit is using RedHat OpenShift on IBM Cloud using the Virtual Private Cloud Infrastructure. If you will be using the IBM Cloud to host your development cluster, then proceed to the next section to configure your IBM Cloud account . If you plan to deploy to a different cloud infrastructure, you should proceed to the provision a cluster section.","title":"Installing the Cloud-Native Toolkit"},{"location":"adopting/setup/provision-cluster/","text":"Provision the cluster \u00b6 Provides the considerations and steps to prepare an OpenShift cluster for an installation of Cloud-Native Toolkit environment. 1. Provision the cluster \u00b6 Provisioning the OpenShift cluster is the minimum requirement to prepare for the Toolkit install. The cluster could be OpenShift CRC running on a laptop, a cloud-managed OpenShift cluster on AWS or Azure, or an on-premise install of OpenShift. We'll leave the steps to provision to you since the details will vary considerably depending upon the approach. However, whichever platform you use we highly recommend utilizing or developing automation scripts to perform the provisioning, so that it can be done repeatably. Here are some resources that might help get started: OpenShift CRC setup OpenShift on AWS OpenShift on GCP OpenShift on Azure OpenShift on VMWare Information The Cloud-Native Toolkit is not responsible for maintaining the Terraform scripts provided in the links above and does not give any guarantee as to their current condition. If there are any issues with those scripts it would be best to pursue it with the maintainers by raising issues against the appropriate repository. 2. Configure access to the cluster \u00b6 In order to interact with the cluster, user credentials need to be established having Cluster Admin rights. The initial kubeadmin user (or whatever name is used after it is changed) can be used or a new user can be added for use by the provisioning scripts. Next steps \u00b6 Now that the cluster has been provisioned and the access control has been configured, you are ready to move to the next step and perform the installation with the Toolkit. Before moving on, be sure to record the following information: OpenShift login user (user id with Cluster Admin permission) OpenShift login password OpenShift server url Once you have your cluster deployed you can use the fast-start install, used in the learning the toolkit section, or the Iteration Zero process.","title":"Install cluster elsewhere"},{"location":"adopting/setup/provision-cluster/#provision-the-cluster","text":"Provides the considerations and steps to prepare an OpenShift cluster for an installation of Cloud-Native Toolkit environment.","title":"Provision the cluster"},{"location":"adopting/setup/provision-cluster/#1-provision-the-cluster","text":"Provisioning the OpenShift cluster is the minimum requirement to prepare for the Toolkit install. The cluster could be OpenShift CRC running on a laptop, a cloud-managed OpenShift cluster on AWS or Azure, or an on-premise install of OpenShift. We'll leave the steps to provision to you since the details will vary considerably depending upon the approach. However, whichever platform you use we highly recommend utilizing or developing automation scripts to perform the provisioning, so that it can be done repeatably. Here are some resources that might help get started: OpenShift CRC setup OpenShift on AWS OpenShift on GCP OpenShift on Azure OpenShift on VMWare Information The Cloud-Native Toolkit is not responsible for maintaining the Terraform scripts provided in the links above and does not give any guarantee as to their current condition. If there are any issues with those scripts it would be best to pursue it with the maintainers by raising issues against the appropriate repository.","title":"1. Provision the cluster"},{"location":"adopting/setup/provision-cluster/#2-configure-access-to-the-cluster","text":"In order to interact with the cluster, user credentials need to be established having Cluster Admin rights. The initial kubeadmin user (or whatever name is used after it is changed) can be used or a new user can be added for use by the provisioning scripts.","title":"2. Configure access to the cluster"},{"location":"adopting/setup/provision-cluster/#next-steps","text":"Now that the cluster has been provisioned and the access control has been configured, you are ready to move to the next step and perform the installation with the Toolkit. Before moving on, be sure to record the following information: OpenShift login user (user id with Cluster Admin permission) OpenShift login password OpenShift server url Once you have your cluster deployed you can use the fast-start install, used in the learning the toolkit section, or the Iteration Zero process.","title":"Next steps"},{"location":"adopting/setup/toolkit-install/","text":"Installing to toolkit to an existing cluster \u00b6 The fast-start install used in the learning setup is one option if you want a standard install. If you want to customize the Toolkit installation, then use the full Iteration Zero scripts. Iteration Zero \u00b6 Steps to install the Cloud-Native Toolkit for running Openshift or Kubernetes clusters. A. Download the Iteration Zero scripts \u00b6 Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git B. Configure the credentials \u00b6 In a terminal, change to the ibm-garage-iteration-zero cloned directory cd ibm-garage-iteration-zero Copy the credentials.template file to a file named credentials.properties cp credentials.template credentials.properties Note credentials.properties is already listed in .gitignore to prevent the private credentials from being committed to the git repository If on IBM Cloud, update the value for the ibmcloud.api.key property in credentials.properties with your IBM Cloud API key C. Configure the Environment Variables \u00b6 IBM Cloud A. Set Environment Variables \u00b6 The settings for creating the on are set in the environment-ibmcloud.tfvars file in the ./terraform/settings directory of the ibm-garage-iteration-zero repository. There are a number of values that can be applied in the file, some required and some optional. Consult with the following table to determine which values should be used: Variable Required? Description eg. Value cluster_type yes The type of cluster into which the toolkit will be installed kubernetes , ocp3 , ocp4 or ocp44 cluster_exists yes Flag indicating if the cluster already exists. ( false means the cluster should be provisioned) true resource_group_name yes Existing resource group in the account where the cluster has been created dev-team-one vpc_cluster yes Flag indicating that the cluster has been built on VPC infrastructure. Defaults to true true or false name_prefix no The prefix that should be applied for any resources that are provisioned. Defaults to {resource_group_name} dev-one region no The region where the cluster has been/will be provisioned us-east , eu-gb , etc vpc_zone_names no A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if cluster_exists is set to false and vpc_cluster is set to true us-south-1 or us-east-1,us-east-2 cluster_name no The name of the cluster (If cluster_exists is set to true then this name should match an existing cluster). Defaults to {prefix_name}-cluster or {resource_group_name}-cluster dev-team-one-iks-117-vpc registry_namespace no The namespace that should be used in the IBM Container Registry. Defaults to {resource_group_name} dev-team-one-registry-2020 provision_logdna no Flag indicating that a new instance of LogDNA should be provisioned. Defaults to false true or false logdna_name no The name of the LogDNA instance (If provision_logdna is set to false this value is used by the scripts to bind the existing LogDNA instance to the cluster) cntk-showcase-logdna provision_sysdig no Flag indicating that a new instance of Sysdig should be provisioned. Defaults to false true or false sysdig_name no The name of the Sysdig instance (If provision_sysdig is set to false this value is used by the scripts to bind the existing Sysdig instance to the cluster) cntk-showcase-sysdig B. Configure the VLAN settings \u00b6 The vlan.tfvars file in terraform/settings contains properties that define the classic infrastructure configuration in order to provision a new cluster. Typical values look like this: shell script vlan_datacenter=\"dal10\" public_vlan_id=\"2366011\" private_vlan_id=\"2366012\" You must set all of these specifically for your cluster. Use the values provided by the account manager. vlan_datacenter -- The zone in the region in which the cluster worker nodes will be provisioned public_vlan_id -- The public VLAN that the cluster will use private_vlan_id -- The private VLAN that the cluster will use Optional: Generate the VLAN properties \u00b6 The IGC CLI can be used to generate these settings, to make the configuration as simple as possible. If your account has numerous VLANs and you want your cluster to use specific ones, then skip this step and provide the values by hand. This tool is for users who don't know what these required settings should be and want a simple way to gather reasonable defaults for their particular account. Log in using the Target the region where the cluster will be provisioned with the shell script ibmcloud target -r {region} Run the VLAN command shell script igc vlan Copy the output values from the CLI Command into your vlan.tfvars files C. (Optional) Customize the installed components \u00b6 The terraform/stages directory contains the default set of stages that define the modules that will be applied to the environment. The stages can be customized to change the makeup of the environment that is provisioned by either removing or adding stages from/to the terraform/stages directory. Note: The stages occasionally have dependencies on other stages (e.g. most all depend on the cluster module, many depend on the namespace module, etc.) so be aware of those dependencies as you start making changes. Dependencies are reflected in the module.{stage name} references in the stage variable list. The terraform/stages/catalog directory contains some optional stages that are prep-configured and can be dropped into the terraform/stages directory. Other modules are available from the Garage Terraform Modules catalog and can be added as stages to the directory as well. Since this is Terraform, any other Terraform scripts and modules can be added to the terraform/stages directory as desired. Multi-Cloud A. Set Environment Variables \u00b6 The configuration values for provisioning the environment on an existing OpenShift cluster are set in the environment-ocp.tfvars file in the ./terraform/settings directory of the ibm-garage-iteration-zero repository. There are only two values available: Variable Required? Description eg. Value server_url yes The url of the OpenShift cluster's API server https://api.mycluster.com cluster_name no The name of the cluster used simply for identification purposes. Defaults to ocp-cluster dev-team-one-ocp44 B. (Optional) Customize the installed components \u00b6 The terraform/stages-ocp4 directory contains the default set of stages that define the modules that will be applied to the environment. The stages can be customized to change the makeup of the environment that is provisioned by either removing or adding stages from/to the terraform/stages-ocp4 directory. Note: The stages occasionally have dependencies on other stages (e.g. most all depend on the cluster module, many depend on the namespace module, etc.) so be aware of those dependencies as you start making changes. Dependencies are reflected in the module.{stage name} references in the stage variable list. The terraform/stages-ocp4/catalog directory contains some optional stages that are prep-configured and can be dropped into the terraform/stages-ocp4 directory. Other modules are available from the Garage Terraform Modules catalog and can be added as stages to the directory as well. Since this is Terraform, any other Terraform scripts and modules can be added to the terraform/stages-ocp4 directory as desired. D. Run the installation \u00b6 Open a terminal to the ibm-garage-iteration-zero directory Launch a Developer Tools Docker container from which the Terraform scripts will be run ./launch.sh This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts and exec shell into the running container. When the container starts it mounts the file system's ./terraform/ directory as /home/devops/src/ and loads the values from the credentials.properties file as environment variables. Apply the Terraform by running the provided runTerraform.sh script ./runTerraform.sh This script collects the values provided in the environment-ibmcloud.tfvars and the stages defined in the terraform/stages to build the Terraform workspace. Along the way it will prompt for a couple pieces of information. Type of installation: ibmcloud or ocp There are two major paths to installing with the Toolkit. In this case, we are installing into an IBM Cloud-managed environment so we will select ibmcloud . This prompt can be skipped by providing --ibmcloud as an argument to ./runTerraform.sh Handling of an old workspace (if applicable): keep or delete If you executed the script previously for the current cluster configuration and the workspace directory still exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if you want to use the state from the previous run as a starting point to either add or remove configuration. Delete the workspace if you want to start with a clean install of the Toolkit. This prompt can be skipped by providing --delete or --keep as an argument to ./runTerraform.sh Verify the installation configuration The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. This prompt can be skipped by providing --auto-approve as an argument to ./runTerraform.sh Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer) and the rest of the process takes about 30 minutes.","title":"Install toolkit to existing cluster"},{"location":"adopting/setup/toolkit-install/#installing-to-toolkit-to-an-existing-cluster","text":"The fast-start install used in the learning setup is one option if you want a standard install. If you want to customize the Toolkit installation, then use the full Iteration Zero scripts.","title":"Installing to toolkit to an existing cluster"},{"location":"adopting/setup/toolkit-install/#iteration-zero","text":"Steps to install the Cloud-Native Toolkit for running Openshift or Kubernetes clusters.","title":"Iteration Zero"},{"location":"adopting/setup/toolkit-install/#a-download-the-iteration-zero-scripts","text":"Clone the ibm-garage-iteration-zero Git repository to your local filesystem git clone git@github.com:cloud-native-toolkit/ibm-garage-iteration-zero.git","title":"A. Download the Iteration Zero scripts"},{"location":"adopting/setup/toolkit-install/#b-configure-the-credentials","text":"In a terminal, change to the ibm-garage-iteration-zero cloned directory cd ibm-garage-iteration-zero Copy the credentials.template file to a file named credentials.properties cp credentials.template credentials.properties Note credentials.properties is already listed in .gitignore to prevent the private credentials from being committed to the git repository If on IBM Cloud, update the value for the ibmcloud.api.key property in credentials.properties with your IBM Cloud API key","title":"B. Configure the credentials"},{"location":"adopting/setup/toolkit-install/#c-configure-the-environment-variables","text":"IBM Cloud","title":"C. Configure the Environment Variables"},{"location":"adopting/setup/toolkit-install/#d-run-the-installation","text":"Open a terminal to the ibm-garage-iteration-zero directory Launch a Developer Tools Docker container from which the Terraform scripts will be run ./launch.sh This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts and exec shell into the running container. When the container starts it mounts the file system's ./terraform/ directory as /home/devops/src/ and loads the values from the credentials.properties file as environment variables. Apply the Terraform by running the provided runTerraform.sh script ./runTerraform.sh This script collects the values provided in the environment-ibmcloud.tfvars and the stages defined in the terraform/stages to build the Terraform workspace. Along the way it will prompt for a couple pieces of information. Type of installation: ibmcloud or ocp There are two major paths to installing with the Toolkit. In this case, we are installing into an IBM Cloud-managed environment so we will select ibmcloud . This prompt can be skipped by providing --ibmcloud as an argument to ./runTerraform.sh Handling of an old workspace (if applicable): keep or delete If you executed the script previously for the current cluster configuration and the workspace directory still exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if you want to use the state from the previous run as a starting point to either add or remove configuration. Delete the workspace if you want to start with a clean install of the Toolkit. This prompt can be skipped by providing --delete or --keep as an argument to ./runTerraform.sh Verify the installation configuration The script will verify some basic settings and prompt if you want to proceed. After you select Y (for yes), the Terraform Apply process will begin to create the infrastructure and services for your environment. This prompt can be skipped by providing --auto-approve as an argument to ./runTerraform.sh Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer) and the rest of the process takes about 30 minutes.","title":"D. Run the installation"},{"location":"adopting/use-cases/add-use-case/","text":"Adding a new use case \u00b6 You may want to extend the capabilities of the toolkit to provide additional functionality, such as adding support for a new programming language or use additional tooling. When adding a new use case there are the areas you need to consider: create a new starter kit or update an existing one to ensure developers can efficiently use the use case? add or amend pipelines or pipeline tasks ? add dashboard and console links (to make use case available to developers)","title":"Add a new use case"},{"location":"adopting/use-cases/add-use-case/#adding-a-new-use-case","text":"You may want to extend the capabilities of the toolkit to provide additional functionality, such as adding support for a new programming language or use additional tooling. When adding a new use case there are the areas you need to consider: create a new starter kit or update an existing one to ensure developers can efficiently use the use case? add or amend pipelines or pipeline tasks ? add dashboard and console links (to make use case available to developers)","title":"Adding a new use case"},{"location":"adopting/use-cases/applying/","text":"Applying the Cloud-Native Toolkit to developer scenarios \u00b6 Todo Add additional use cases Create an operator : Shows how the toolkit can be used to create an operator StoreFront - Quarkus Version : Exemplar application for developers to understand the cloud native toolkit and use cases on Red Hat OpenShift. App Connect REST API Workflow If you have requirements not supported by the Toolkit you can extend the toolkit by adding a new use case","title":"User cases"},{"location":"adopting/use-cases/applying/#applying-the-cloud-native-toolkit-to-developer-scenarios","text":"Todo Add additional use cases Create an operator : Shows how the toolkit can be used to create an operator StoreFront - Quarkus Version : Exemplar application for developers to understand the cloud native toolkit and use cases on Red Hat OpenShift. App Connect REST API Workflow If you have requirements not supported by the Toolkit you can extend the toolkit by adding a new use case","title":"Applying the Cloud-Native Toolkit to developer scenarios"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/","text":"Build and deploy an App Connect REST API workflow \u00b6 Learning tasks for developers to understand the IBM middleware integration use cases on Red Hat OpenShift. Self-paced agenda to build and deploy an App Connect REST API workflow \u00b6 This activity provides a working example of a Tekton based CICD pipeline to build and deploy an App Connect application invoking the REST API of the Inventory Management Service . The Pipeline and Task resources available in the Cloud Native Toolkit can be used as a starting point to build BAR files for other ACE workflows. The activity consists of the following tasks: Prerequisites Deploy the Inventory Management Service Configure the App Connect workflow with the Inventory REST API URL Execute the the App Connect pipeline to build and deploy the configured workflow Prerequisites \u00b6 Task Instructions Active OpenShift 4.x Cluster Set up accounts and tools Instructions Install the Cloud Native Toolkit Install the Cloud Native Toolkit Install IBM Cloud Pak for Integration Install Cloud Pak for Integration v2020.4 Deploy the backend Inventory Management Service \u00b6 The image below depicts the Tekton pipeline executed. Open a terminal and log into your OpenShift cluster. For IBM Cloud, navigate to your cluster in the IBM Cloud console , click on the Access tab, and follow the instructions to log in to the cluster from the command line. Create a development namespace. oc sync ${ DEV_NAMESPACE } --dev Open the Developer Dashboard oc dashboard From the Developer Dashboard, click on Starter Kits tab. Click on the Inventory Service tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a validate GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a valid name for you repo, GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template and the new repository will be created in your selected organization. With the browser open to the newly created repository, click on Clone or download and copy the clone SSH link , and use the git clone command to clone it to your developer desktop machine. git clone git@github.com: { gitid } /inventory-management-svc-solution.git Change into the cloned directory. cd inventory-management-svc-solution Set a base path for the REST API for the Inventory Management Service. OpenShift 4.0 - 4.5 Edit src/main/resources/application.yml and update the server section to include the /api base path to server.servlet.context-path . server: port: ${ PORT : 9080 } servlet: context-path: \"/api\" Edit the file chart/base/values.yaml and set the route.path from / to /api . route: enabled: false rewriteTarget: \"/\" path: \"/api\" OpenShift 4.6+ Edit the file chart/base/values.yaml and set the route.path from / to /api . route: enabled: false rewriteTarget: \"/\" path: \"/api\" Set the namespace context. oc project { DEV_NAMESPACE } Register the App in a DevOps Pipeline oc pipeline Select the Tekton pipeline type. The first time a pipeline is registered in the namespace, the CLI will ask for your username and Personal Access Token for the Git repository. The credentials will be stored in a secret named git-credentials . It will use the current branch. Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, you will also be prompted to select which pipeline you want to use for your application. Select ibm-java-gradle . Select Y / n to enable the pipeline to scan the image for vulnerabilities. Provide /api/health as the health endpoint. This is needed by the pipeline when running the health Task. After the pipeline has been created, the command will set up a webhook from the Git host to the pipeline event listener. Note If the webhook registration step fails it is likely because the Git credentials are incorrect or do not have enough permission in the repository. The pipeline will be registered in your development cluster and a pipelinerun will be started. 17. View your application pipeline. oc console From menu on the left switch to the Developer mode and select the dev project that was used for the application pipeline registration. In the left menu, select Pipelines and click on the link to the inventory-management-svc-solution-xxxxxx PipelineRun (PLR). Validate the REST API of the Inventory Management service is working correctly in the terminal. curl https:// $( oc get route inventory-management-svc-solution -o jsonpath = '{ .spec.host }' ) /api/stock-items The response should be similar to the following output. [{\"name\":\"Item 1\",\"id\":\"1\",\"stock\":100,\"price\":10.5,\"manufacturer\":\"Sony\"},{\"name\":\"Item 2\",\"id\":\"2\",\"stock\":150,\"price\":100.0,\"manufacturer\":\"Insignia\"},{\"name\":\"Item 3\",\"id\":\"3\",\"stock\":10,\"price\":1000.0,\"manufacturer\":\"Panasonic\"},{\"name\":\"Item 4\",\"id\":\"4\",\"stock\":9,\"price\":990.0,\"manufacturer\":\"JVC\"}] Here is a view of a completed and successful pipelinerun. Configure the App Connect workflow \u00b6 Download and install the ACE Toolkit and follow steps 1 and 2. Fork the inventory-mgmt-ace-solution repository. The new repository will be created in your selected organization. With the browser open to the newly created repository, click on Clone or download and copy the clone SSH link , and use the git clone command to clone it to your developer desktop machine. git clone git@github.com: { gitid } /inventory-mgmt-ace-solution.git Change into the cloned directory cd inventory-mgmt-ace-solution/workspace/InventoryManagementSvc Obtain the URL of the Inventory Management SVC route. shell script echo $(oc get route inventory-management-svc-solution -o jsonpath='{ .spec.host }') Search and replace the placeholder INVENTORY_MANAGEMENT_SVC_BASE_URL with the Route URL in the listStockItemsUsingGET.subflow and ace-inventory-management-svc.json files. Commit and push the changes into your forked repository. Execute the App Connect pipeline \u00b6 The image below depicts the Tekton pipeline executed. Create a Secret containing the IBM Entitled Registry credentials to pull the ACE image for the CI process in the pipeline. oc create secret generic ibm-entitled-registry --type = \"kubernetes.io/basic-auth\" --from-literal = username = cp --from-literal = password = <IBM Entitlement Key> Register the App in a DevOps Pipeline oc pipeline Select the Tekton pipeline type. You should not be prompted for Git credentials as a Secret already exists with your username and token. When registering a Tekton pipeline, you will also be prompted to select which pipeline you want to use for your application. Select ibm-ace-bar . Select Y / n to enable the pipeline to scan the image for vulnerabilities. Provide /api/stock-items as the health endpoint. This is needed by the pipeline when running the health Task. After the pipeline has been created, the command will set up a webhook from the Git host to the pipeline event listener Note If the webhook registration step fails it is likely because the Git credentials are incorrect or do not have enough permission in the repository. The pipeline will be registered in your development cluster. 8. View your application pipeline oc console From menu on the left switch to the Developer mode and select dev project that was used for the application pipeline registration. In the left menu, select Pipelines and click on the link to the inventory-mgmt-ace-solution-xxxxxx PipelineRun (PLR). Validate the App Connect server is working correctly in the terminal. curl http:// $( oc get route inventory-mgmt-ace-solution-http -o jsonpath = '{ .spec.host }' ) /api/stock-items The response should be similar to the following output. [{\"name\":\"Item 1\",\"id\":\"1\",\"stock\":100,\"price\":10.5,\"manufacturer\":\"Sony\"},{\"name\":\"Item 2\",\"id\":\"2\",\"stock\":150,\"price\":100.0,\"manufacturer\":\"Insignia\"},{\"name\":\"Item 3\",\"id\":\"3\",\"stock\":10,\"price\":1000.0,\"manufacturer\":\"Panasonic\"},{\"name\":\"Item 4\",\"id\":\"4\",\"stock\":9,\"price\":990.0,\"manufacturer\":\"JVC\"}] Here is a view of a completed and successful pipelinerun.","title":"App Connect REST API workflow"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#build-and-deploy-an-app-connect-rest-api-workflow","text":"Learning tasks for developers to understand the IBM middleware integration use cases on Red Hat OpenShift.","title":"Build and deploy an App Connect REST API workflow"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#self-paced-agenda-to-build-and-deploy-an-app-connect-rest-api-workflow","text":"This activity provides a working example of a Tekton based CICD pipeline to build and deploy an App Connect application invoking the REST API of the Inventory Management Service . The Pipeline and Task resources available in the Cloud Native Toolkit can be used as a starting point to build BAR files for other ACE workflows. The activity consists of the following tasks: Prerequisites Deploy the Inventory Management Service Configure the App Connect workflow with the Inventory REST API URL Execute the the App Connect pipeline to build and deploy the configured workflow","title":"Self-paced agenda to build and deploy an App Connect REST API workflow"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#prerequisites","text":"Task Instructions Active OpenShift 4.x Cluster Set up accounts and tools Instructions Install the Cloud Native Toolkit Install the Cloud Native Toolkit Install IBM Cloud Pak for Integration Install Cloud Pak for Integration v2020.4","title":"Prerequisites"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#deploy-the-backend-inventory-management-service","text":"The image below depicts the Tekton pipeline executed. Open a terminal and log into your OpenShift cluster. For IBM Cloud, navigate to your cluster in the IBM Cloud console , click on the Access tab, and follow the instructions to log in to the cluster from the command line. Create a development namespace. oc sync ${ DEV_NAMESPACE } --dev Open the Developer Dashboard oc dashboard From the Developer Dashboard, click on Starter Kits tab. Click on the Inventory Service tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a validate GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a valid name for you repo, GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template and the new repository will be created in your selected organization. With the browser open to the newly created repository, click on Clone or download and copy the clone SSH link , and use the git clone command to clone it to your developer desktop machine. git clone git@github.com: { gitid } /inventory-management-svc-solution.git Change into the cloned directory. cd inventory-management-svc-solution Set a base path for the REST API for the Inventory Management Service. OpenShift 4.0 - 4.5 Edit src/main/resources/application.yml and update the server section to include the /api base path to server.servlet.context-path . server: port: ${ PORT : 9080 } servlet: context-path: \"/api\" Edit the file chart/base/values.yaml and set the route.path from / to /api . route: enabled: false rewriteTarget: \"/\" path: \"/api\" OpenShift 4.6+ Edit the file chart/base/values.yaml and set the route.path from / to /api . route: enabled: false rewriteTarget: \"/\" path: \"/api\" Set the namespace context. oc project { DEV_NAMESPACE } Register the App in a DevOps Pipeline oc pipeline Select the Tekton pipeline type. The first time a pipeline is registered in the namespace, the CLI will ask for your username and Personal Access Token for the Git repository. The credentials will be stored in a secret named git-credentials . It will use the current branch. Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, you will also be prompted to select which pipeline you want to use for your application. Select ibm-java-gradle . Select Y / n to enable the pipeline to scan the image for vulnerabilities. Provide /api/health as the health endpoint. This is needed by the pipeline when running the health Task. After the pipeline has been created, the command will set up a webhook from the Git host to the pipeline event listener. Note If the webhook registration step fails it is likely because the Git credentials are incorrect or do not have enough permission in the repository. The pipeline will be registered in your development cluster and a pipelinerun will be started. 17. View your application pipeline. oc console From menu on the left switch to the Developer mode and select the dev project that was used for the application pipeline registration. In the left menu, select Pipelines and click on the link to the inventory-management-svc-solution-xxxxxx PipelineRun (PLR). Validate the REST API of the Inventory Management service is working correctly in the terminal. curl https:// $( oc get route inventory-management-svc-solution -o jsonpath = '{ .spec.host }' ) /api/stock-items The response should be similar to the following output. [{\"name\":\"Item 1\",\"id\":\"1\",\"stock\":100,\"price\":10.5,\"manufacturer\":\"Sony\"},{\"name\":\"Item 2\",\"id\":\"2\",\"stock\":150,\"price\":100.0,\"manufacturer\":\"Insignia\"},{\"name\":\"Item 3\",\"id\":\"3\",\"stock\":10,\"price\":1000.0,\"manufacturer\":\"Panasonic\"},{\"name\":\"Item 4\",\"id\":\"4\",\"stock\":9,\"price\":990.0,\"manufacturer\":\"JVC\"}] Here is a view of a completed and successful pipelinerun.","title":"Deploy the backend Inventory Management Service"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#configure-the-app-connect-workflow","text":"Download and install the ACE Toolkit and follow steps 1 and 2. Fork the inventory-mgmt-ace-solution repository. The new repository will be created in your selected organization. With the browser open to the newly created repository, click on Clone or download and copy the clone SSH link , and use the git clone command to clone it to your developer desktop machine. git clone git@github.com: { gitid } /inventory-mgmt-ace-solution.git Change into the cloned directory cd inventory-mgmt-ace-solution/workspace/InventoryManagementSvc Obtain the URL of the Inventory Management SVC route. shell script echo $(oc get route inventory-management-svc-solution -o jsonpath='{ .spec.host }') Search and replace the placeholder INVENTORY_MANAGEMENT_SVC_BASE_URL with the Route URL in the listStockItemsUsingGET.subflow and ace-inventory-management-svc.json files. Commit and push the changes into your forked repository.","title":"Configure the App Connect workflow"},{"location":"adopting/use-cases/ace-pipeline/ace-pipeline/#execute-the-app-connect-pipeline","text":"The image below depicts the Tekton pipeline executed. Create a Secret containing the IBM Entitled Registry credentials to pull the ACE image for the CI process in the pipeline. oc create secret generic ibm-entitled-registry --type = \"kubernetes.io/basic-auth\" --from-literal = username = cp --from-literal = password = <IBM Entitlement Key> Register the App in a DevOps Pipeline oc pipeline Select the Tekton pipeline type. You should not be prompted for Git credentials as a Secret already exists with your username and token. When registering a Tekton pipeline, you will also be prompted to select which pipeline you want to use for your application. Select ibm-ace-bar . Select Y / n to enable the pipeline to scan the image for vulnerabilities. Provide /api/stock-items as the health endpoint. This is needed by the pipeline when running the health Task. After the pipeline has been created, the command will set up a webhook from the Git host to the pipeline event listener Note If the webhook registration step fails it is likely because the Git credentials are incorrect or do not have enough permission in the repository. The pipeline will be registered in your development cluster. 8. View your application pipeline oc console From menu on the left switch to the Developer mode and select dev project that was used for the application pipeline registration. In the left menu, select Pipelines and click on the link to the inventory-mgmt-ace-solution-xxxxxx PipelineRun (PLR). Validate the App Connect server is working correctly in the terminal. curl http:// $( oc get route inventory-mgmt-ace-solution-http -o jsonpath = '{ .spec.host }' ) /api/stock-items The response should be similar to the following output. [{\"name\":\"Item 1\",\"id\":\"1\",\"stock\":100,\"price\":10.5,\"manufacturer\":\"Sony\"},{\"name\":\"Item 2\",\"id\":\"2\",\"stock\":150,\"price\":100.0,\"manufacturer\":\"Insignia\"},{\"name\":\"Item 3\",\"id\":\"3\",\"stock\":10,\"price\":1000.0,\"manufacturer\":\"Panasonic\"},{\"name\":\"Item 4\",\"id\":\"4\",\"stock\":9,\"price\":990.0,\"manufacturer\":\"JVC\"}] Here is a view of a completed and successful pipelinerun.","title":"Execute the App Connect pipeline"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/","text":"Install IBM Cloud Paks using a GitOps workflow \u00b6 The GitOps concept originated from Weaveworks back in 2017 and the goal was to automate the operations of a Kubernetes (K8s) system using a model external to the system as the source of truth ( History of GitOps ). There are various GitOps workflows this is our opinionated point of view on how GitOps can be used to manage the infrastructure, services and application layers of K8s based systems. It takes into account the various personas interacting with the system and accounts for separation of duties. Refer to the https://github.com/cloud-native-toolkit/multi-tenancy-gitops repository for instructions to try out the GitOps workflow. It is focused around deploying IBM Cloud Paks on the Red Hat OpenShift platform. GitOps Principles \u00b6 With the ever growing adoption of GitOps, the OpenGitOps project was started in 2021 to define a set of open-source standards and best practices. These will help organizations adopt a standard and structured approach when implementing GitOps. It is currently a CNCF Sandbox project . The GitOps Working Group has released v0.1.0 of the GitOps Principles : The principle of declarative desired state : A system managed by GitOps must have its Desired State expressed declaratively as data in a format writable and readable by both humans and machines. The principle of immutable desired state versions : Desired State is stored in a way that supports versioning, immutability of versions, and retains a complete version history. The principle of continuous state reconciliation : Software agents continuously, and automatically, compare a system's Actual State to its Desired State. If the actual and desired states differ for any reason, automated actions to reconcile them are initiated. The principle of operations through declaration : The only mechanism through which the system is intentionally operated on is through these principles. GitOps Repository Structure \u00b6 There are a total of 4 Git repositories involved with the GitOps workflow. Main GitOps repository - Repository that the various personas will interact with to update the desired state of the OpenShift cluster. Infrastructure repository - Repository Services repository Application repository GitOps \u00b6 Main GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops ): This repository contains all the ArgoCD Applications for the infrastructure , services and application layers. Each ArgoCD Application will reference a specific K8s resource (yaml resides in a separate git repository), contain the configuration of the K8s resource, and determine where it will be deployed into the cluster. Directory structure for single-cluster or multi-cluster profiles: . \u251c\u2500\u2500 1 -infra \u251c\u2500\u2500 2 -services \u251c\u2500\u2500 3 -apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml The contents of the kustomization.yaml will determine which layer(s) will be deployed to the cluster. This is based on whether the resources are commented out or not. Each of the listed YAMLs contains an ArgoCD Application which in turn tracks all the K8s resources available to be deployed. This follows the ArgoCD app of apps pattern . resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml Infrastructure Layer \u00b6 Infrastructure GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-infra ): Contains the YAMLs for cluster-wide and/or infrastructure related K8s resources managed by a cluster administrator. This would include namespaces , clusterroles , clusterrolebindings , machinesets to name a few. 1 -infra \u251c\u2500\u2500 1 -infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/operator-ocs.yaml #- argocd/refarch-infraconfig.yaml #- argocd/refarch-machinesets.yaml Services Layer \u00b6 Services GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-services ): Contains the YAMLs for K8s resources which will be used by the application layer. This could include subscriptions for Operators, YAMLs of custom resources provided, or Helm Charts for tools provided by a third party. These resource would usually be managed by the Administrator(s) and/or a DevOps team supporting application developers. 2 -services \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u2514\u2500\u2500 openshift-service-mesh.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml Application Layer \u00b6 Application GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-apps ): Contains the YAMLs for K8s resources required for Tekton pipelines and webhooks. It can also contains the YAMLs to deploy microservice(s), web application(s), instance(S) of the ACE integration server or queue manager(s). 3 -apps \u251c\u2500\u2500 3 -apps.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 <PRODUCT> \u2502 \u2502 \u251c\u2500\u2500 cicd.yaml \u2502 \u2502 \u251c\u2500\u2500 dev.yaml \u2502 \u2502 \u251c\u2500\u2500 prod.yaml \u2502 \u2502 \u2514\u2500\u2500 stage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/<PRODUCT>/cicd.yaml #- argocd/<PRODUCT>/dev.yaml #- argocd/<PRODUCT>/stage.yaml #- argocd/<PRODUCT>/prod.yaml Personas \u00b6 Persona Responsibilities Not responsible for Required skills OpenShift cluster administrator - Provision/install the cluster - Configure compute, network, storage (i.e. compute infrastructure) - Install gitOps and pipelines operator (ArgoCD and Tekton technologies) - Creates required cluster-wide resources - Install common pipelines and tasks used by product tenants - - Very little knowledge of products installed in the cluster, for example MQ or ACE - Delegates cluster security admin to Cluster security administrator - Strong skills in Kubernetes administration and Kubernetes operations - Working knowledge of DevOps: practices, tools, implementation OpenShift cluster security administrator - Responsible for administering OCP cluster security - Responsible for implementing organization wide security practices Works with both cluster admin and tenant product admin - Responsible for administering tenant product security - Security artifacts have the same structure for different tenant products - Unlikely to have knowledge of tenant products such as MQ, ACE - Strong security skills - Strong Kubernetes skills administration and operations OpenShift cluster tenant product administrator - Implements CICD DevOps for tenant product such as MQ or ACE - Can add tenant product specific cluster resources - Delegates cluster security admin to OCP cluster security administrator - Good skills in Kubernetes administration and Kubernetes operations - Good knowledge of DevOps: practices, tools, implementation - Good application development experience using containers and Kubernetes - Basic/working knowledge tenant product such as MQ or ACE Tenant product administrator - The product/integration administrator - May have some knowledge/experience of Kubernetes - Working/strong admin skills for tenant product, e.g. MQ or ACE - Basic knowledge of Kubernetes Tenant product developer - The developer who writes an application program that uses MQ - The integration developer who authors an ACE integration flow - Maintains development assets in source repository - No knowledge of Kubernetes required - Working/strong developer skills for tenant product, e.g. MQ or ACE DevOps leader - Is an expert in in DevOps - Establishes initial GitOps configuration at a customer - Works in close partnership with OCP Cluster administrator and cluster tenant administrator - Establishes/fixes/updates/evolves the DevOps resources deployed to the cluster - Ensures that DevOps infrastructure evolves in a well governed manner - Ensures the DevOps infrastructure is tested before it is rolled out - Strong Kubernetes skills administration and operations - Strong knowledge of DevOps: practices, tools, implementation - Basic work knowledge of how tenant products such as MQ, ACE interact with Kubernetes Site reliability engineer (SRE) - A Kubernetes engineer responsible for diagnosing and fixing issues with the cluster - Under time pressure to get failed service(s) back up and running - - - No knowledge of tenant products such as MQ, ACE - Strong Kubernetes skills administration and operations - Working knowledge of DevOps: practices, tools, implementation","title":"GitOps with IBM Cloud Paks"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#install-ibm-cloud-paks-using-a-gitops-workflow","text":"The GitOps concept originated from Weaveworks back in 2017 and the goal was to automate the operations of a Kubernetes (K8s) system using a model external to the system as the source of truth ( History of GitOps ). There are various GitOps workflows this is our opinionated point of view on how GitOps can be used to manage the infrastructure, services and application layers of K8s based systems. It takes into account the various personas interacting with the system and accounts for separation of duties. Refer to the https://github.com/cloud-native-toolkit/multi-tenancy-gitops repository for instructions to try out the GitOps workflow. It is focused around deploying IBM Cloud Paks on the Red Hat OpenShift platform.","title":"Install IBM Cloud Paks using a GitOps workflow"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#gitops-principles","text":"With the ever growing adoption of GitOps, the OpenGitOps project was started in 2021 to define a set of open-source standards and best practices. These will help organizations adopt a standard and structured approach when implementing GitOps. It is currently a CNCF Sandbox project . The GitOps Working Group has released v0.1.0 of the GitOps Principles : The principle of declarative desired state : A system managed by GitOps must have its Desired State expressed declaratively as data in a format writable and readable by both humans and machines. The principle of immutable desired state versions : Desired State is stored in a way that supports versioning, immutability of versions, and retains a complete version history. The principle of continuous state reconciliation : Software agents continuously, and automatically, compare a system's Actual State to its Desired State. If the actual and desired states differ for any reason, automated actions to reconcile them are initiated. The principle of operations through declaration : The only mechanism through which the system is intentionally operated on is through these principles.","title":"GitOps Principles"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#gitops-repository-structure","text":"There are a total of 4 Git repositories involved with the GitOps workflow. Main GitOps repository - Repository that the various personas will interact with to update the desired state of the OpenShift cluster. Infrastructure repository - Repository Services repository Application repository","title":"GitOps Repository Structure"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#gitops","text":"Main GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops ): This repository contains all the ArgoCD Applications for the infrastructure , services and application layers. Each ArgoCD Application will reference a specific K8s resource (yaml resides in a separate git repository), contain the configuration of the K8s resource, and determine where it will be deployed into the cluster. Directory structure for single-cluster or multi-cluster profiles: . \u251c\u2500\u2500 1 -infra \u251c\u2500\u2500 2 -services \u251c\u2500\u2500 3 -apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml The contents of the kustomization.yaml will determine which layer(s) will be deployed to the cluster. This is based on whether the resources are commented out or not. Each of the listed YAMLs contains an ArgoCD Application which in turn tracks all the K8s resources available to be deployed. This follows the ArgoCD app of apps pattern . resources : - 1-infra/1-infra.yaml - 2-services/2-services.yaml - 3-apps/3-apps.yaml","title":"GitOps"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#infrastructure-layer","text":"Infrastructure GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-infra ): Contains the YAMLs for cluster-wide and/or infrastructure related K8s resources managed by a cluster administrator. This would include namespaces , clusterroles , clusterrolebindings , machinesets to name a few. 1 -infra \u251c\u2500\u2500 1 -infra.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 consolelink.yaml \u2502 \u251c\u2500\u2500 consolenotification.yaml \u2502 \u251c\u2500\u2500 infraconfig.yaml \u2502 \u251c\u2500\u2500 machinesets.yaml \u2502 \u251c\u2500\u2500 namespace-ci.yaml \u2502 \u251c\u2500\u2500 namespace-dev.yaml \u2502 \u251c\u2500\u2500 namespace-ibm-common-services.yaml \u2502 \u251c\u2500\u2500 namespace-istio-system.yaml \u2502 \u251c\u2500\u2500 namespace-openldap.yaml \u2502 \u251c\u2500\u2500 namespace-openshift-storage.yaml \u2502 \u251c\u2500\u2500 namespace-prod.yaml \u2502 \u251c\u2500\u2500 namespace-sealed-secrets.yaml \u2502 \u251c\u2500\u2500 namespace-staging.yaml \u2502 \u251c\u2500\u2500 namespace-tools.yaml \u2502 \u2514\u2500\u2500 storage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/consolelink.yaml #- argocd/consolenotification.yaml #- argocd/namespace-ibm-common-services.yaml #- argocd/namespace-ci.yaml #- argocd/namespace-dev.yaml #- argocd/namespace-staging.yaml #- argocd/namespace-prod.yaml #- argocd/namespace-istio-system.yaml #- argocd/namespace-openldap.yaml #- argocd/namespace-sealed-secrets.yaml #- argocd/namespace-tools.yaml #- argocd/namespace-openshift-storage.yaml #- argocd/operator-ocs.yaml #- argocd/refarch-infraconfig.yaml #- argocd/refarch-machinesets.yaml","title":"Infrastructure Layer"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#services-layer","text":"Services GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-services ): Contains the YAMLs for K8s resources which will be used by the application layer. This could include subscriptions for Operators, YAMLs of custom resources provided, or Helm Charts for tools provided by a third party. These resource would usually be managed by the Administrator(s) and/or a DevOps team supporting application developers. 2 -services \u251c\u2500\u2500 2 -services.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 instances \u2502 \u2502 \u251c\u2500\u2500 artifactory.yaml \u2502 \u2502 \u251c\u2500\u2500 cert-manager-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 chartmuseum.yaml \u2502 \u2502 \u251c\u2500\u2500 developer-dashboard.yaml \u2502 \u2502 \u251c\u2500\u2500 grafana-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-foundational-services-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-platform-navigator-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 ibm-process-mining-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 openldap.yaml \u2502 \u2502 \u251c\u2500\u2500 openshift-service-mesh-instance.yaml \u2502 \u2502 \u251c\u2500\u2500 pact-broker.yaml \u2502 \u2502 \u251c\u2500\u2500 sealed-secrets.yaml \u2502 \u2502 \u251c\u2500\u2500 sonarqube.yaml \u2502 \u2502 \u2514\u2500\u2500 swaggereditor.yaml \u2502 \u2514\u2500\u2500 operators \u2502 \u251c\u2500\u2500 cert-manager.yaml \u2502 \u251c\u2500\u2500 elasticsearch.yaml \u2502 \u251c\u2500\u2500 grafana-operator.yaml \u2502 \u251c\u2500\u2500 ibm-ace-operator.yaml \u2502 \u251c\u2500\u2500 ibm-apic-operator.yaml \u2502 \u251c\u2500\u2500 ibm-aspera-operator.yaml \u2502 \u251c\u2500\u2500 ibm-assetrepository-operator.yaml \u2502 \u251c\u2500\u2500 ibm-automation-foundation-core-operator.yaml \u2502 \u251c\u2500\u2500 ibm-catalogs.yaml \u2502 \u251c\u2500\u2500 ibm-cp4i-operators.yaml \u2502 \u251c\u2500\u2500 ibm-datapower-operator.yaml \u2502 \u251c\u2500\u2500 ibm-eventstreams-operator.yaml \u2502 \u251c\u2500\u2500 ibm-foundations.yaml \u2502 \u251c\u2500\u2500 ibm-mq-operator.yaml \u2502 \u251c\u2500\u2500 ibm-opsdashboard-operator.yaml \u2502 \u251c\u2500\u2500 ibm-platform-navigator.yaml \u2502 \u251c\u2500\u2500 ibm-process-mining-operator.yaml \u2502 \u251c\u2500\u2500 jaeger.yaml \u2502 \u251c\u2500\u2500 kiali.yaml \u2502 \u251c\u2500\u2500 openshift-gitops.yaml \u2502 \u251c\u2500\u2500 openshift-pipelines.yaml \u2502 \u2514\u2500\u2500 openshift-service-mesh.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : # IBM Software #- argocd/operators/ibm-ace-operator.yaml #- argocd/operators/ibm-apic-operator.yaml #- argocd/operators/ibm-aspera-operator.yaml #- argocd/operators/ibm-assetrepository-operator.yaml #- argocd/operators/ibm-cp4i-operators.yaml #- argocd/operators/ibm-datapower-operator.yaml #- argocd/operators/ibm-eventstreams-operator.yaml #- argocd/operators/ibm-mq-operator.yaml #- argocd/operators/ibm-opsdashboard-operator.yaml #- argocd/operators/ibm-process-mining-operator.yaml #- argocd/instances/ibm-process-mining-instance.yaml #- argocd/operators/ibm-platform-navigator.yaml #- argocd/instances/ibm-platform-navigator-instance.yaml # IBM Foundations / Common Services #- argocd/operators/ibm-foundations.yaml #- argocd/instances/ibm-foundational-services-instance.yaml #- argocd/operators/ibm-automation-foundation-core-operator.yaml # IBM Catalogs #- argocd/operators/ibm-catalogs.yaml # Required for IBM MQ #- argocd/instances/openldap.yaml # Required for IBM ACE, IBM MQ #- argocd/operators/cert-manager.yaml #- argocd/instances/cert-manager-instance.yaml # Sealed Secrets #- argocd/instances/sealed-secrets.yaml # CICD #- argocd/operators/grafana-operator.yaml #- argocd/instances/grafana-instance.yaml #- argocd/instances/artifactory.yaml #- argocd/instances/chartmuseum.yaml #- argocd/instances/developer-dashboard.yaml #- argocd/instances/swaggereditor.yaml #- argocd/instances/sonarqube.yaml #- argocd/instances/pact-broker.yaml # In OCP 4.7+ we need to install openshift-pipelines and possibly privileged scc to the pipeline serviceaccount #- argocd/operators/openshift-pipelines.yaml # Service Mesh #- argocd/operators/elasticsearch.yaml #- argocd/operators/jaeger.yaml #- argocd/operators/kiali.yaml #- argocd/operators/openshift-service-mesh.yaml","title":"Services Layer"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#application-layer","text":"Application GitOps repository ( https://github.com/cloud-native-toolkit/multi-tenancy-gitops-apps ): Contains the YAMLs for K8s resources required for Tekton pipelines and webhooks. It can also contains the YAMLs to deploy microservice(s), web application(s), instance(S) of the ACE integration server or queue manager(s). 3 -apps \u251c\u2500\u2500 3 -apps.yaml \u251c\u2500\u2500 argocd \u2502 \u251c\u2500\u2500 <PRODUCT> \u2502 \u2502 \u251c\u2500\u2500 cicd.yaml \u2502 \u2502 \u251c\u2500\u2500 dev.yaml \u2502 \u2502 \u251c\u2500\u2500 prod.yaml \u2502 \u2502 \u2514\u2500\u2500 stage.yaml \u2514\u2500\u2500 kustomization.yaml Contents of the `kustomization.yaml` will determine which resources are deployed to the cluster resources : #- argocd/<PRODUCT>/cicd.yaml #- argocd/<PRODUCT>/dev.yaml #- argocd/<PRODUCT>/stage.yaml #- argocd/<PRODUCT>/prod.yaml","title":"Application Layer"},{"location":"adopting/use-cases/gitops/gitops-ibm-cloud-paks/#personas","text":"Persona Responsibilities Not responsible for Required skills OpenShift cluster administrator - Provision/install the cluster - Configure compute, network, storage (i.e. compute infrastructure) - Install gitOps and pipelines operator (ArgoCD and Tekton technologies) - Creates required cluster-wide resources - Install common pipelines and tasks used by product tenants - - Very little knowledge of products installed in the cluster, for example MQ or ACE - Delegates cluster security admin to Cluster security administrator - Strong skills in Kubernetes administration and Kubernetes operations - Working knowledge of DevOps: practices, tools, implementation OpenShift cluster security administrator - Responsible for administering OCP cluster security - Responsible for implementing organization wide security practices Works with both cluster admin and tenant product admin - Responsible for administering tenant product security - Security artifacts have the same structure for different tenant products - Unlikely to have knowledge of tenant products such as MQ, ACE - Strong security skills - Strong Kubernetes skills administration and operations OpenShift cluster tenant product administrator - Implements CICD DevOps for tenant product such as MQ or ACE - Can add tenant product specific cluster resources - Delegates cluster security admin to OCP cluster security administrator - Good skills in Kubernetes administration and Kubernetes operations - Good knowledge of DevOps: practices, tools, implementation - Good application development experience using containers and Kubernetes - Basic/working knowledge tenant product such as MQ or ACE Tenant product administrator - The product/integration administrator - May have some knowledge/experience of Kubernetes - Working/strong admin skills for tenant product, e.g. MQ or ACE - Basic knowledge of Kubernetes Tenant product developer - The developer who writes an application program that uses MQ - The integration developer who authors an ACE integration flow - Maintains development assets in source repository - No knowledge of Kubernetes required - Working/strong developer skills for tenant product, e.g. MQ or ACE DevOps leader - Is an expert in in DevOps - Establishes initial GitOps configuration at a customer - Works in close partnership with OCP Cluster administrator and cluster tenant administrator - Establishes/fixes/updates/evolves the DevOps resources deployed to the cluster - Ensures that DevOps infrastructure evolves in a well governed manner - Ensures the DevOps infrastructure is tested before it is rolled out - Strong Kubernetes skills administration and operations - Strong knowledge of DevOps: practices, tools, implementation - Basic work knowledge of how tenant products such as MQ, ACE interact with Kubernetes Site reliability engineer (SRE) - A Kubernetes engineer responsible for diagnosing and fixing issues with the cluster - Under time pressure to get failed service(s) back up and running - - - No knowledge of tenant products such as MQ, ACE - Strong Kubernetes skills administration and operations - Working knowledge of DevOps: practices, tools, implementation","title":"Personas"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/","text":"Install Serverless and Cloud Events on an existing cluster using GitOps \u00b6 Note This is a work in progress, come back for updates. Overview \u00b6 This guide will illustrate the steps to install Serverless and Cloud Events in an existing cluster using ArgoCD provided by Red Hat OpenShift GitOps operator. Additionally it shows the steps to configure Red Hat Openshift Pipelines operator based on Tekton to send cloud events and steps to configure a \"slack-notification\" app to receive those events. Install Serverless and Cloud Eventing \u00b6 Pre-requisites \u00b6 The following is required before proceeding to the next section. Provision an OpenShift cluster. Login to the cluster via the oc cli. Installation Steps \u00b6 Fork the multi-tenancy-gitops repository and clone your fork. git clone git@github.com: { gitid } /multi-tenancy-gitops.git Change to the kustomize branch of your fork. cd multi-tenancy-gitops git checkout kustomize Install the Red Hat OpenShift GitOps operator. For Openshift 4.6 oc apply -f setup/ocp46/ For Openshift 4.7 oc apply -f setup/ocp47/ Update the files to reference your forked repository. Run the set-git-source.sh script that will replace cloud-native-toolkit Github Org references with your {gitid}. export GIT_USER ={ gitid } ./scripts/set-git-source.sh Push the changes to your forked repository. git add . git commit -m \"push repo gitid changes\" git push Their are different deployment options provided in folders in the repository. In this guide we will use the default single-server deployment. The other options are located in the others folder. ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster If you choose to use a different deployment option edit the 0-bootstrap/argocd/bootstrap.yaml and modify the spec.source.path and update the metadata.name accordingly. For example to use the 1-shared-cluster change the path to 0-bootstrap/argocd/others/1-shared-cluster . apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bootstrap-1-shared-cluster namespace: openshift-gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: default source: path: 0-bootstrap/argocd/others/1-shared-cluster repoURL: https://github.com/lsteck/multi-tenancy-gitops.git targetRevision: kustomize syncPolicy: automated: prune: true selfHeal: true In this guide we will use the unchanged 0-bootstrap/argocd/bootstrap.yaml which uses the single-cluster deployment. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bootstrap-single-cluster namespace: openshift-gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: default source: path: 0-bootstrap/argocd/single-cluster repoURL: https://github.com/lsteck/multi-tenancy-gitops.git targetRevision: kustomize syncPolicy: automated: prune: true selfHeal: true Under the cluster's folder there are 1-infra , 2-services and 3-apps folders which define what infrastructure, services and app resources are to be deployed respectively. ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Open the kustomization.yaml file under the 1-infra folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 1-infra \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Openshift Serverless/Eventing section to deploy those resources. # Openshift Serverless/Eventing - argocd/namespace-openshift-serverless.yaml - argocd/namespace-knative-serving.yaml - argocd/namespace-knative-eventing.yaml Open the kustomization.yaml file under the 2-services folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u2514\u2500\u2500 2-services \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Openshift Serverless/Eventing section to deploy those resources. # Openshift Serverless/Eventing - argocd/operators/openshift-serverless.yaml - argocd/instances/knative-eventing-instance.yaml Installing the Serverless and Eventing doesn't require any resources under the 3-apps folder so the kustomization.yaml in that folder doesn't need to be changed. Push the changes to your forked repository. git add . git commit -m \"push serverless and eventing\" git push Create the bootstrap ArgoCD application. oc apply -f 0 -bootstrap/argocd/bootstrap.yaml -n openshift-gitops From the OpenShift console launch ArgoCD by clicking the ArgoCD link from the Applications (9 squares) menu The ArcoCD user id is admin and the password can be found in the argocd-cluster-cluster secret in the openshift-gitops project namespace. You can extract the secret with the command oc extract secret/argocd-cluster-cluster --to = - -n openshift-gitops On the ArgoCD UI you can see the newly created bootstrap application. After several minutes you will see all the other ArgoCD applications with a status Healthy and Synced . The status will progress from Missing , OutOfSync , Syncing . If you see a status of Sync failed there were errors. You can check that the Red Hat OpenShift Serverless operator that provides serverless and eventing capabilities has been installed from the Installed Operators page on the console. Install Slack Notification app and configure Tekton to emit Cloud Events \u00b6 Note Both installation steps could be performed at the same time. They were broken out in this guide to illustrate how you could install Serverless and Eventing without installing the Slack Notification app. Pre-requisites \u00b6 The following are required before proceeding. Compete the previous section Install Serverless and Cloud Eventing Create a Slack Incoming Webhook . Installation Steps \u00b6 Open the kustomization.yaml file under the 1-infra folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 1-infra \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Slack Notifications section to deploy those resources. # Slack Notifications - argocd/namespace-slack-notifications.yaml Push the changes to your forked repository. git add . git commit -m \"push slack notifications namespace\" git push After a few minutes you should see see an ArgoCD namespace-slack-notifications app. This app creates the slack-notifications project namespace where we will deploy the slack notification app. Before we deploy the app we need to create a secret to store the slack incoming webhook you created as a pre-requisite. This secret needs to be in the slack-notifications project namespace. You can generate an encrypted secret containing the slack notification webhook using the Sealed Secret Operator or you can manually create the secret as follows NOTE: replace <webhook-url> with your slack webhook url . WEBHOOK = <webhook-url> oc project slack-notifications oc create secret generic slack-secret \\ --from-literal = SLACK_URL = ${ WEBHOOK } Installing the Slack Notification app doesn't require any resources under the 2-services folder so the kustomization.yaml in that folder doesn't need to be changed. Open the kustomization.yaml file under the 3-apps folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps \u251c\u2500\u2500 3-apps.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Slack Notifications section to deploy those resources. # Slack Notifications - argocd/slack-notifications/slack-notifications.yaml - argocd/shared/config/openshift-pipelines/configmap/openshift-pipelines-config.yaml If you changed the slack-secret name or key you need to update the secret name and key value in the slack-notification.yaml Open slack-notifications.yaml ./0-bootstrap \u2514\u2500\u2500 argocd \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 3-apps \u2514\u2500\u2500 argocd \u2514\u2500\u2500 slack-notifications \u2514\u2500\u2500 slack-notifications.yaml Modify the name and key to match your secret name and the key name. secret: # provide name of the secret that contains slack url name: slack-secret # provide key of the secret that contains slack url key: SLACK_URL Push the changes to your forked repository. git add . git commit -m \"push slack notifications app\" git push After a few minutes you will see an apps-slack-notifications ArgoCD app and a openshift-pipelines-config ArgoCD app. On the OpenShift Console you can see the slack notifications app deployment in the slack-notifications project namespace. Now when you run Pipelines you will receive Slack Notifications.","title":"GitOps with Serverless/Events"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#install-serverless-and-cloud-events-on-an-existing-cluster-using-gitops","text":"Note This is a work in progress, come back for updates.","title":"Install Serverless and Cloud Events on an existing cluster using GitOps"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#overview","text":"This guide will illustrate the steps to install Serverless and Cloud Events in an existing cluster using ArgoCD provided by Red Hat OpenShift GitOps operator. Additionally it shows the steps to configure Red Hat Openshift Pipelines operator based on Tekton to send cloud events and steps to configure a \"slack-notification\" app to receive those events.","title":"Overview"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#install-serverless-and-cloud-eventing","text":"","title":"Install Serverless and Cloud Eventing"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#pre-requisites","text":"The following is required before proceeding to the next section. Provision an OpenShift cluster. Login to the cluster via the oc cli.","title":"Pre-requisites"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#installation-steps","text":"Fork the multi-tenancy-gitops repository and clone your fork. git clone git@github.com: { gitid } /multi-tenancy-gitops.git Change to the kustomize branch of your fork. cd multi-tenancy-gitops git checkout kustomize Install the Red Hat OpenShift GitOps operator. For Openshift 4.6 oc apply -f setup/ocp46/ For Openshift 4.7 oc apply -f setup/ocp47/ Update the files to reference your forked repository. Run the set-git-source.sh script that will replace cloud-native-toolkit Github Org references with your {gitid}. export GIT_USER ={ gitid } ./scripts/set-git-source.sh Push the changes to your forked repository. git add . git commit -m \"push repo gitid changes\" git push Their are different deployment options provided in folders in the repository. In this guide we will use the default single-server deployment. The other options are located in the others folder. ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2502 \u251c\u2500\u2500 1-shared-cluster \u2502 \u251c\u2500\u2500 2-isolated-cluster \u2502 \u2514\u2500\u2500 3-multi-cluster \u2514\u2500\u2500 single-cluster If you choose to use a different deployment option edit the 0-bootstrap/argocd/bootstrap.yaml and modify the spec.source.path and update the metadata.name accordingly. For example to use the 1-shared-cluster change the path to 0-bootstrap/argocd/others/1-shared-cluster . apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bootstrap-1-shared-cluster namespace: openshift-gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: default source: path: 0-bootstrap/argocd/others/1-shared-cluster repoURL: https://github.com/lsteck/multi-tenancy-gitops.git targetRevision: kustomize syncPolicy: automated: prune: true selfHeal: true In this guide we will use the unchanged 0-bootstrap/argocd/bootstrap.yaml which uses the single-cluster deployment. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: bootstrap-single-cluster namespace: openshift-gitops spec: destination: namespace: openshift-gitops server: https://kubernetes.default.svc project: default source: path: 0-bootstrap/argocd/single-cluster repoURL: https://github.com/lsteck/multi-tenancy-gitops.git targetRevision: kustomize syncPolicy: automated: prune: true selfHeal: true Under the cluster's folder there are 1-infra , 2-services and 3-apps folders which define what infrastructure, services and app resources are to be deployed respectively. ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u251c\u2500\u2500 3-apps \u251c\u2500\u2500 bootstrap.yaml \u2514\u2500\u2500 kustomization.yaml Open the kustomization.yaml file under the 1-infra folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 1-infra \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Openshift Serverless/Eventing section to deploy those resources. # Openshift Serverless/Eventing - argocd/namespace-openshift-serverless.yaml - argocd/namespace-knative-serving.yaml - argocd/namespace-knative-eventing.yaml Open the kustomization.yaml file under the 2-services folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u2514\u2500\u2500 2-services \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Openshift Serverless/Eventing section to deploy those resources. # Openshift Serverless/Eventing - argocd/operators/openshift-serverless.yaml - argocd/instances/knative-eventing-instance.yaml Installing the Serverless and Eventing doesn't require any resources under the 3-apps folder so the kustomization.yaml in that folder doesn't need to be changed. Push the changes to your forked repository. git add . git commit -m \"push serverless and eventing\" git push Create the bootstrap ArgoCD application. oc apply -f 0 -bootstrap/argocd/bootstrap.yaml -n openshift-gitops From the OpenShift console launch ArgoCD by clicking the ArgoCD link from the Applications (9 squares) menu The ArcoCD user id is admin and the password can be found in the argocd-cluster-cluster secret in the openshift-gitops project namespace. You can extract the secret with the command oc extract secret/argocd-cluster-cluster --to = - -n openshift-gitops On the ArgoCD UI you can see the newly created bootstrap application. After several minutes you will see all the other ArgoCD applications with a status Healthy and Synced . The status will progress from Missing , OutOfSync , Syncing . If you see a status of Sync failed there were errors. You can check that the Red Hat OpenShift Serverless operator that provides serverless and eventing capabilities has been installed from the Installed Operators page on the console.","title":"Installation Steps"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#install-slack-notification-app-and-configure-tekton-to-emit-cloud-events","text":"Note Both installation steps could be performed at the same time. They were broken out in this guide to illustrate how you could install Serverless and Eventing without installing the Slack Notification app.","title":"Install Slack Notification app and configure Tekton to emit Cloud Events"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#pre-requisites_1","text":"The following are required before proceeding. Compete the previous section Install Serverless and Cloud Eventing Create a Slack Incoming Webhook .","title":"Pre-requisites"},{"location":"adopting/use-cases/gitops/gitops-serverless-events/#installation-steps_1","text":"Open the kustomization.yaml file under the 1-infra folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 1-infra \u251c\u2500\u2500 1-infra.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Slack Notifications section to deploy those resources. # Slack Notifications - argocd/namespace-slack-notifications.yaml Push the changes to your forked repository. git add . git commit -m \"push slack notifications namespace\" git push After a few minutes you should see see an ArgoCD namespace-slack-notifications app. This app creates the slack-notifications project namespace where we will deploy the slack notification app. Before we deploy the app we need to create a secret to store the slack incoming webhook you created as a pre-requisite. This secret needs to be in the slack-notifications project namespace. You can generate an encrypted secret containing the slack notification webhook using the Sealed Secret Operator or you can manually create the secret as follows NOTE: replace <webhook-url> with your slack webhook url . WEBHOOK = <webhook-url> oc project slack-notifications oc create secret generic slack-secret \\ --from-literal = SLACK_URL = ${ WEBHOOK } Installing the Slack Notification app doesn't require any resources under the 2-services folder so the kustomization.yaml in that folder doesn't need to be changed. Open the kustomization.yaml file under the 3-apps folder ./0-bootstrap \u2514\u2500\u2500 argocd \u251c\u2500\u2500 bootstrap.yaml \u251c\u2500\u2500 others \u2514\u2500\u2500 single-cluster \u251c\u2500\u2500 1-infra \u251c\u2500\u2500 2-services \u2514\u2500\u2500 3-apps \u251c\u2500\u2500 3-apps.yaml \u251c\u2500\u2500 argocd \u2514\u2500\u2500 kustomization.yaml Uncomment the lines under the # Slack Notifications section to deploy those resources. # Slack Notifications - argocd/slack-notifications/slack-notifications.yaml - argocd/shared/config/openshift-pipelines/configmap/openshift-pipelines-config.yaml If you changed the slack-secret name or key you need to update the secret name and key value in the slack-notification.yaml Open slack-notifications.yaml ./0-bootstrap \u2514\u2500\u2500 argocd \u2514\u2500\u2500 single-cluster \u2514\u2500\u2500 3-apps \u2514\u2500\u2500 argocd \u2514\u2500\u2500 slack-notifications \u2514\u2500\u2500 slack-notifications.yaml Modify the name and key to match your secret name and the key name. secret: # provide name of the secret that contains slack url name: slack-secret # provide key of the secret that contains slack url key: SLACK_URL Push the changes to your forked repository. git add . git commit -m \"push slack notifications app\" git push After a few minutes you will see an apps-slack-notifications ArgoCD app and a openshift-pipelines-config ArgoCD app. On the OpenShift Console you can see the slack notifications app deployment in the slack-notifications project namespace. Now when you run Pipelines you will receive Slack Notifications.","title":"Installation Steps"},{"location":"adopting/use-cases/operator/operator/","text":"Building an operator \u00b6 Set up DevOps pipelines to build an operator and operator catalog The Cloud-Native Toolkit provides Tekton pipelines to automate the process to build and deploy operator-based applications. The operator pipeline supports Ansible, Go, and Helm-based operators that have been built using the Operator SDK. The Cloud-Native Toolkit also provides a companion Tekton pipeline to build a custom Operator Lifecycle Manager catalog that simplifies deployment of the operators to a cluster. The following steps will walk you through using the pipelines to build an operator and operator catalog and deploy them to a cluster. Prerequisites \u00b6 Operator SDK \u00b6 The Tekton pipeline for building operators requires that the operator has been created by at least v1.0.0 of the Operator SDK but we recommend using the latest available (v1.3.0 at the time of this writing). The Operator SDK structure had a major architectural change with v1.0.0 and continues to have significant improvements made with each release. If you do not have operator-sdk , follow the installation instructions to get it. (Optional) Create your operator \u00b6 These steps will walk you through building a simple operator following the Operator SDK quick-start guide. You can choose between Ansible , Go , or Helm quick start guides for the following example. Note If you already have an operator built with v1.0.0+ of the Operator SDK then you can skip to the next section. The Ansible operator quick start seems to be the simplest way to get up and running quickly and exercise the Tekton pipelines. We've included the abbreviated steps from the Ansible quick start guide below. Create a directory for your operator and change to that directory mkdir memcached-operator cd memcached-operator Initialize the operator operator-sdk init --plugins = ansible --domain = example.com where: --plugins can be either ansible , go , or helm . If --plugins is not provided then it defaults to go --domain is the base domain name used for the Custom Resource Definition generated for the operator Create an API for the operator operator-sdk create api --group cache --version v1 --kind Memcached --generate-role where: - --group is the api group for the CRD - --kind is the resource kind for the CRD - --version is the resource version for the CRD Generate metadata for Operator bundle make bundle Answer the interactive questions Create the README.md and initialize git repository echo \"# memcached operator\" > README.md git init git add . git commit -m \"Initial commit\" Create a new remote git repository on a git server such as GitHub and copy the url Push the local git repository to the remote GitHub repository git remote add origin ${ GIT_URL } git push -u origin $( git rev-parse --abbrev-ref HEAD ) At this point you have enough of a functioning operator to try out the Tekton operator pipeline. If you want to explore the operator you have created and/or enhance its capabilities, there are more detailed instructions on the operator sdk page. Register the operator pipeline \u00b6 This step is similar to what you learned in the fast-start Continuous Integration section deploying an application 1. Log into your Development Cluster from the command line \u00b6 Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Info If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding 2. Create the operator development namespace \u00b6 Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ OPERATOR_NAMESPACE } 3. Register the operator in a DevOps Pipeline \u00b6 Start the process to create a pipeline using Tekton. oc pipeline --tekton The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token The CLI will attempt to determine the runtime of your repository. It should detect that it is an operator and prompt you to select ibm-operator pipeline or ibm-operator-catalog pipeline. Select ibm-operator pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No). Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console. 4. View your application pipeline \u00b6 Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Register the operator catalog pipeline \u00b6 The operator catalog repository is a simple repo used to keep track of the operator bundles that will be packaged together in the catalog. Even though you may start with one operator, it is assumed that ultimately multiple operators will be managed together in one catalog. Open the operator catalog template repository and create a repository for the operator catalog using the template Copy the repository url Create/connect to the project for the operator development. This does not need to be the same one used for the operator build. oc sync ${ OPERATOR_NAMESPACE } Register the pipeline for the operator catalog oc pipeline --tekton ${ GIT_URL } Register the catalog repository with the operator pipeline \u00b6 The last step of the operator pipeline will update the operator catalog repository with the new version of the operator bundle that was created. This update will trigger the creation of a new version of the catalog. Change to the project where the operator pipeline was registered oc project ${ OPERATOR_NAMESPACE } Change the directory to the local clone of your operator catalog repository Register the url for the operator catalog repository for the operator pipeline igc git-secret olm-catalog-repo Kick off the operator pipeline again to update the operator catalog repo Configure the GitOps repo \u00b6 If you don't already have one, create a GitOps repo from the gitops template template Clone the GitOps repo to the local machine git clone ${ GIT_URL } Change the directory to the cloned GitOps repo Change to the project where the operator catalog pipeline was registered oc project ${ OPERATOR_NAMESPACE } Register the GitOps repo url oc gitops Trigger the operator catalog pipeline build to register the catalog source in the GitOps repo Configure ArgoCD to deploy the CatalogSource into the namespace openshift-marketplace","title":"Build an Operator"},{"location":"adopting/use-cases/operator/operator/#building-an-operator","text":"Set up DevOps pipelines to build an operator and operator catalog The Cloud-Native Toolkit provides Tekton pipelines to automate the process to build and deploy operator-based applications. The operator pipeline supports Ansible, Go, and Helm-based operators that have been built using the Operator SDK. The Cloud-Native Toolkit also provides a companion Tekton pipeline to build a custom Operator Lifecycle Manager catalog that simplifies deployment of the operators to a cluster. The following steps will walk you through using the pipelines to build an operator and operator catalog and deploy them to a cluster.","title":"Building an operator"},{"location":"adopting/use-cases/operator/operator/#prerequisites","text":"","title":"Prerequisites"},{"location":"adopting/use-cases/operator/operator/#operator-sdk","text":"The Tekton pipeline for building operators requires that the operator has been created by at least v1.0.0 of the Operator SDK but we recommend using the latest available (v1.3.0 at the time of this writing). The Operator SDK structure had a major architectural change with v1.0.0 and continues to have significant improvements made with each release. If you do not have operator-sdk , follow the installation instructions to get it.","title":"Operator SDK"},{"location":"adopting/use-cases/operator/operator/#optional-create-your-operator","text":"These steps will walk you through building a simple operator following the Operator SDK quick-start guide. You can choose between Ansible , Go , or Helm quick start guides for the following example. Note If you already have an operator built with v1.0.0+ of the Operator SDK then you can skip to the next section. The Ansible operator quick start seems to be the simplest way to get up and running quickly and exercise the Tekton pipelines. We've included the abbreviated steps from the Ansible quick start guide below. Create a directory for your operator and change to that directory mkdir memcached-operator cd memcached-operator Initialize the operator operator-sdk init --plugins = ansible --domain = example.com where: --plugins can be either ansible , go , or helm . If --plugins is not provided then it defaults to go --domain is the base domain name used for the Custom Resource Definition generated for the operator Create an API for the operator operator-sdk create api --group cache --version v1 --kind Memcached --generate-role where: - --group is the api group for the CRD - --kind is the resource kind for the CRD - --version is the resource version for the CRD Generate metadata for Operator bundle make bundle Answer the interactive questions Create the README.md and initialize git repository echo \"# memcached operator\" > README.md git init git add . git commit -m \"Initial commit\" Create a new remote git repository on a git server such as GitHub and copy the url Push the local git repository to the remote GitHub repository git remote add origin ${ GIT_URL } git push -u origin $( git rev-parse --abbrev-ref HEAD ) At this point you have enough of a functioning operator to try out the Tekton operator pipeline. If you want to explore the operator you have created and/or enhance its capabilities, there are more detailed instructions on the operator sdk page.","title":"(Optional) Create your operator"},{"location":"adopting/use-cases/operator/operator/#register-the-operator-pipeline","text":"This step is similar to what you learned in the fast-start Continuous Integration section deploying an application","title":"Register the operator pipeline"},{"location":"adopting/use-cases/operator/operator/#1-log-into-your-development-cluster-from-the-command-line","text":"Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Info If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding","title":"1. Log into your Development Cluster from the command line"},{"location":"adopting/use-cases/operator/operator/#2-create-the-operator-development-namespace","text":"Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync ${ OPERATOR_NAMESPACE }","title":"2. Create the operator development namespace"},{"location":"adopting/use-cases/operator/operator/#3-register-the-operator-in-a-devops-pipeline","text":"Start the process to create a pipeline using Tekton. oc pipeline --tekton The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token The CLI will attempt to determine the runtime of your repository. It should detect that it is an operator and prompt you to select ibm-operator pipeline or ibm-operator-catalog pipeline. Select ibm-operator pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No). Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console.","title":"3. Register the operator in a DevOps Pipeline"},{"location":"adopting/use-cases/operator/operator/#4-view-your-application-pipeline","text":"Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below.","title":"4. View your application pipeline"},{"location":"adopting/use-cases/operator/operator/#register-the-operator-catalog-pipeline","text":"The operator catalog repository is a simple repo used to keep track of the operator bundles that will be packaged together in the catalog. Even though you may start with one operator, it is assumed that ultimately multiple operators will be managed together in one catalog. Open the operator catalog template repository and create a repository for the operator catalog using the template Copy the repository url Create/connect to the project for the operator development. This does not need to be the same one used for the operator build. oc sync ${ OPERATOR_NAMESPACE } Register the pipeline for the operator catalog oc pipeline --tekton ${ GIT_URL }","title":"Register the operator catalog pipeline"},{"location":"adopting/use-cases/operator/operator/#register-the-catalog-repository-with-the-operator-pipeline","text":"The last step of the operator pipeline will update the operator catalog repository with the new version of the operator bundle that was created. This update will trigger the creation of a new version of the catalog. Change to the project where the operator pipeline was registered oc project ${ OPERATOR_NAMESPACE } Change the directory to the local clone of your operator catalog repository Register the url for the operator catalog repository for the operator pipeline igc git-secret olm-catalog-repo Kick off the operator pipeline again to update the operator catalog repo","title":"Register the catalog repository with the operator pipeline"},{"location":"adopting/use-cases/operator/operator/#configure-the-gitops-repo","text":"If you don't already have one, create a GitOps repo from the gitops template template Clone the GitOps repo to the local machine git clone ${ GIT_URL } Change the directory to the cloned GitOps repo Change to the project where the operator catalog pipeline was registered oc project ${ OPERATOR_NAMESPACE } Register the GitOps repo url oc gitops Trigger the operator catalog pipeline build to register the catalog source in the GitOps repo Configure ArgoCD to deploy the CatalogSource into the namespace openshift-marketplace","title":"Configure the GitOps repo"},{"location":"adopting/use-cases/storefront/storefront/","text":"Storefront - Cloud Native Exemplar \u00b6 Exemplar application for developers to understand the cloud native toolkit and use cases on Red Hat OpenShift. Build and deploy a cloudnative reference implementation using Cloud native toolkit \u00b6 This provides a working example of a Tekton based CICD pipeline to build and deploy a StoreFront . The Pipeline and Task resources available in Cloud Native Toolkit can be used to deploy different microservices that are part of this application. Prerequisites \u00b6 Task Instructions Active OpenShift 4.x Cluster Set up accounts and tools Instructions Install the Cloud Native Toolkit Install the Cloud Native Toolkit Deploy the StoreFront \u00b6 Get started with deploying a simple retail application, Storefront using Cloud Native Toolkit. The Storefront application implements a simple shopping application that displays a catalog of antique computing devices. People can search for and buy products from the application\u2019s web interface. The logical architecture for this reference implementation is shown in the picture below. This application has a web interface that relies on separate BFF (Backend for Frontend) services to interact with the backend data. These are several components of this architecture. This OmniChannel application contains an AngularJS based web application. The Web app invokes its own backend Microservice to fetch data, we call this component BFFs following the Backend for Frontends pattern. The Web BFF is implemented using the Node.js Express Framework. The BFFs invokes another layer of reusable Java Microservices. The reusable microservices are written in Java using Quarkus framework. The Java Microservices are as follows: The Inventory Service uses an instance of MySQL to store the inventory items. The Catalog service retrieves items from a searchable JSON data source using ElasticSearch . Keycloak delegates authentication and authorization. The Customer service stores and retrieves Customer data from a searchable JSON data source using CouchDB . The Orders Service uses an instance of MariaDB to store order information. Deploy the Storefront using Cloud native toolkit \u00b6 Follow the below guide for instructions on how to deploy all the microservices of StoreFront onto Openshift using Cloud native toolkit. Cloud native toolkit - StoreFront Quarkus version To get an idea, here is a view of a completed and successful pipeline runs for all the microservices.","title":"Storefront - Cloud Native Exemplar"},{"location":"adopting/use-cases/storefront/storefront/#storefront-cloud-native-exemplar","text":"Exemplar application for developers to understand the cloud native toolkit and use cases on Red Hat OpenShift.","title":"Storefront - Cloud Native Exemplar"},{"location":"adopting/use-cases/storefront/storefront/#build-and-deploy-a-cloudnative-reference-implementation-using-cloud-native-toolkit","text":"This provides a working example of a Tekton based CICD pipeline to build and deploy a StoreFront . The Pipeline and Task resources available in Cloud Native Toolkit can be used to deploy different microservices that are part of this application.","title":"Build and deploy a cloudnative reference implementation using Cloud native toolkit"},{"location":"adopting/use-cases/storefront/storefront/#prerequisites","text":"Task Instructions Active OpenShift 4.x Cluster Set up accounts and tools Instructions Install the Cloud Native Toolkit Install the Cloud Native Toolkit","title":"Prerequisites"},{"location":"adopting/use-cases/storefront/storefront/#deploy-the-storefront","text":"Get started with deploying a simple retail application, Storefront using Cloud Native Toolkit. The Storefront application implements a simple shopping application that displays a catalog of antique computing devices. People can search for and buy products from the application\u2019s web interface. The logical architecture for this reference implementation is shown in the picture below. This application has a web interface that relies on separate BFF (Backend for Frontend) services to interact with the backend data. These are several components of this architecture. This OmniChannel application contains an AngularJS based web application. The Web app invokes its own backend Microservice to fetch data, we call this component BFFs following the Backend for Frontends pattern. The Web BFF is implemented using the Node.js Express Framework. The BFFs invokes another layer of reusable Java Microservices. The reusable microservices are written in Java using Quarkus framework. The Java Microservices are as follows: The Inventory Service uses an instance of MySQL to store the inventory items. The Catalog service retrieves items from a searchable JSON data source using ElasticSearch . Keycloak delegates authentication and authorization. The Customer service stores and retrieves Customer data from a searchable JSON data source using CouchDB . The Orders Service uses an instance of MariaDB to store order information.","title":"Deploy the StoreFront"},{"location":"adopting/use-cases/storefront/storefront/#deploy-the-storefront-using-cloud-native-toolkit","text":"Follow the below guide for instructions on how to deploy all the microservices of StoreFront onto Openshift using Cloud native toolkit. Cloud native toolkit - StoreFront Quarkus version To get an idea, here is a view of a completed and successful pipeline runs for all the microservices.","title":"Deploy the Storefront using Cloud native toolkit"},{"location":"contribute/automation/","text":"Automation used to deliver the Toolkit \u00b6 Todo Complete this content from the overview session recently run - video to be posted soon","title":"Build Automation"},{"location":"contribute/automation/#automation-used-to-deliver-the-toolkit","text":"Todo Complete this content from the overview session recently run - video to be posted soon","title":"Automation used to deliver the Toolkit"},{"location":"contribute/contribute/","text":"Contributing to the project \u00b6 Cloud-Native Toolkit \u00b6 Cloud-Native Toolkit is an organized effort managed by the IBM Garage to help development teams build and deliver cloud-native solutions deployed to IBM Cloud with IBM Kubernetes and Red Hat OpenShift Managed Service. The work is managed as an Open Source project and contribution is welcome from anyone that would like to participate. The following guidelines will help everyone navigate the process of contributing. (Most of these steps will generally apply to any Open Source project.) Project structure \u00b6 Before diving into the steps, it's important to get a general understanding of the organization structure of the repositories. The various components that make up the Cloud-Native Toolkit are organized into different git repositories that are aligned around solution space, release schedule, and intended audience. Currently, these repositories are divided between two public Git organizations: github.com/ibm-garage-cloud github.com/IBM All of the repositories in the github.com/ibm-garage-cloud organization belong to the Cloud-Native Toolkit. Within the github.com/IBM organization only the following repositories contain Cloud-Native Toolkit content: IBM/template-graphql-typescript IBM/template-java-spring IBM/template-node-angular IBM/template-node-react IBM/template-node-typescript IBM/ibm-garage-tekton-tasks IBM/template-argocd-gitops IBM/template-ibmcloud-services (These repositories have been relocated to the github.com/IBM because their content applies more broadly than the Cloud-Native Toolkit and/or fits in with the larger catalog of related Code Pattern and Starter Kit content). How to contribute \u00b6 1. Look through the issues \u00b6 Info Prerequisite: We use ZenHub to add metadata to the Git issues so they can be managed via a Kanban board. ZenHub also allows us to show the issues and pull requests from the various repositories in on place. Viewing the issues with ZenHub requires a Chrome extension . We highly recommend installing this extension to help search the issues. Whether you have a question/issue with existing behavior or an idea for a new feature that can be added to the Toolkit, the place to begin is looking through the issues. Each repository has its own list of issues. If you have an issue with content from a specific repository then any related issues will likely be found there. However, given the wide range of repositories and the interconnected nature of many of the components it is usually best to look at the ZenHub board. The following steps are the best way to view the board: Open the github.com/ibm-garage-cloud/planning repository Click on the ZenHub link along the top menu. Note: You must install the ZenHub extension for the link to appear Look at the Repos drop-down in the top-right of the board to ensure all the issues for the various Cloud-Native Toolkit repositories have been displayed. If the drop-down does not read Repos (24/24) then do the following to select them all: Click on the Repos drop-down Select the Show all repos link Use the filters and/or the search bar to find cards related to your issue. If you find an existing ticket that is related to your issue, add your information to the card. Existing defect \u00b6 If you find an existing defect, be sure to add a comment with the details of your specific scenario. When the defect has been addressed and the ticket has been moved to the Review/QA stage, we will ask you to help test the solution in your environment. Existing feature \u00b6 If you find an existing feature request, please indicate your interest through a comment. As appropriate, include the business requirements or user story that is driving the request. We will use this input to help determine prioritization of new features. No issue found \u00b6 If you are unable to find a card that is related to your issue or feature, proceed to the next step to create a new issue. The search doesn't need to be an exhaustive one and if there is any question whether the item is new or a duplicate, go ahead and create a new issue. We'd rather have the item captured and mark it as a duplicate after the fact, if necessary, than to have an issue fall through the cracks. 2. Create a new issue \u00b6 If you could not find an existing issue related to your problem or feature then its time to create a new issue. Issues fall in one of two categories: Bug Feature Bug \u00b6 Bugs can be either be reported against the repository that has the problem or in the general github.com/ibm-garage-cloud/planning repository. These are the steps for reporting a bug: Navigate to the repository in the browser Click on the Issues menu item at the top then click on New Issue Click Get Started on the Bug report template to create a new issue from the template. Note At the moment, some repositories do not have Bug report templates defined. If a template does not exist you will see the blank issue dialog. 4. Provide all the information relevant to the bug, particularly the details of your scenario and the steps to reproduce Feature \u00b6 Features should be reported against the github.com/ibm-garage-cloud/planning repository. These are the steps for requesting a feature: Navigate to the github.com/ibm-garage-cloud/planning repository in the browser Click on the Issues menu item at the top then click on New Issue Click Get Started on the Feature request template to create a new issue from the template Provide all the information relevant to the bug, particularly the details of the problem addressed by the feature and the impact/benefit of implementing the feature 3. Setup your repository for development \u00b6 On an Open Source project there are typically a few maintainers who have direct access to the repositories and a larger number of contributors who do not. In this case, the way to introduce changes to the repositories is through a fork. The process for setting up the fork is as follows: Note Even though the maintainers have direct access to the repositories, we follow this same process of working off of a fork of the upstream repository as a best practice. Clone the repository to which you want to introduce changes (the upstream repository) to your local machine git clone { upstream repo url } Create a fork of the upstream repository by pressing the Fork button at the top of the page Copy the url of the repository fork Open a terminal to the cloned directory and run the following to set the push url to be the url of the repository fork git remote set-url --push origin { fork repo url } List the remotes for the local repository clone to verify that the fetch and push urls are pointing to the appropriate repositories git remote -v The output should look something like the following, with the fetch pointing to the upstream url and the push pointing to the fork With the local clone set up this way, any time you get changes from the remote (e.g. git pull ) the changes will be pulled from the upstream repository. Similarly, when you push changes they will be pushed to the fork . 4. Develop your changes \u00b6 Now that you have a fork and a local clone of the repository, you can start making your changes. This part is mostly business-as-usual for software development. We have a couple of best practices we recommend: Work in a branch \u00b6 It is a best practice to make your changes in a separate branch, even though the changes will be made in your own fork. There are at least two good reasons for doing this: The branch can be named after the issue number and the feature Naming the branch according to the change that is being made provides a bit of documentation for the purpose of the branch. It also helps enforce the notion that the branch exists only for the implementation of that feature. The branch can be rebased when new changes come in from the upstream Through the course of development of the branch, other changes may be introduced in the upstream repository. Making the changes in a separate branch allows for the upstream changes to be easily pulled in on the master branch and applied to other branches as appropriate. Create the branch by running: git checkout -b { branch name } Push the branch up to your fork by running: git push -u origin { branch name } Commit message format \u00b6 Each commit message should clearly describe the changes that are being made. During the development process as many small changes are made, a single one-liner is sufficient for the commit message. With larger changes or when the changes in the branch are squashed into a single commit, the following commit message format is preferred. Writing commit messages \u00b6 <type> indicates the type of commit that\u2019s being made. This can be: feat , fix , perf , docs , chore , style , refactor . <scope> the scope could be anything specifying place of the commit change or the thing(s) that changed. <subject> the subject should be a short overview of the change. <body> the body should be a detailed description of the change(s). This can be prose or a bulleted listing. <issue reference> the issue reference should be a reference to the issue number under which this change is being made. The issue reference should be in the format of {git org}/{git repo}#{issue number} Commit message format: type(<scope>): <subject> <BLANK LINE> <body> <BLANK LINE> <issue reference> Create a draft pull request when the branch is first pushed to the fork \u00b6 GitHub recently introduced draft pull requests that allow a pull request to be recorded but marked as not yet ready for review. Git provides a url to open a pull request the first time a branch is first pushed to the remote, which gives an excellent and easy opportunity to create the draft. Note Be sure to link the pull request with the issue Creating a draft pull request early has the following benefits: Clicking the link provided by Git sets up the source and target repos/branches for you so you don't need to hunt around Having the draft pull request gives insight for everyone else where the work is being done Push changes to your fork frequently during development \u00b6 As you are making changes, push them frequently to the fork. This ensures that your code is backed up somewhere and allows everyone else to see what activity is happening. It also means that if you get pulled into some other work, the latest version of your changes are available for others to possibly pick up where you left off. Pull in the latest changes from master frequently and rebase your branch \u00b6 It is good to make sure you are always working off of the latest code from the upstream. With the changes in a separate branch, it is easy to bring in upstream changes with the following steps: Checkout master in the local clone git checkout master Pull in the changes from the upstream repository git pull Check out your branch git checkout { branch } Rebase your branch on master git rebase master Force push the changes git push --force-with-lease 5. Create your pull request \u00b6 GitHub has recently added a new feature that allows a pull request to put into draft status. This is helpful to record a pull request as pending work even if the changes are not yet ready for review. Open your fork repository in a browser Click New pull request Select the appropriate target and source branches for the pull request and press Create pull request base repository is the target repository into which your changes should be merged (should be the upstream repository) base is the target branch in the upstream repository into which your changes should be merged (typically this will be master ) head repository is the source of the changes (this should be your fork repository) compare is the branch containing your changes that should be merged Write the title and description of the pull request How to write the perfect pull request Link the pull request to the related issue Click Create pull request or Create draft pull request to submit Note A pull request can be converted to a draft after it was created by clicking on the Convert to draft link located under the Reviewers section on the right-hand side. 6. Prepare your branch to submit the pull request for review \u00b6 Pull in the latest changes from master frequently and rebase your branch, as described in the previous section (Optionally) Rebase your branch to squash commits and clean up commit messages. An interactive rebase will allow you to clean up your branch before submitting it for review. This will reduce the number of commits down to the core set of changes that reflect the feature/bug fix, remove any commits that aren't part of the change you are making, and clean up the commit messages to clearly describe the changes and follow the commit message format guidelines. 7. Submit your pull request for review \u00b6 Assuming you have previously created a draft pull request, when you are ready to have your code reviewed and merged then you will need to indicate that in the pull request. Open the browser to the upstream repository. Select the Pull requests tab, find your pull request in the list and open it. Press the Ready for review button to tell the maintainers the pull request is ready to be processed. 8. Update your pull request \u00b6 Keep an eye on the pull request after it has been submitted for review. The maintainers may have questions or request changes before the pull request can be closed. The GitHub system should notify you when changes are made to your pull request. Also, the maintainers all have day jobs and sometimes pull requests get overlooked. If your pull request has sat for a while you can get some attention to it by tagging one of the maintainers in a commit comment: E.g. @seansund @csantana @bwoolf1 @lsteck","title":"Contribute"},{"location":"contribute/contribute/#contributing-to-the-project","text":"","title":"Contributing to the project"},{"location":"contribute/contribute/#cloud-native-toolkit","text":"Cloud-Native Toolkit is an organized effort managed by the IBM Garage to help development teams build and deliver cloud-native solutions deployed to IBM Cloud with IBM Kubernetes and Red Hat OpenShift Managed Service. The work is managed as an Open Source project and contribution is welcome from anyone that would like to participate. The following guidelines will help everyone navigate the process of contributing. (Most of these steps will generally apply to any Open Source project.)","title":"Cloud-Native Toolkit"},{"location":"contribute/contribute/#project-structure","text":"Before diving into the steps, it's important to get a general understanding of the organization structure of the repositories. The various components that make up the Cloud-Native Toolkit are organized into different git repositories that are aligned around solution space, release schedule, and intended audience. Currently, these repositories are divided between two public Git organizations: github.com/ibm-garage-cloud github.com/IBM All of the repositories in the github.com/ibm-garage-cloud organization belong to the Cloud-Native Toolkit. Within the github.com/IBM organization only the following repositories contain Cloud-Native Toolkit content: IBM/template-graphql-typescript IBM/template-java-spring IBM/template-node-angular IBM/template-node-react IBM/template-node-typescript IBM/ibm-garage-tekton-tasks IBM/template-argocd-gitops IBM/template-ibmcloud-services (These repositories have been relocated to the github.com/IBM because their content applies more broadly than the Cloud-Native Toolkit and/or fits in with the larger catalog of related Code Pattern and Starter Kit content).","title":"Project structure"},{"location":"contribute/contribute/#how-to-contribute","text":"","title":"How to contribute"},{"location":"contribute/contribute/#1-look-through-the-issues","text":"Info Prerequisite: We use ZenHub to add metadata to the Git issues so they can be managed via a Kanban board. ZenHub also allows us to show the issues and pull requests from the various repositories in on place. Viewing the issues with ZenHub requires a Chrome extension . We highly recommend installing this extension to help search the issues. Whether you have a question/issue with existing behavior or an idea for a new feature that can be added to the Toolkit, the place to begin is looking through the issues. Each repository has its own list of issues. If you have an issue with content from a specific repository then any related issues will likely be found there. However, given the wide range of repositories and the interconnected nature of many of the components it is usually best to look at the ZenHub board. The following steps are the best way to view the board: Open the github.com/ibm-garage-cloud/planning repository Click on the ZenHub link along the top menu. Note: You must install the ZenHub extension for the link to appear Look at the Repos drop-down in the top-right of the board to ensure all the issues for the various Cloud-Native Toolkit repositories have been displayed. If the drop-down does not read Repos (24/24) then do the following to select them all: Click on the Repos drop-down Select the Show all repos link Use the filters and/or the search bar to find cards related to your issue. If you find an existing ticket that is related to your issue, add your information to the card.","title":"1. Look through the issues"},{"location":"contribute/contribute/#2-create-a-new-issue","text":"If you could not find an existing issue related to your problem or feature then its time to create a new issue. Issues fall in one of two categories: Bug Feature","title":"2. Create a new issue"},{"location":"contribute/contribute/#3-setup-your-repository-for-development","text":"On an Open Source project there are typically a few maintainers who have direct access to the repositories and a larger number of contributors who do not. In this case, the way to introduce changes to the repositories is through a fork. The process for setting up the fork is as follows: Note Even though the maintainers have direct access to the repositories, we follow this same process of working off of a fork of the upstream repository as a best practice. Clone the repository to which you want to introduce changes (the upstream repository) to your local machine git clone { upstream repo url } Create a fork of the upstream repository by pressing the Fork button at the top of the page Copy the url of the repository fork Open a terminal to the cloned directory and run the following to set the push url to be the url of the repository fork git remote set-url --push origin { fork repo url } List the remotes for the local repository clone to verify that the fetch and push urls are pointing to the appropriate repositories git remote -v The output should look something like the following, with the fetch pointing to the upstream url and the push pointing to the fork With the local clone set up this way, any time you get changes from the remote (e.g. git pull ) the changes will be pulled from the upstream repository. Similarly, when you push changes they will be pushed to the fork .","title":"3. Setup your repository for development"},{"location":"contribute/contribute/#4-develop-your-changes","text":"Now that you have a fork and a local clone of the repository, you can start making your changes. This part is mostly business-as-usual for software development. We have a couple of best practices we recommend:","title":"4. Develop your changes"},{"location":"contribute/contribute/#5-create-your-pull-request","text":"GitHub has recently added a new feature that allows a pull request to put into draft status. This is helpful to record a pull request as pending work even if the changes are not yet ready for review. Open your fork repository in a browser Click New pull request Select the appropriate target and source branches for the pull request and press Create pull request base repository is the target repository into which your changes should be merged (should be the upstream repository) base is the target branch in the upstream repository into which your changes should be merged (typically this will be master ) head repository is the source of the changes (this should be your fork repository) compare is the branch containing your changes that should be merged Write the title and description of the pull request How to write the perfect pull request Link the pull request to the related issue Click Create pull request or Create draft pull request to submit Note A pull request can be converted to a draft after it was created by clicking on the Convert to draft link located under the Reviewers section on the right-hand side.","title":"5. Create your pull request"},{"location":"contribute/contribute/#6-prepare-your-branch-to-submit-the-pull-request-for-review","text":"Pull in the latest changes from master frequently and rebase your branch, as described in the previous section (Optionally) Rebase your branch to squash commits and clean up commit messages. An interactive rebase will allow you to clean up your branch before submitting it for review. This will reduce the number of commits down to the core set of changes that reflect the feature/bug fix, remove any commits that aren't part of the change you are making, and clean up the commit messages to clearly describe the changes and follow the commit message format guidelines.","title":"6. Prepare your branch to submit the pull request for review"},{"location":"contribute/contribute/#7-submit-your-pull-request-for-review","text":"Assuming you have previously created a draft pull request, when you are ready to have your code reviewed and merged then you will need to indicate that in the pull request. Open the browser to the upstream repository. Select the Pull requests tab, find your pull request in the list and open it. Press the Ready for review button to tell the maintainers the pull request is ready to be processed.","title":"7. Submit your pull request for review"},{"location":"contribute/contribute/#8-update-your-pull-request","text":"Keep an eye on the pull request after it has been submitted for review. The maintainers may have questions or request changes before the pull request can be closed. The GitHub system should notify you when changes are made to your pull request. Also, the maintainers all have day jobs and sometimes pull requests get overlooked. If your pull request has sat for a while you can get some attention to it by tagging one of the maintainers in a commit comment: E.g. @seansund @csantana @bwoolf1 @lsteck","title":"8. Update your pull request"},{"location":"contribute/documentation/","text":"Updating the Developer Guide \u00b6 Setting up a documentation environment \u00b6 To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Tooling within a Docker container You can use a Docker container and run MkDocs from the container, so no local installation is required: You need to have Docker installed and running on your system There are helper configurations installed if you have npm from Node.JS installed. Build the development docker container image, this is only need it once if the dependencies have not changed. Run npm run dev:build To start developing run command npm run dev in the root directory of the git repo (where package.json and mkdocs.yaml are located) Open a browser to http://localhost:8000 , where you will see the documentation site. This will live update as you save changes to the Markdown files in the docs folder To stop developing run command npm dev:stop in another terminal window, which will terminate the docker container View the scripts section of package.json in the root folder of the git repo for additional options available To build the static HTML file and check all links and spelling run command npm run build To check links in the built site ( npm run build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website. Document layout \u00b6 The documentation is organized into distinct sections to provide easy navigation and allow self-service viewers to get started, then dive into deeper content with a clear path. The sections are as follows: Overview - This is high level information about the Cloud-Native Toolkit. What it is and why does it exist. Install - This is a section designed to help the first time user get up and running with minimal knowledge. More advanced install options are provided in a later section Learning - This section is designed to teach the fundamentals of Cloud-Native development with the toolkit. Using a default installation of the Toolkit to work through Continuous Integration and Continuous Delivery. This section is more about Cloud-Native development and how the Toolkit provides functionality to support Cloud-Native development rather than a deep dive on specific tools that are part of the toolkit. Adopting - This section is designed to move from the initial learning phase to adopting the Toolkit as part of development activities. It covers more advanced installation options, customization options, best practices and how the Toolkit can be applied to certain use cases Reference - The reference section is the technical documentation for the resources delivered by the Toolkit. Resources - The resources section provides links to content outside the toolkit that someone learning the toolkit may find useful Contributing - This section provides how someone can become a contributor to the Cloud-Native Toolkit project, which includes the core Toolkit, adding additional starter kits or pipeline tasks, updating or adding to the documentation. Creating content \u00b6 MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page. Standard Markdown features \u00b6 The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions. Links within MkDocs generated content \u00b6 MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information Extensions used in the prototype \u00b6 There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here Link configuration \u00b6 Links on the page or embedded images can be annotated to control the links and also the appearance of the links: Image \u00b6 Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center } External Links \u00b6 External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can use {: target=_blank} to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank} YouTube videos \u00b6 It is possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube-nocookie.com/embed/V-BFLaPdoPo\" title = \"YouTube video player\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe > Tabs \u00b6 Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World Information boxes \u00b6 The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note Supported Admonition Classes \u00b6 The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote Code blocks \u00b6 Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ``` shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content Advanced highlighting of code blocks \u00b6 There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation . Line numbers \u00b6 You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected Code highlighting \u00b6 You can highlight specific lines of code using the hl_lines directive. The line numbers starts at 1 for the first line of code. If you want multiple lines highlighted, then provide the line numbers separated with a space. Below lines 1 and 3 are highlighted: ``` javascript hl_lines=\"1 3\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> It is possible to combine line number and highlighting. Below I start the line numbers at 10 and highlight the second line of code: ``` javascript linenums=\"10\" hl_lines=\"2\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 10 11 12 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Redirects \u00b6 To help external sites wanting to link to the documentation there are a number of vanity links maintained using the redirect plugin . The links are defined in the mkdocs.yml file and documented on the Additional Resources page. To ensure the auto-generated link page does not get reported by the link checker, an entry needs to be added to the nofollow section of the link checker config file, linkcheckerrc in the root directory of the project. E.g. if a link /help was created then an entry in the nofollow section should be public/help.html$ . Spell checking \u00b6 This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project. Adding local words \u00b6 You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment. Adding global words \u00b6 The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Documentation"},{"location":"contribute/documentation/#updating-the-developer-guide","text":"","title":"Updating the Developer Guide"},{"location":"contribute/documentation/#setting-up-a-documentation-environment","text":"To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Tooling within a Docker container You can use a Docker container and run MkDocs from the container, so no local installation is required: You need to have Docker installed and running on your system There are helper configurations installed if you have npm from Node.JS installed. Build the development docker container image, this is only need it once if the dependencies have not changed. Run npm run dev:build To start developing run command npm run dev in the root directory of the git repo (where package.json and mkdocs.yaml are located) Open a browser to http://localhost:8000 , where you will see the documentation site. This will live update as you save changes to the Markdown files in the docs folder To stop developing run command npm dev:stop in another terminal window, which will terminate the docker container View the scripts section of package.json in the root folder of the git repo for additional options available To build the static HTML file and check all links and spelling run command npm run build To check links in the built site ( npm run build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Clone or Fork the documentation repository cd into the documentation directory Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command npm run dev:links . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling npm run dev:spell should be run in the root folder of the project, containing the cspell.json file. The developer guide is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website, that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website.","title":"Setting up a documentation environment"},{"location":"contribute/documentation/#document-layout","text":"The documentation is organized into distinct sections to provide easy navigation and allow self-service viewers to get started, then dive into deeper content with a clear path. The sections are as follows: Overview - This is high level information about the Cloud-Native Toolkit. What it is and why does it exist. Install - This is a section designed to help the first time user get up and running with minimal knowledge. More advanced install options are provided in a later section Learning - This section is designed to teach the fundamentals of Cloud-Native development with the toolkit. Using a default installation of the Toolkit to work through Continuous Integration and Continuous Delivery. This section is more about Cloud-Native development and how the Toolkit provides functionality to support Cloud-Native development rather than a deep dive on specific tools that are part of the toolkit. Adopting - This section is designed to move from the initial learning phase to adopting the Toolkit as part of development activities. It covers more advanced installation options, customization options, best practices and how the Toolkit can be applied to certain use cases Reference - The reference section is the technical documentation for the resources delivered by the Toolkit. Resources - The resources section provides links to content outside the toolkit that someone learning the toolkit may find useful Contributing - This section provides how someone can become a contributor to the Cloud-Native Toolkit project, which includes the core Toolkit, adding additional starter kits or pipeline tasks, updating or adding to the documentation.","title":"Document layout"},{"location":"contribute/documentation/#creating-content","text":"MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. For the prototype, the existing Gatsby content is still within the repository, with the prototype content located in the mkdocs directory. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the page navigation on the right of the page.","title":"Creating content"},{"location":"contribute/documentation/#standard-markdown-features","text":"The following markdown syntax is used within the documentation Todo Complete the table below Syntax Result # Title a level 1 heading containing. You can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions.","title":"Standard Markdown features"},{"location":"contribute/documentation/#links-within-mkdocs-generated-content","text":"MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information","title":"Links within MkDocs generated content"},{"location":"contribute/documentation/#extensions-used-in-the-prototype","text":"There are a number of Markdown extensions being used in the prototype. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here","title":"Extensions used in the prototype"},{"location":"contribute/documentation/#link-configuration","text":"Links on the page or embedded images can be annotated to control the links and also the appearance of the links:","title":"Link configuration"},{"location":"contribute/documentation/#tabs","text":"Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World","title":"Tabs"},{"location":"contribute/documentation/#information-boxes","text":"The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note","title":"Information boxes"},{"location":"contribute/documentation/#code-blocks","text":"Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ``` shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content","title":"Code blocks"},{"location":"contribute/documentation/#redirects","text":"To help external sites wanting to link to the documentation there are a number of vanity links maintained using the redirect plugin . The links are defined in the mkdocs.yml file and documented on the Additional Resources page. To ensure the auto-generated link page does not get reported by the link checker, an entry needs to be added to the nofollow section of the link checker config file, linkcheckerrc in the root directory of the project. E.g. if a link /help was created then an entry in the nofollow section should be public/help.html$ .","title":"Redirects"},{"location":"contribute/documentation/#spell-checking","text":"This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project.","title":"Spell checking"},{"location":"contribute/documentation/#adding-local-words","text":"You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment.","title":"Adding local words"},{"location":"contribute/documentation/#adding-global-words","text":"The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Adding global words"},{"location":"contribute/governance/","text":"Governance \u00b6 Roles and responsibilities \u00b6 As mentioned above, we value participation from anyone that is interested in this space. For the Cloud-Native Toolkit participation can take on a number of different forms. The following roles detail a number of ways in which people might interact with the Toolkit. Consumers \u00b6 Consumers are members of the community who are applying assets to their development projects. Anyone who wants to apply any of the assets can be a user. We encourage consumers to participate as evangelists and contributors as well. Evangelists \u00b6 Evangelists are members of the community who help others become consumers of the assets. They do so by: Advertising the assets and encouraging others to use them Supporting new consumers and answering questions, such as on Slack (IBM internal) Reporting bugs or missing features through GitHub issues Contributors \u00b6 Contributors are members of the community who help maintain, improve, and expand the assets. In addition to using and evangelizing the assets, they make the assets better by: Resolving issues in GitHub to fix bugs, add features, and improve documentation Submitting changes as GitHub pull requests Maintainers \u00b6 Project maintainers (aka maintainers) are owners of the project who are committed to the success of the assets in that project. Each project has a team of maintainers, and each team has a lead. In addition to their participation as contributors, maintainers have privileges to: Label, close, and manage GitHub issues Close and merge GitHub pull requests Nominate and vote on new maintainers Types of teams \u00b6 Core team \u00b6 Core team members are IBM employees responsible for the leadership and strategic direction of the set of Catalyst projects as a whole. The core team also directs how the Catalyst strategy will evolve with IBM Cloud product decisions. Core team responsibilities include: Actively engaging with the projects' communities Setting overall direction and vision Setting priorities and release schedule Focusing on broad, cross-cutting concerns Spinning up or shutting down project teams The core team will operate the technical steering committee. Technical steering committee \u00b6 The technical steering committee coordinates the project teams to ensure consistency between the projects and fosters collaboration between the core team and each project team. This close communication on cross-cutting concerns greatly mitigates the risk of misalignment that can come from decentralized efforts. The committee consists of the project leads of all of the projects as well as other members of the core team who may not presently be leading any projects. Project teams \u00b6 Each project team maintains the assets in its project. Therefore, its members are the maintainers of the assets. Each project operates independently, though it should follow this governance structure to define roles, responsibilities, and decision-making protocols. The project has a project lead, a lead maintainer who should also be a member of the technical steering committee. Each project lead is responsible for: Acting as a point of primary contact for the team Participating in the technical steering committee Deciding on the initial membership of project maintainers (in consultation with the core team) Determining and publishing project team policies and mechanics, including the way maintainers join and leave the team (which should be based on team consensus) Communicating core vision to the team Ensuring that issues and pull requests progress at a reasonable rate Making final decisions in cases where the team is unable to reach consensus (should be rare) The way that project teams communicate internally and externally is left to each team, but: Technical discussion should take place in the public domain as much as possible, ideally in GitHub issues and pull requests. Each project should have a dedicated Slack channel (IBM internal). Decisions from Slack discussions should be captured in GitHub issues. Project teams should actively seek out discussion and input from stakeholders who are not members of the team. Project Governance \u00b6 Planning \u00b6 Project planning is managed in a Kanban board , specifically this Zenhub board: Planning Zenhub Decision-making \u00b6 Project teams should use consensus decision making as much as possible, but resort to lack of consensus decision making when necessary. Consensus \u00b6 Project teams use consensus decision-making with the premise that a successful outcome is not where one side of a debate has \"won,\" but rather where concerns from all sides have been addressed in some way. This emphatically does not mean design by committee, nor compromised design. Rather, it's a recognition that every design or implementation choice carries a trade-off and numerous costs. There is seldom a 100% right answer. Breakthrough thinking sometimes end up changing the playing field by eliminating tradeoffs altogether, but more often, difficult decisions have to be made. The key is to have a clear vision and set of values and priorities , which is the core team's responsibility to set and communicate, and the project teams' responsibility to act upon. Whenever possible, seek to reach consensus through discussion and design iteration. Concretely, the steps are: New GitHub issue or pull request is created with initial analysis of tradeoffs. Comments reveal additional drawbacks, problems, or tradeoffs. The issue or pull request is revised to address comments, often by improving the design or implementation. Repeat above until \"major objections\" are fully addressed, or it's clear that there is a fundamental choice to be made. Consensus is reached when most people are left with only \"minor\" objections. While they might choose the tradeoffs slightly differently, they do not feel a strong need to actively block the issue or pull request from progressing. One important question is: consensus among which people, exactly? Of course, the broader the consensus, the better. When a decision in a project team affects other teams (e.g. new/changed API), the team will be encouraged to invite people (e.g. leads) from affected teams. But at the very least, consensus within the members of the project team should be the norm for most decisions . If the core team has done its job of communicating the values and priorities, it should be possible to fit the debate about the issue into that framework and reach a fairly clear outcome. Lack of consensus \u00b6 In some cases, though, consensus cannot be reached. These cases tend to split into two very different camps: \"Trivial\" reasons , e.g., there is not widespread agreement about naming, but there is consensus about the substance. \"Deep\" reasons , e.g., the design fundamentally improves one set of concerns at the expense of another, and people on both sides feel strongly about it. In either case, an alternative form of decision-making is needed. For the \"trivial\" case, the project lead will make an executive decision or defer the decision to another maintainer on the team. For the \"deep\" case, the project lead is empowered to make a final decision, but should consult with the core team before doing so. Contribution process \u00b6 Catalyst assets are typically stored in GitHub repositories and use a fork and pull request workflow for contributions. Specific instructions can be found in each project's GitHub CONTRIBUTING.md file. Contributor License Agreement \u00b6 We require contributors outside of IBM to sign our Contributor License Agreement (CLA) before code contributions can be reviewed and merged.","title":"Governance"},{"location":"contribute/governance/#governance","text":"","title":"Governance"},{"location":"contribute/governance/#roles-and-responsibilities","text":"As mentioned above, we value participation from anyone that is interested in this space. For the Cloud-Native Toolkit participation can take on a number of different forms. The following roles detail a number of ways in which people might interact with the Toolkit.","title":"Roles and responsibilities"},{"location":"contribute/governance/#consumers","text":"Consumers are members of the community who are applying assets to their development projects. Anyone who wants to apply any of the assets can be a user. We encourage consumers to participate as evangelists and contributors as well.","title":"Consumers"},{"location":"contribute/governance/#evangelists","text":"Evangelists are members of the community who help others become consumers of the assets. They do so by: Advertising the assets and encouraging others to use them Supporting new consumers and answering questions, such as on Slack (IBM internal) Reporting bugs or missing features through GitHub issues","title":"Evangelists"},{"location":"contribute/governance/#contributors","text":"Contributors are members of the community who help maintain, improve, and expand the assets. In addition to using and evangelizing the assets, they make the assets better by: Resolving issues in GitHub to fix bugs, add features, and improve documentation Submitting changes as GitHub pull requests","title":"Contributors"},{"location":"contribute/governance/#maintainers","text":"Project maintainers (aka maintainers) are owners of the project who are committed to the success of the assets in that project. Each project has a team of maintainers, and each team has a lead. In addition to their participation as contributors, maintainers have privileges to: Label, close, and manage GitHub issues Close and merge GitHub pull requests Nominate and vote on new maintainers","title":"Maintainers"},{"location":"contribute/governance/#types-of-teams","text":"","title":"Types of teams"},{"location":"contribute/governance/#core-team","text":"Core team members are IBM employees responsible for the leadership and strategic direction of the set of Catalyst projects as a whole. The core team also directs how the Catalyst strategy will evolve with IBM Cloud product decisions. Core team responsibilities include: Actively engaging with the projects' communities Setting overall direction and vision Setting priorities and release schedule Focusing on broad, cross-cutting concerns Spinning up or shutting down project teams The core team will operate the technical steering committee.","title":"Core team"},{"location":"contribute/governance/#project-teams","text":"Each project team maintains the assets in its project. Therefore, its members are the maintainers of the assets. Each project operates independently, though it should follow this governance structure to define roles, responsibilities, and decision-making protocols. The project has a project lead, a lead maintainer who should also be a member of the technical steering committee. Each project lead is responsible for: Acting as a point of primary contact for the team Participating in the technical steering committee Deciding on the initial membership of project maintainers (in consultation with the core team) Determining and publishing project team policies and mechanics, including the way maintainers join and leave the team (which should be based on team consensus) Communicating core vision to the team Ensuring that issues and pull requests progress at a reasonable rate Making final decisions in cases where the team is unable to reach consensus (should be rare) The way that project teams communicate internally and externally is left to each team, but: Technical discussion should take place in the public domain as much as possible, ideally in GitHub issues and pull requests. Each project should have a dedicated Slack channel (IBM internal). Decisions from Slack discussions should be captured in GitHub issues. Project teams should actively seek out discussion and input from stakeholders who are not members of the team.","title":"Project teams"},{"location":"contribute/governance/#project-governance","text":"","title":"Project Governance"},{"location":"contribute/governance/#planning","text":"Project planning is managed in a Kanban board , specifically this Zenhub board: Planning Zenhub","title":"Planning"},{"location":"contribute/governance/#decision-making","text":"Project teams should use consensus decision making as much as possible, but resort to lack of consensus decision making when necessary.","title":"Decision-making"},{"location":"contribute/governance/#contribution-process","text":"Catalyst assets are typically stored in GitHub repositories and use a fork and pull request workflow for contributions. Specific instructions can be found in each project's GitHub CONTRIBUTING.md file.","title":"Contribution process"},{"location":"contribute/governance/#contributor-license-agreement","text":"We require contributors outside of IBM to sign our Contributor License Agreement (CLA) before code contributions can be reviewed and merged.","title":"Contributor License Agreement"},{"location":"contribute/migration/","text":"Site Migration \u00b6 This page is a place holder to send users when a link is removed, either because the content was retired or the content got move to other page(s). To help the migration we need it to create redirects for some use cases: - For pages that remain then a redirect to the new location url - For pages that got removed a redirect to a page explaining what happened with the content. It could be the content got spread into other pages or the page was not relevant anymore. Updates \u00b6 2021/05/28 \u00b6 Previous version of the site is located here Redirect = accessing the old link will be redirect to the new location with the same content Removed = accessing the old link will be forwarded to this page Status Previous Location Current Location Redirect Overview Overview/What is the Cloud Native Redirect What's New Overview/What's New Redirect Getting Started/Overview Learning the Toolkit/Basics/Learning Overview Redirect Getting Started/Prerequisites Learning the Toolkit/Developer setup Redirect Getting Started/Developer Env Setup Resources/IBM Cloud/Cloud shell Redirect Getting Started/CLI Reference Redirect Getting Started/ICC Resources/IBM Cloud/ICC tool Redirect Getting Started/Dev-Ops Concepts Adopting the Toolkit/Best Practices/DevOps Redirect Workshop/Overview Learning the Toolkit/Try it/Overview Redirect Workshop/Setup Workshop Environment Learning the Toolkit/Try it/Setup Cluster Redirect Workshop/Setup Terminal Learning the Toolkit/Try it/Setup Terminal Redirect Workshop/CI Pipelines with Tekton Learning the Toolkit/Try it/Continuous integration Redirect Workshop/CD GitOps with Argo Learning the Toolkit/Try it/Continuous deployment Redirect Workshop/Microservices Inventory App Learning the Toolkit/Try it/3-tier application Redirect Workshop/App Modernization Learning the Toolkit/Try it/Application Modernization Redirect Workshop/Artificial Intelligence Learning the Toolkit/Try it/Artificial Intelligence Redirect Day 0 - Install/Overview IBM Cloud/roles Redirect Day 0 - Install/Prepare the account Advanced setup options/Configuring IBM Cloud based setup Redirect Day 0 - Install/Provision a cluster Advanced setup options/Install IBM Cloud cluster from catalog Redirect Day 0 - Install/Install the toolkit Install/Fast Start install Redirect Day 0 - Install/Complete the configuration Toolkit administration Redirect Day 1 - Build and deploy/Overview Learning the Toolkit/Cloud-Native Development with the Toolkit Redirect Day 1 - Build and deploy/Deploy an app Learning the Toolkit/Continuous Integration Redirect Day 1 - Build and deploy/Developer Dashboard Reference/Developer Dashboard Redirect Day 1 - Build and deploy/(Advanced) Build an operator Use Cases/Build an Operator Redirect Day 2 - Run and manage Learning the Toolkit/In Depth/DevOps Redirect Tool Guides/Overview Overview/What is the Cloud Native Toolkit#Environment components Redirect Tool Guides/CI with Tekton Reference/Toolkit Components/OpenShift Pipelines Redirect Tool Guides/CI with Jenkins Reference/Toolkit Components/Jenkins Redirect Tool Guides/IBM Cloud Container Registry Reference/Toolkit Components/IBM Container Registry Redirect Tool Guides/Artifact mgmt with Artifactory Reference/Toolkit Components/Artifactory Redirect Tool Guides/Code analysis with SonarQube Reference/Toolkit Components/Sonarqube Redirect Tool Guides/Contract testing with PACT Reference/Toolkit Components/PACT Redirect Tool Guides/Log mgmt with LogDNA Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard#logdna Redirect Tool Guides/Monitoring with Sysdig Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard#sysdig Redirect Tool Guides/CD with ArgoCD Learning the Toolkit/Continuous Deployment Redirect Tool Guides/Secret mgmt with Key Protect Reference/Toolkit Components/IBM Key Protect Removed Tool Guides/Architecture as Code with SolSA -- Removed Tool Guides/CLI Tools Image -- Removed Administrator Guide/Overview -- Redirect Administrator Guide/Install CodeReady Workspaces Advanced setup options/Install CodeReady Containers Redirect Administrator Guide/Configure the Dashboard Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard Redirect Administrator Guide/Artifactory Setup Adopting the Toolkit/Toolkit administration/Artifactory setup Redirect Administrator Guide/Sysdig Setup Adopting the Toolkit/Toolkit administration/Sysdig setup Redirect Administrator Guide/Terraform Modules Reference/Iteration Zero#terraform-modules Redirect Administrator Guide/Cluster Configuration Adopting the Toolkit/Toolkit administration/cluster configuration Redirect Administrator Guide/Destroying Reference/Iteration Zero#scripts Removed Architecture Guide/Environment Architecture -- Removed Architecture Guide/Development Teams -- Removed Architecture Guide/Application Architecture -- Removed Architecture Guide/Build to Manage -- Redirect Starter Kits/Overview Reference/Starter Kits/Start Kits overview Redirect Starter Kits/Git Repos Reference/Starter Kits/Available Starter Kits Redirect Learning Journey/Cloud-Native Bootcamp Cloud Native BootCamp Removed Learning Journey/Intermediate Topics -- Removed Learning Journey/Advanced Topics -- Removed Learning Journey/Cloud-Native Development -- Removed Learning Journey/Cloud-Native Deployment -- Removed Learning Journey/OpenShift Learning -- Removed Learning Journey/Garage Method Development -- Redirect Learning Journey/App Connect Pipeline Adopting the Toolkit/Use Cases/App Connect REST API workflow Redirect Learning Journey/GitOps and Integration Adopting the Toolkit/Use Cases/GitOps with Toolkit Removed Resources/Videos Adopting the Toolkit/Use Cases/GitOps with App Connect Removed Resources/Office Hours -- Redirect Resources/Resource Access Management Resources/IBM Cloud/Access control Redirect How to contribute/ Contributing/Contribute","title":"Site Migration"},{"location":"contribute/migration/#site-migration","text":"This page is a place holder to send users when a link is removed, either because the content was retired or the content got move to other page(s). To help the migration we need it to create redirects for some use cases: - For pages that remain then a redirect to the new location url - For pages that got removed a redirect to a page explaining what happened with the content. It could be the content got spread into other pages or the page was not relevant anymore.","title":"Site Migration"},{"location":"contribute/migration/#updates","text":"","title":"Updates"},{"location":"contribute/migration/#20210528","text":"Previous version of the site is located here Redirect = accessing the old link will be redirect to the new location with the same content Removed = accessing the old link will be forwarded to this page Status Previous Location Current Location Redirect Overview Overview/What is the Cloud Native Redirect What's New Overview/What's New Redirect Getting Started/Overview Learning the Toolkit/Basics/Learning Overview Redirect Getting Started/Prerequisites Learning the Toolkit/Developer setup Redirect Getting Started/Developer Env Setup Resources/IBM Cloud/Cloud shell Redirect Getting Started/CLI Reference Redirect Getting Started/ICC Resources/IBM Cloud/ICC tool Redirect Getting Started/Dev-Ops Concepts Adopting the Toolkit/Best Practices/DevOps Redirect Workshop/Overview Learning the Toolkit/Try it/Overview Redirect Workshop/Setup Workshop Environment Learning the Toolkit/Try it/Setup Cluster Redirect Workshop/Setup Terminal Learning the Toolkit/Try it/Setup Terminal Redirect Workshop/CI Pipelines with Tekton Learning the Toolkit/Try it/Continuous integration Redirect Workshop/CD GitOps with Argo Learning the Toolkit/Try it/Continuous deployment Redirect Workshop/Microservices Inventory App Learning the Toolkit/Try it/3-tier application Redirect Workshop/App Modernization Learning the Toolkit/Try it/Application Modernization Redirect Workshop/Artificial Intelligence Learning the Toolkit/Try it/Artificial Intelligence Redirect Day 0 - Install/Overview IBM Cloud/roles Redirect Day 0 - Install/Prepare the account Advanced setup options/Configuring IBM Cloud based setup Redirect Day 0 - Install/Provision a cluster Advanced setup options/Install IBM Cloud cluster from catalog Redirect Day 0 - Install/Install the toolkit Install/Fast Start install Redirect Day 0 - Install/Complete the configuration Toolkit administration Redirect Day 1 - Build and deploy/Overview Learning the Toolkit/Cloud-Native Development with the Toolkit Redirect Day 1 - Build and deploy/Deploy an app Learning the Toolkit/Continuous Integration Redirect Day 1 - Build and deploy/Developer Dashboard Reference/Developer Dashboard Redirect Day 1 - Build and deploy/(Advanced) Build an operator Use Cases/Build an Operator Redirect Day 2 - Run and manage Learning the Toolkit/In Depth/DevOps Redirect Tool Guides/Overview Overview/What is the Cloud Native Toolkit#Environment components Redirect Tool Guides/CI with Tekton Reference/Toolkit Components/OpenShift Pipelines Redirect Tool Guides/CI with Jenkins Reference/Toolkit Components/Jenkins Redirect Tool Guides/IBM Cloud Container Registry Reference/Toolkit Components/IBM Container Registry Redirect Tool Guides/Artifact mgmt with Artifactory Reference/Toolkit Components/Artifactory Redirect Tool Guides/Code analysis with SonarQube Reference/Toolkit Components/Sonarqube Redirect Tool Guides/Contract testing with PACT Reference/Toolkit Components/PACT Redirect Tool Guides/Log mgmt with LogDNA Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard#logdna Redirect Tool Guides/Monitoring with Sysdig Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard#sysdig Redirect Tool Guides/CD with ArgoCD Learning the Toolkit/Continuous Deployment Redirect Tool Guides/Secret mgmt with Key Protect Reference/Toolkit Components/IBM Key Protect Removed Tool Guides/Architecture as Code with SolSA -- Removed Tool Guides/CLI Tools Image -- Removed Administrator Guide/Overview -- Redirect Administrator Guide/Install CodeReady Workspaces Advanced setup options/Install CodeReady Containers Redirect Administrator Guide/Configure the Dashboard Adopting the Toolkit/Customizing the Toolkit/Customizing the dashboard Redirect Administrator Guide/Artifactory Setup Adopting the Toolkit/Toolkit administration/Artifactory setup Redirect Administrator Guide/Sysdig Setup Adopting the Toolkit/Toolkit administration/Sysdig setup Redirect Administrator Guide/Terraform Modules Reference/Iteration Zero#terraform-modules Redirect Administrator Guide/Cluster Configuration Adopting the Toolkit/Toolkit administration/cluster configuration Redirect Administrator Guide/Destroying Reference/Iteration Zero#scripts Removed Architecture Guide/Environment Architecture -- Removed Architecture Guide/Development Teams -- Removed Architecture Guide/Application Architecture -- Removed Architecture Guide/Build to Manage -- Redirect Starter Kits/Overview Reference/Starter Kits/Start Kits overview Redirect Starter Kits/Git Repos Reference/Starter Kits/Available Starter Kits Redirect Learning Journey/Cloud-Native Bootcamp Cloud Native BootCamp Removed Learning Journey/Intermediate Topics -- Removed Learning Journey/Advanced Topics -- Removed Learning Journey/Cloud-Native Development -- Removed Learning Journey/Cloud-Native Deployment -- Removed Learning Journey/OpenShift Learning -- Removed Learning Journey/Garage Method Development -- Redirect Learning Journey/App Connect Pipeline Adopting the Toolkit/Use Cases/App Connect REST API workflow Redirect Learning Journey/GitOps and Integration Adopting the Toolkit/Use Cases/GitOps with Toolkit Removed Resources/Videos Adopting the Toolkit/Use Cases/GitOps with App Connect Removed Resources/Office Hours -- Redirect Resources/Resource Access Management Resources/IBM Cloud/Access control Redirect How to contribute/ Contributing/Contribute","title":"2021/05/28"},{"location":"learning/analysis/","text":"Static Analysis \u00b6 Static analysis tools are a valuable part of a software development toolkit to identify potential issues. Static analysis is not restricted to source code. It can be applied to tooling configuration, such as a Dockerfile and also the the output of the build process, such as a container image. There are a number of analysis tools used in a default install of the Cloud-Native Toolkit. SonarQube \u00b6 SonarQube is a code analysis tool. It works across multiple languages and can identify a number of issues within the source code, such as: common coding errors like forgetting the this keyword or using assign operator '=' instead of the equality operator '==', which create valid syntax for the language, but the incorrect functionality security issues, such as using versions of libraries or packages that have known security issues; Using older encryption technologies for authentication where better alternatives now exist identifying poorly written code, such as overly complex code or duplicated code SonarQube can be configured with quality gates, so poorly written or buggy code will fail analysis, but minor issues will be highlighted but still pass analysis. The standard pipelines used by the Cloud-Native Toolkit use SonarQube analysis. Hadolint \u00b6 Hadolint is a linter that checks Dockerfiles to ensure that container best practices are being used. Vulnerability advisor \u00b6 The vulnerability advisor scans container images to identify if the container contains any packages that contain known security contained in operating system packages. Some scanning applications will also check application dependencies contained in the container image. The Cloud-Native Toolkit uses 2 scanning applications: IBM Vulnerability Advisor for containers stored in the IBM Container registry on the IBM Cloud Trivy is an open source tool providing vulnerability scanning The pipelines installed by the Cloud-Native Toolkit include vulnerability scanning. However, the scan can be controlled by setting a variable on the pipeline to opt to skip the scan or perform the scan.","title":"Static Analysis"},{"location":"learning/analysis/#static-analysis","text":"Static analysis tools are a valuable part of a software development toolkit to identify potential issues. Static analysis is not restricted to source code. It can be applied to tooling configuration, such as a Dockerfile and also the the output of the build process, such as a container image. There are a number of analysis tools used in a default install of the Cloud-Native Toolkit.","title":"Static Analysis"},{"location":"learning/analysis/#sonarqube","text":"SonarQube is a code analysis tool. It works across multiple languages and can identify a number of issues within the source code, such as: common coding errors like forgetting the this keyword or using assign operator '=' instead of the equality operator '==', which create valid syntax for the language, but the incorrect functionality security issues, such as using versions of libraries or packages that have known security issues; Using older encryption technologies for authentication where better alternatives now exist identifying poorly written code, such as overly complex code or duplicated code SonarQube can be configured with quality gates, so poorly written or buggy code will fail analysis, but minor issues will be highlighted but still pass analysis. The standard pipelines used by the Cloud-Native Toolkit use SonarQube analysis.","title":"SonarQube"},{"location":"learning/analysis/#hadolint","text":"Hadolint is a linter that checks Dockerfiles to ensure that container best practices are being used.","title":"Hadolint"},{"location":"learning/analysis/#vulnerability-advisor","text":"The vulnerability advisor scans container images to identify if the container contains any packages that contain known security contained in operating system packages. Some scanning applications will also check application dependencies contained in the container image. The Cloud-Native Toolkit uses 2 scanning applications: IBM Vulnerability Advisor for containers stored in the IBM Container registry on the IBM Cloud Trivy is an open source tool providing vulnerability scanning The pipelines installed by the Cloud-Native Toolkit include vulnerability scanning. However, the scan can be controlled by setting a variable on the pipeline to opt to skip the scan or perform the scan.","title":"Vulnerability advisor"},{"location":"learning/dev-setup/","text":"Cloud-Native Toolkit Developer Setup \u00b6 Before starting the learning activities, you need to setup your development environment. This section assumes you have the development tools for your preferred programming language already installed. It only covers the tooling for the Cloud-Native Toolkit. Prerequisites \u00b6 To complete the cloud-Native Toolkit learning, you need to have a laptop or workstation running Linux, MacOS or MS Windows. If running linux you may need to adapt some of the instructions to match your Linux distribution a web browser, capable of running modern web sites an active, public, GitHub account access to a Red Hat OpenShift cluster. If one has not been provided or you did not set one up please work through the Install section to get access to a cluster. This guide assumes that you have some basic knowledge of Kubernetes, Docker, and modern software delivery techniques including CI/CD. There are some suggested educational materials listed in the Resources section. Cluster Command Line Tools \u00b6 You will be using the command line for some of the tasks you will learn about as you work through the learning material. If you are using the Open Labs environment, then you can skip installing command line tools locally as the Open Labs environment provides a web-based command line. If you are working with a cluster running on the IBM Cloud then you have the option to use the IBM Cloud Shell rather than a local command prompt. If you opt to use the IBM Cloud Shell then don't need any local command line tools installed. If you installed your own cluster following the fast-start instructions then you should already have the command line tools installed. If you don't need, or already have the command line tools installed then you can jump to the User accounts section, other wise continue to install the required tools. Installing Command Line Tools \u00b6 The set of command line tools you need depends on the cluster you will be using to complete the learning: IBM Cloud based clusters need the IBM Cloud Command Line Interface (CLI) OpenShift clusters need the RedHat oc command IBM Cloud CLI \u00b6 To install the IBM Cloud CLI follow the instructions in the IBM Cloud documentation . RedHat oc command \u00b6 The oc command is available from all installations of RedHat OpenShift or CodeReady Containers. Navigate and log into the web console for the cluster, then in the dropdown accessed by clicking the help icon (a question mark next to you username at the top of the web console) you will find a link to the install images for various operating systems. The install images are also available to download from RedHat . Be sure to get the latest version of the oc command. User accounts \u00b6 You'll need the following accounts to complete the Cloud-Native Toolkit learning. Github account \u00b6 Todo CRC cannot be triggered by github - what to use? You will need a GitHub account (public, not enterprise) to use the Starter Kit templates. Create one if you do not have one already. If you have not logged in for a while, make sure your login is working. Ensure you have a git command line tool installed. You can verify you have a working git command line interface by entering the command git version in a command or terminal window. If you don't have git installed then you can download it from here Configure a Github personal access token \u00b6 For the automation you will learn as part of the Continuous Integration process you will need a GitHub personal access token with public_repo and write:repo_hook scopes. A Personal Access Token is used in place of a user password to authenticate with GitHub. The Personal Access Token only needs to be generated once because it is associated with a GitHub user or organization and can be used to access any of the user's/organization's repositories. Navigate to Developer Settings and generate a new token; name it something like \"CI pipeline\" Select public_repo scope to enable git clone Select write:repo_hook scope so the pipeline can create a web hook The GitHub UI will never again let you see this token, so be sure to save the token in your password manager or somewhere safe that you can access later on IBM Cloud account \u00b6 If you will be using a cluster hosted in the IBM cloud, then you need to have an active IBM Cloud account. Create an IBM Cloud account , if you don't have one already, and make sure you can log in. Create an IBM Cloud API Key \u00b6 API Keys are tokens scoped to a particular IBM Cloud account that can be used to access cloud services, particularly through the IBM Cloud CLI. Generate an API Key for whichever account contains the cluster you will be using for the Getting Started activities. Follow these steps to create an API key and download the key to a file. Be sure to include a descriptive name of the API Key so you know where it is used. Install the Cloud-Native Toolkit Command Line Interface (CLI) \u00b6 The Cloud-Native Toolkit provides extensions to the kubernetes and OpenShift command line tools to provide convenient helper functions to speed up development activities. The additional commands are detailed in the reference section. The Cloud-Native Toolkit CLI needs to be installed in all development environments (including the OpenLabs and IBM Cloud Shell environments) To install the Cloud-Native Toolkit CLI run the following command: npm i -g @ibmgaragecloud/cloud-native-toolkit-cli Note If you have access to multiple IBM Cloud accounts you may find the IBM Cloud cluster fast-switching tool (icc) of use. The icc tool is installed as part of the Cloud-Native Toolkit CLI. If you are working on the IBM Cloud there is a browser based shell environment you can use. Information about setting up and accessing the browser based shell to work with the Cloud-Native Toolkit can be found in the resources section","title":"Developer setup"},{"location":"learning/dev-setup/#cloud-native-toolkit-developer-setup","text":"Before starting the learning activities, you need to setup your development environment. This section assumes you have the development tools for your preferred programming language already installed. It only covers the tooling for the Cloud-Native Toolkit.","title":"Cloud-Native Toolkit Developer Setup"},{"location":"learning/dev-setup/#prerequisites","text":"To complete the cloud-Native Toolkit learning, you need to have a laptop or workstation running Linux, MacOS or MS Windows. If running linux you may need to adapt some of the instructions to match your Linux distribution a web browser, capable of running modern web sites an active, public, GitHub account access to a Red Hat OpenShift cluster. If one has not been provided or you did not set one up please work through the Install section to get access to a cluster. This guide assumes that you have some basic knowledge of Kubernetes, Docker, and modern software delivery techniques including CI/CD. There are some suggested educational materials listed in the Resources section.","title":"Prerequisites"},{"location":"learning/dev-setup/#cluster-command-line-tools","text":"You will be using the command line for some of the tasks you will learn about as you work through the learning material. If you are using the Open Labs environment, then you can skip installing command line tools locally as the Open Labs environment provides a web-based command line. If you are working with a cluster running on the IBM Cloud then you have the option to use the IBM Cloud Shell rather than a local command prompt. If you opt to use the IBM Cloud Shell then don't need any local command line tools installed. If you installed your own cluster following the fast-start instructions then you should already have the command line tools installed. If you don't need, or already have the command line tools installed then you can jump to the User accounts section, other wise continue to install the required tools.","title":"Cluster Command Line Tools"},{"location":"learning/dev-setup/#installing-command-line-tools","text":"The set of command line tools you need depends on the cluster you will be using to complete the learning: IBM Cloud based clusters need the IBM Cloud Command Line Interface (CLI) OpenShift clusters need the RedHat oc command","title":"Installing Command Line Tools"},{"location":"learning/dev-setup/#user-accounts","text":"You'll need the following accounts to complete the Cloud-Native Toolkit learning.","title":"User accounts"},{"location":"learning/dev-setup/#github-account","text":"Todo CRC cannot be triggered by github - what to use? You will need a GitHub account (public, not enterprise) to use the Starter Kit templates. Create one if you do not have one already. If you have not logged in for a while, make sure your login is working. Ensure you have a git command line tool installed. You can verify you have a working git command line interface by entering the command git version in a command or terminal window. If you don't have git installed then you can download it from here","title":"Github account"},{"location":"learning/dev-setup/#ibm-cloud-account","text":"If you will be using a cluster hosted in the IBM cloud, then you need to have an active IBM Cloud account. Create an IBM Cloud account , if you don't have one already, and make sure you can log in.","title":"IBM Cloud account"},{"location":"learning/dev-setup/#install-the-cloud-native-toolkit-command-line-interface-cli","text":"The Cloud-Native Toolkit provides extensions to the kubernetes and OpenShift command line tools to provide convenient helper functions to speed up development activities. The additional commands are detailed in the reference section. The Cloud-Native Toolkit CLI needs to be installed in all development environments (including the OpenLabs and IBM Cloud Shell environments) To install the Cloud-Native Toolkit CLI run the following command: npm i -g @ibmgaragecloud/cloud-native-toolkit-cli Note If you have access to multiple IBM Cloud accounts you may find the IBM Cloud cluster fast-switching tool (icc) of use. The icc tool is installed as part of the Cloud-Native Toolkit CLI. If you are working on the IBM Cloud there is a browser based shell environment you can use. Information about setting up and accessing the browser based shell to work with the Cloud-Native Toolkit can be found in the resources section","title":"Install the Cloud-Native Toolkit Command Line Interface (CLI)"},{"location":"learning/devops/","text":"Dev-Ops Concepts \u00b6 This short video introduces the concepts of DevOps with Red Hat OpenShift: Continuous Delivery \u00b6 In IBM Garage Method, one of the Develop practices is continuous delivery . A preferred model for implementing continuous delivery is GitOps, where the desired state of the operational environment is defined in a source control repository (namely Git). What is continuous delivery \u00b6 Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: Continuous delivery deploys an application when a user manually triggers deployment Continuous deployment deploys an application automatically when it is ready Typically, continuous deployment is an evolution of a continuous delivery process. An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. Once these tests have been automated in a way that reliably verifies the application components then the deployment can be automated. Additionally, for continuous delivery it important to employ other best practices around moving and managing changes in an environment: blue-green deployments, shadow deployments, and feature toggles to name a few. Until these practices are in place and verified, it is best to stick with continuous delivery. As with most cloud-native practices, the move from continuous deployment to continuous delivery would not be done in a \"big bang\" but incrementally and as different application components are ready. What is GitOps \u00b6 GitOps is the operational pattern of using source code repositories (namely Git) as the source of truth for defining the configuration that makes up the desired state of the operational environment. Git repositories are used to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: Git provides a versioned history of changes, listing what was changed and who made the change Change releases can be managed from a pull request, allowing multiple people to make changes but a select few to approve the changes Git provides access control mechanisms to limit who can change and view the configuration Git enables changes to be rolled back quickly if there is an issue with a new release Git supports multiple models for change management: Branches, Forks, GitFlow, etc Hosted git providers (like GitHub) provide a rich API that allows the different operations to be automated, if desired CI/CD integration \u00b6 For the full end-to-end build and delivery process, both the CI and CD pipelines are used. When working in a containerized environment such as Kubernetes or Red Hat OpenShift, the responsibilities between the two processes are clearly defined: The CI pipeline is responsible for building validating and packaging the \"raw materials\" (source code, deployment configuration, etc) into versioned, deployable artifacts (container images, helm charts, published artifacts, etc) The CD pipeline is responsible for applying the deployable artifacts into a particular target environment A change made to one of the source repositories triggers the CI process. The CI process builds, validates, and packages those changes into deployable artifacts that are stored in the image registry and artifact repository(ies). The last step of the CI process updates the GitOps repository with information about the updated artifacts. At a minimum this step stores updates the version number to the newly released versions of the artifacts but depending on the environment this step might also update the deployment configuration. Note It is also possible to trigger a process when a new image is available in the image registry or a new artifact is available to the artifact management system. In this case, the CI process could be split into two parts: 1. create the container image and artifacts, and 2. update the GitOps repo with the available artifacts. Changes to the GitOps repository trigger the CD pipeline to run In the CD pipeline, the configuration describing the desired state as defined in the GitOps repository is reconciled with the actual state of the environment and resources are created, updated, or destroyed as appropriate. Tools to support Continuous Delivery \u00b6 The practice of (CD) can be accomplished in different ways and with different tools. It is possible and certainly valid to use the same tool for both CI and CD (e.g. Tekton or Jenkins) with caution you enforce a clear separation between the two processes. Typically, that would result in two distinct pipelines to respond to changes that happen within the two different Git repositories - source repo and gitops repo. Another class of tools is available that are particularly suited for Continuous Delivery and GitOps. The following is by no means an exhaustive list but it does provide some of the common tools used for CD in a cloud-native environment: ArgoCD Flux IBM Multicloud Manager","title":"DevOps"},{"location":"learning/devops/#dev-ops-concepts","text":"This short video introduces the concepts of DevOps with Red Hat OpenShift:","title":"Dev-Ops Concepts"},{"location":"learning/devops/#continuous-delivery","text":"In IBM Garage Method, one of the Develop practices is continuous delivery . A preferred model for implementing continuous delivery is GitOps, where the desired state of the operational environment is defined in a source control repository (namely Git).","title":"Continuous Delivery"},{"location":"learning/devops/#what-is-continuous-delivery","text":"Continuous delivery is the DevOps approach of frequently making new versions of an application's components available for deployment to a runtime environment. The process involves automation of the build and validation process and concludes with a new version of the application that is available for promotion to another environment. Continuous delivery is closely related to continuous deployment. The distinction is: Continuous delivery deploys an application when a user manually triggers deployment Continuous deployment deploys an application automatically when it is ready Typically, continuous deployment is an evolution of a continuous delivery process. An application is ready for deployment when it passes a set of tests that prove it doesn't contain any significant problems. Once these tests have been automated in a way that reliably verifies the application components then the deployment can be automated. Additionally, for continuous delivery it important to employ other best practices around moving and managing changes in an environment: blue-green deployments, shadow deployments, and feature toggles to name a few. Until these practices are in place and verified, it is best to stick with continuous delivery. As with most cloud-native practices, the move from continuous deployment to continuous delivery would not be done in a \"big bang\" but incrementally and as different application components are ready.","title":"What is continuous delivery"},{"location":"learning/devops/#what-is-gitops","text":"GitOps is the operational pattern of using source code repositories (namely Git) as the source of truth for defining the configuration that makes up the desired state of the operational environment. Git repositories are used to declaratively represent the desired state of applications in deployment environments. GitOps takes advantage of several Git features: Git provides a versioned history of changes, listing what was changed and who made the change Change releases can be managed from a pull request, allowing multiple people to make changes but a select few to approve the changes Git provides access control mechanisms to limit who can change and view the configuration Git enables changes to be rolled back quickly if there is an issue with a new release Git supports multiple models for change management: Branches, Forks, GitFlow, etc Hosted git providers (like GitHub) provide a rich API that allows the different operations to be automated, if desired","title":"What is GitOps"},{"location":"learning/devops/#cicd-integration","text":"For the full end-to-end build and delivery process, both the CI and CD pipelines are used. When working in a containerized environment such as Kubernetes or Red Hat OpenShift, the responsibilities between the two processes are clearly defined: The CI pipeline is responsible for building validating and packaging the \"raw materials\" (source code, deployment configuration, etc) into versioned, deployable artifacts (container images, helm charts, published artifacts, etc) The CD pipeline is responsible for applying the deployable artifacts into a particular target environment A change made to one of the source repositories triggers the CI process. The CI process builds, validates, and packages those changes into deployable artifacts that are stored in the image registry and artifact repository(ies). The last step of the CI process updates the GitOps repository with information about the updated artifacts. At a minimum this step stores updates the version number to the newly released versions of the artifacts but depending on the environment this step might also update the deployment configuration. Note It is also possible to trigger a process when a new image is available in the image registry or a new artifact is available to the artifact management system. In this case, the CI process could be split into two parts: 1. create the container image and artifacts, and 2. update the GitOps repo with the available artifacts. Changes to the GitOps repository trigger the CD pipeline to run In the CD pipeline, the configuration describing the desired state as defined in the GitOps repository is reconciled with the actual state of the environment and resources are created, updated, or destroyed as appropriate.","title":"CI/CD integration"},{"location":"learning/devops/#tools-to-support-continuous-delivery","text":"The practice of (CD) can be accomplished in different ways and with different tools. It is possible and certainly valid to use the same tool for both CI and CD (e.g. Tekton or Jenkins) with caution you enforce a clear separation between the two processes. Typically, that would result in two distinct pipelines to respond to changes that happen within the two different Git repositories - source repo and gitops repo. Another class of tools is available that are particularly suited for Continuous Delivery and GitOps. The following is by no means an exhaustive list but it does provide some of the common tools used for CD in a cloud-native environment: ArgoCD Flux IBM Multicloud Manager","title":"Tools to support Continuous Delivery"},{"location":"learning/fast-cd/","text":"Continuous Delivery - fast start \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo. Configuring GitOps with Argo CD \u00b6 Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution Set up the GitOps repo \u00b6 Todo Is this better in the developer setup section? Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD Starter Kit used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD Starter Kit https://github.com/IBM/template-argocd-gitops/generate . click Use this template - If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test Hook the CI pipeline to the CD pipeline \u00b6 The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the repositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc. If you used the Toolkit CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the Toolkit CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the Toolkit CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD. Configure Release namespaces \u00b6 ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Create a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets Register the GitOps repo in ArgoCD \u00b6 Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops Create a project in Argo CD \u00b6 In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note: The --dest and --src arguments can be provided multiple times if there are multiple destinations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f project.yaml -n tools Add an application in Argo CD for each application component \u00b6 Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The ArgoCD project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) revision - The Git branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools Managing secrets in Argo CD \u00b6 The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation . Prepare the Key Protect instance \u00b6 As the name suggests, the Argo CD Key Protect plugin leverages the capabilities of the Key Protect service to manage the protected information. The details for setting up and managing the Key Protect instance can be found in Secret management with Key Protect . From those instructions you can find the information required for the subsequent steps. Create the secret configuration \u00b6 The input to the plugin is a directory that contains one or more yaml \"secret templates\". In this case the \"secret template\" provides the structure of the desired secret with placeholders for the values that will be pulled from the key management system. Create a directory to contain the secret configuration. The Argo CD Starter Kit repository has a template in templates/secrets-plugin that can be copied as a starting point Update the values in the yaml file for the secret that will be created apiVersion : keymanagement.ibm/v1 kind : SecretTemplate metadata : name : mysecret annotations : key-manager : key-protect key-protect/instanceId : instance-id key-protect/region : us-east spec : labels : {} annotations : {} values : - name : url value : https://ibm.com - name : username b64value : dGVhbS1jYXA= - name : password keyId : 36397b07-d98d-4c0b-bd7a-d6c290163684 The metadata.annotations value is optional. key-manager - the only value supported currently is key-protect key-protect/instanceId - the instance id of the key protect instance. If not provided then the instance-id value from the key-protect-access secret will be used. key-protect/region - the region where the key protect instance has been provisioned. If not provided then the region value from the key-protect-access secret will be used. The metadata.name value given will be used as the name for the Secret that will be generated. The information in spec.labels and spec.annotations will be copied over as the labels and annotations in the Secret that is generated The spec.values section contains the information that should be provided in the data section of the generated Secret. There are three possible ways the values can be provided: value - the actual value can be provided directly as clear text. This would be appropriate for information that is not sensitive but is required in the secret b64value - a base64 encoded value can be provided to the secret. This can be used for large values that might present formatting issues or for information that is not sensitive but that might be obfuscated a bit (like a username) keyId - the id (not the name) of the Standard Key that has been stored in Key Protect. The value stored in Key Protect can be anything Commit the changes to the GitOps repository Add the secret application in Argo CD \u00b6 Once the configuration has been added to the GitOps repository, Argo CD needs to be configured to deploy the secrets. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored revision - The branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed Plugin - In the last section of the UI select Plugin from the dropdown key-protect-secret - Click in the name field and select key-protect-secret from the dropdown Repeat that step for each secret application and environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when SSO authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } \\ --config-management-plugin key-protect-secret where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named secret-application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } plugin : name : key-protect-secret syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f secret-application.yaml -n tools Configure another cluster as an Argo CD deployment target \u00b6 Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the cluster api server can be selected as a deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"Continuous Delivery"},{"location":"learning/fast-cd/#continuous-delivery-fast-start","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo.","title":"Continuous Delivery - fast start"},{"location":"learning/fast-cd/#configuring-gitops-with-argo-cd","text":"Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution","title":"Configuring GitOps with Argo CD"},{"location":"learning/fast-cd/#set-up-the-gitops-repo","text":"Todo Is this better in the developer setup section? Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD Starter Kit used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD Starter Kit https://github.com/IBM/template-argocd-gitops/generate . click Use this template - If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test","title":"Set up the GitOps repo"},{"location":"learning/fast-cd/#hook-the-ci-pipeline-to-the-cd-pipeline","text":"The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the repositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc. If you used the Toolkit CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the Toolkit CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the Toolkit CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD.","title":"Hook the CI pipeline to the CD pipeline"},{"location":"learning/fast-cd/#configure-release-namespaces","text":"ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Create a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets","title":"Configure Release namespaces"},{"location":"learning/fast-cd/#register-the-gitops-repo-in-argocd","text":"Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops","title":"Register the GitOps repo in ArgoCD"},{"location":"learning/fast-cd/#create-a-project-in-argo-cd","text":"In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note: The --dest and --src arguments can be provided multiple times if there are multiple destinations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f project.yaml -n tools","title":"Create a project in Argo CD"},{"location":"learning/fast-cd/#add-an-application-in-argo-cd-for-each-application-component","text":"Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. Argo CD UI Log into Argo CD user interface Press New Application and provide the following values: application name - The name of the application. It is recommend to use the format of {namespace}-{image name} project - The ArgoCD project with which the application should be included sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) revision - The Git branch where the configuration for this instance is stored path - The path within the repository where the application config is located (should be the application name) destination cluster - The cluster url for the deployment destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools","title":"Add an application in Argo CD for each application component"},{"location":"learning/fast-cd/#managing-secrets-in-argo-cd","text":"The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation .","title":"Managing secrets in Argo CD"},{"location":"learning/fast-cd/#configure-another-cluster-as-an-argo-cd-deployment-target","text":"Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the cluster api server can be selected as a deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"Configure another cluster as an Argo CD deployment target"},{"location":"learning/fast-ci/","text":"Continuous Integration - fast start \u00b6 Overview \u00b6 The environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. The approach for getting started is exactly the same for an environment based on Kubernetes or Red Hat OpenShift . Note The instructions provided below lean heavily on the use of the IGC Command Line Interface (CLI) tool, to both show how the CLI works in context and to streamline the process (the reason for creating the CLI in the first place). However, the use of the CLI is in no way required to use the Cloud-Native Toolkit. If you would prefer to work through these instructions without the use of the CLI, we have provided the equivalent manual steps for each command on the Cloud-Native Toolkit CLI page. The video below (click to play the video) demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster. Create an application \u00b6 1. Log into your Development Cluster from the command line \u00b6 Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Note If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding 2. Create the development namespace \u00b6 Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync -p ${ DEV_NAMESPACE } Info The -p or --tekton flag adds additional privileges to the tekton pipeline service account. The equivalent command is oc adm policy add-scc-to-user privileged -z pipeline . In OpenShift 4.7, and later versions, this additional privilege is needed for the buildah task. 3. Open the Developer Dashboard \u00b6 The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of pre-configured Starter Kits that make seeding your development project very easy. Before starting, open a browser and make sure you are logged into Github . There are two options for how to access the dashboard: OpenShift console Open the Application Launcher dropdown from the top-right and select Developer Dashboard Command Line oc dashboard 4. Create your app in Git \u00b6 Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Warning Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. From the Developer Dashboard, click on Starter Kits tab Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Starter Kit Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization. Create the DevOps pipeline \u00b6 5. Register the application in a DevOps Pipeline \u00b6 Info We will be using the pipeline command of the IBM Garage Cloud cli to register the DevOps pipeline. The pipeline command gives an option for both Jenkins and Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline. oc pipeline ${ GIT_URL } For example: oc pipeline https://github.com/gct-showcase/inventory-svc For the deployment of your first app with OpenShift select Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console. 6. View your application pipeline \u00b6 The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. Tekton OpenShift 4.x Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Kubernetes Open the Developer Dashboard kubectl dashboard Select the Tekton tile to launch the Tekton UI Select your development project Jenkins OpenShift 4.x Open the OpenShift Web Console oc console OR From the left-hand menu, select Builds -> Build Configs Select your project from the drop-down menu at the top The registered pipeline should appear in the list Kubernetes Run the command oc dashboard in your terminal to open your Developer Dashboard Select the Jenkins tool to open the Jenkins dashboard Run the command kubectl credentials in your terminal to get the list of logins for the tools Use the Jenkins userid and password to log into the Jenkins dashboard Wait for the pipeline stages to start building. Once the stages have completed, you will see a view similar to the one below. 7. View your application artifacts \u00b6 Todo What happens if the cluster is not running on IBM Cloud - registry? The pipeline built two artifacts for deploying your app: Container image -- The image registry includes a container image with your app built in Helm chart -- The artifact repository includes a Helm chart repository that includes a Helm chart for deploying your app Let's look at these artifacts in the Toolkit environment. The container image is stored in the IBM Cloud Container Registry: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Image Registry. In the image registry, you'll see the image the pipeline built for your app, such as us.icr.io/isv-scenarios/stockbffnode-bw with a different tag for each build. The Helm chart is stored in Artifactory: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Artifactory. In the Artifactory console, select Artifactory > Artifacts > generic-local. You'll see a isv-scenarios folder with a different chart for each build, such as generic-local/isv-scenarios/stockbffnode-bw-0.0.1.tgz . 8. Access the running app \u00b6 Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note Be sure the namespace context is set correctly before running the following commands Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected. 9. Locate the app in the web console \u00b6 The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry. After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. OpenShift 4.x Open the OpenShift web console oc console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Kubernetes Open the Kubernetes Dashboard kubectl console Change to the namespace from default to either dev or the namespace you used to deploy your app Click on Deployments You should see the deployment of your application Click on your application , and the corresponding Replica Set Try scaling the application, click on Scale in the header, change number of pods to 2 and click OK Click on one of the pod instances Click on Logs You can see the running state of your application Navigate around the console to understand where your deployment, service and pods are running Success You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it. Run the application locally \u00b6 10. Clone your code to you local machine \u00b6 Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Clone the repository using the url from the terminal. git clone ${ GIT_URL } For example: git clone https://github.com/gct-showcase/inventory-svc You will be required to enter your GitHub User ID and use your Git Hub Personal Access Token as your password. This will complete the clone of your git repository. Change into the cloned directory cd stockbffnode 11. Run the application locally \u00b6 Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:3000/api-docs/ You can try out the sample API that is provided with this Code Pattern You can now add new features and function from inside the Cloud Shell and experiment with your code before you push any changes back to git. 12. Test the webhook \u00b6 Go to your cloned git project and navigate to chart/base directory. cd stockbffnode cd chart/base Open the file Chart.yaml in edit mode and change the description field's value from \"A Helm chart for Kubernetes\" to \"A Helm chart for [yourprojectName]\" Save the edits Push the changes back to your repository git add . git commit -m \"Update application name\" git push As soon as you push your code changes successfully, the webhook will trigger a new pipeline run for your project in your namespace in OCP. Note if the webhook registration step failed, the git push will not trigger the pipeline.","title":"Continuous Integration"},{"location":"learning/fast-ci/#continuous-integration-fast-start","text":"","title":"Continuous Integration - fast start"},{"location":"learning/fast-ci/#overview","text":"The environment supports end-to-end development and deployment of an application. The instructions below will show you how to do it. You can create a new app using one of the Starter Kits . These have been created to include all the key components, configuration, and frameworks to get you started on creating the code you need for your solutions. The approach for getting started is exactly the same for an environment based on Kubernetes or Red Hat OpenShift . Note The instructions provided below lean heavily on the use of the IGC Command Line Interface (CLI) tool, to both show how the CLI works in context and to streamline the process (the reason for creating the CLI in the first place). However, the use of the CLI is in no way required to use the Cloud-Native Toolkit. If you would prefer to work through these instructions without the use of the CLI, we have provided the equivalent manual steps for each command on the Cloud-Native Toolkit CLI page. The video below (click to play the video) demonstrates how to work through the steps to create an application and use a deployment pipeline to install it into your development cluster.","title":"Overview"},{"location":"learning/fast-ci/#create-an-application","text":"","title":"Create an application"},{"location":"learning/fast-ci/#1-log-into-your-development-cluster-from-the-command-line","text":"Before starting, make sure you have set up your development tools . Cluster managed by IBM Cloud Log into the cluster with icc [cluster name|cluster nickname] OpenShift cluster Run oc login $SERVER -u $OCP_USERNAME -p $OCP_PASSWORD You should now be able to access the OpenShift console: oc console Note If your workshop is on Code Ready Workspaces, follow the steps in Code Ready Workspaces Setup before logging in to the cluster. The remaining steps assume this step has already been performed. If you stop and then come back later it is a good idea to re-run this step again before proceeding","title":"1. Log into your Development Cluster from the command line"},{"location":"learning/fast-ci/#2-create-the-development-namespace","text":"Before getting started, the development namespace/project needs to be created and prepared for the DevOps pipelines. This is something that would typically happen once at the beginning of a project when a development team is formed and assigned to the cluster. This step copies the common secrets and configMaps that contain the CI/CD configuration from the tools namespace into the development namespace/project. This enables the pipelines to reference the values easily for your project. oc sync -p ${ DEV_NAMESPACE } Info The -p or --tekton flag adds additional privileges to the tekton pipeline service account. The equivalent command is oc adm policy add-scc-to-user privileged -z pipeline . In OpenShift 4.7, and later versions, this additional privilege is needed for the buildah task.","title":"2. Create the development namespace"},{"location":"learning/fast-ci/#3-open-the-developer-dashboard","text":"The Developer Dashboard makes it easy for you to navigate to the tools, including a section that allows you to select a set of pre-configured Starter Kits that make seeding your development project very easy. Before starting, open a browser and make sure you are logged into Github . There are two options for how to access the dashboard: OpenShift console Open the Application Launcher dropdown from the top-right and select Developer Dashboard Command Line oc dashboard","title":"3. Open the Developer Dashboard"},{"location":"learning/fast-ci/#4-create-your-app-in-git","text":"Warning If you are developing on a shared education cluster, you need to make it easy to identify your app. Please suffix the app name with your initials {app name}-{your initials} (e.g. stockbffnode-mjp ) and use the Git Organization for the shared environment. Warning Your browser needs to be logged in to your GitHub account for a template to work. If the link from the tile displays the GitHub 404 page, log in and reload the page. From the Developer Dashboard, click on Starter Kits tab Pick one of the templates that is a good architectural fit for your application and the language and framework that you prefer to work with. For your first application, select the Typescript Microservice . This also works well in the Cloud Shell. Click on a Starter Kit Tile to create your app github repository from the template repository selected. You can also click on the Git Icon to browse the source template repository and click on the Template to create the template. Complete the GitHub create repository from template process. Owner : Select a valid GitHub organization that you are authorized to create repositories within or the one you were given for the shared cluster (See warning above) Repository name : Enter a name for your repo. GitHub will help with showing a green tick if it is valid (See warning above) Description : Describe your app Press Create repository from template The new repository will be created in your selected organization.","title":"4. Create your app in Git"},{"location":"learning/fast-ci/#create-the-devops-pipeline","text":"","title":"Create the DevOps pipeline"},{"location":"learning/fast-ci/#5-register-the-application-in-a-devops-pipeline","text":"Info We will be using the pipeline command of the IBM Garage Cloud cli to register the DevOps pipeline. The pipeline command gives an option for both Jenkins and Tekton . For more information about working with the different build engines, please see Continuous Integration with Jenkins Guide and Continuous Integration with Tekton Guide Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Start the process to create a pipeline. oc pipeline ${ GIT_URL } For example: oc pipeline https://github.com/gct-showcase/inventory-svc For the deployment of your first app with OpenShift select Tekton as the CI engine. The first time a pipeline is registered in the namespace, the CLI will ask for a username and Password / Personal Access Token for the Git repository that will be stored in a secret named git-credentials . Username : Enter your GitHub user id Personal Access Token : Paste your GitHub personal access token When registering a Tekton pipeline, the CLI will attempt to determine the runtime used by the repository that was provided and filter the available pipelines. If only one matches then it will be automatically selected. If it cannot find a match or there is more than one match then you will be prompted to select the pipeline. When registering a Tekton pipeline, the CLI also reads the available parameters from the pipeline and generates prompts for input. In this case, the option of scanning the built image for vulnerabilities is the only options. The scan is performed by the Vulnerability Advisor if you are using IBM Image Registry or by Trivy if another image registry is used. This scan is performed in \"scan\" stage of pipeline after \"img-release\" stage. ? scan-image: Enable the pipeline to scan the image for vulnerabilities? ( Y/n ) To skip the scan, you have type \"n\" (No).Otherwise, type \"y\" (Yes) for performing Vulnerability Scanning on the image. After the pipeline has been created,the command will set up a webhook from the Git host to the pipeline event listener. Note if the webhook registration step fails, it is likely because the Git credentials are incorrect or do not have enough permission in the repository. When the command is completed it will present options for next steps. You can use the Tekton cli commands to inspect the pipeline run that has been created and tail the log and/or navigate to the provided url to see the pipeline running from the OpenShift console.","title":"5. Register the application in a DevOps Pipeline"},{"location":"learning/fast-ci/#6-view-your-application-pipeline","text":"The steps to view your registered pipeline will vary based on type of pipeline ( Jenkins or Tekton ) and container platform version. Tekton OpenShift 4.x Open the OpenShift Web Console oc console OR From menu on the left switch to the Developer mode Select dev project that was used for the application pipeline registration In the left menu, select Pipelines You will see your application DevOps pipeline now starting to build and once completed will look like the image below. Kubernetes Open the Developer Dashboard kubectl dashboard Select the Tekton tile to launch the Tekton UI Select your development project Jenkins OpenShift 4.x Open the OpenShift Web Console oc console OR From the left-hand menu, select Builds -> Build Configs Select your project from the drop-down menu at the top The registered pipeline should appear in the list Kubernetes Run the command oc dashboard in your terminal to open your Developer Dashboard Select the Jenkins tool to open the Jenkins dashboard Run the command kubectl credentials in your terminal to get the list of logins for the tools Use the Jenkins userid and password to log into the Jenkins dashboard Wait for the pipeline stages to start building. Once the stages have completed, you will see a view similar to the one below.","title":"6. View your application pipeline"},{"location":"learning/fast-ci/#7-view-your-application-artifacts","text":"Todo What happens if the cluster is not running on IBM Cloud - registry? The pipeline built two artifacts for deploying your app: Container image -- The image registry includes a container image with your app built in Helm chart -- The artifact repository includes a Helm chart repository that includes a Helm chart for deploying your app Let's look at these artifacts in the Toolkit environment. The container image is stored in the IBM Cloud Container Registry: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Image Registry. In the image registry, you'll see the image the pipeline built for your app, such as us.icr.io/isv-scenarios/stockbffnode-bw with a different tag for each build. The Helm chart is stored in Artifactory: From the OpenShift console's tools menu or from the Developer Dashboard's tools page, select Artifactory. In the Artifactory console, select Artifactory > Artifacts > generic-local. You'll see a isv-scenarios folder with a different chart for each build, such as generic-local/isv-scenarios/stockbffnode-bw-0.0.1.tgz .","title":"7. View your application artifacts"},{"location":"learning/fast-ci/#8-access-the-running-app","text":"Once the pipeline has completed successfully, the app will be deployed into the namespace used when registering the pipeline. To validate the app is running follow these steps: Note Be sure the namespace context is set correctly before running the following commands Retrieve the app ingress endpoint using commands below in your terminal. oc endpoints From the endpoints listed, select the URL for the repo that was just deployed and press Enter to open that URL in a web browser. Validate the application is working as expected.","title":"8. Access the running app"},{"location":"learning/fast-ci/#9-locate-the-app-in-the-web-console","text":"The build pipeline is configured to build the source code from the Git repository into a container image. This image is stored in the Image Registry. After that, the image is deployed into the same namespace/project within the development cluster where the pipeline ran and validated for its health. The steps below will walk through locating the installed application within the Web Console. OpenShift 4.x Open the OpenShift web console oc console Change to the Developer view Click on Topology menu Click on your application deployment in the topology view Click on the Overview tab Increase the number of running pods to 2 pods Click on the Resources tab to view the list of pods Click on View Logs link to view the logs from each pod You can see the running state of your application Kubernetes Open the Kubernetes Dashboard kubectl console Change to the namespace from default to either dev or the namespace you used to deploy your app Click on Deployments You should see the deployment of your application Click on your application , and the corresponding Replica Set Try scaling the application, click on Scale in the header, change number of pods to 2 and click OK Click on one of the pod instances Click on Logs You can see the running state of your application Navigate around the console to understand where your deployment, service and pods are running Success You now have your application running inside your development cluster and being delivered using a Tekton based CI pipeline. This is the beginning of the developer journey with IBM Cloud. Having reached this point, we recommend you repeat the process a few more times using different Code Patterns templates and explore the Developer view in OpenShift to get familiar with it.","title":"9. Locate the app in the web console"},{"location":"learning/fast-ci/#run-the-application-locally","text":"","title":"Run the application locally"},{"location":"learning/fast-ci/#10-clone-your-code-to-you-local-machine","text":"Open a browser to the Git repository created in the previous step. Copy the url to the Git repository. For GitHub this is done by pressing the Code button and copying the url provided in the Clone section. Clone the repository using the url from the terminal. git clone ${ GIT_URL } For example: git clone https://github.com/gct-showcase/inventory-svc You will be required to enter your GitHub User ID and use your Git Hub Personal Access Token as your password. This will complete the clone of your git repository. Change into the cloned directory cd stockbffnode","title":"10. Clone your code to you local machine"},{"location":"learning/fast-ci/#11-run-the-application-locally","text":"Most developers like to run the code natively in local development environment. To do so, follow the instructions listed in the README.md file to run the code locally. You may be required to install a specific runtime like Java , Node or Go . If you want to quickly access your git repo you can run a helper command to open the git webpage. oc git From inside the folder where the code was cloned from GitHub, run the following command to install the Node.js dependencies. npm install Run the following command to start the application. npm run start Cloud Shell To view the running app click on the Eye Icon on the top right and select the port 3000 this will open a browser tab and display the running app on that port. Cloud Ready Workspaces Click on open link To view this application in new tab click top right corner arrow icon Desktop/Laptop Open a browser to http://localhost:3000/api-docs/ You can try out the sample API that is provided with this Code Pattern You can now add new features and function from inside the Cloud Shell and experiment with your code before you push any changes back to git.","title":"11. Run the application locally"},{"location":"learning/fast-ci/#12-test-the-webhook","text":"Go to your cloned git project and navigate to chart/base directory. cd stockbffnode cd chart/base Open the file Chart.yaml in edit mode and change the description field's value from \"A Helm chart for Kubernetes\" to \"A Helm chart for [yourprojectName]\" Save the edits Push the changes back to your repository git add . git commit -m \"Update application name\" git push As soon as you push your code changes successfully, the webhook will trigger a new pipeline run for your project in your namespace in OCP. Note if the webhook registration step failed, the git push will not trigger the pipeline.","title":"12. Test the webhook"},{"location":"learning/fast-start/","text":"Learning to use the Cloud-Native Toolkit \u00b6 This section provides a fast-start learning path to using the Cloud-Native Toolkit. The content is divided into 3 sections: Setup - ensure you have the required tools, accounts and environment needed to use the toolkit Continuous Integration - Learn how to build, test and deliver your code ready for deployment Continuous Delivery - Learn how to use gitops and automation to deploy your applications to a cloud environment","title":"Learning Overview"},{"location":"learning/fast-start/#learning-to-use-the-cloud-native-toolkit","text":"This section provides a fast-start learning path to using the Cloud-Native Toolkit. The content is divided into 3 sections: Setup - ensure you have the required tools, accounts and environment needed to use the toolkit Continuous Integration - Learn how to build, test and deliver your code ready for deployment Continuous Delivery - Learn how to use gitops and automation to deploy your applications to a cloud environment","title":"Learning to use the Cloud-Native Toolkit"},{"location":"learning/image-sign/","text":"Image Signing \u00b6 Using signed images with Container Image Security Enforcement enables you to enforce trusted workloads within your Kubernetes deployments. In other words, you can restrict the images running within your cluster to only those that you've explicitly allowed and have been signed with valid keys that you have specified. This ensure that the container image has not changed since it left your build process, thus preventing the execution of compromised workloads. See Container Image Security Enforcement for additional details and a step by step guide to configure Container Image Security Enforcement on your clusters.","title":"Image Signing"},{"location":"learning/image-sign/#image-signing","text":"Using signed images with Container Image Security Enforcement enables you to enforce trusted workloads within your Kubernetes deployments. In other words, you can restrict the images running within your cluster to only those that you've explicitly allowed and have been signed with valid keys that you have specified. This ensure that the container image has not changed since it left your build process, thus preventing the execution of compromised workloads. See Container Image Security Enforcement for additional details and a step by step guide to configure Container Image Security Enforcement on your clusters.","title":"Image Signing"},{"location":"learning/in-depth/","text":"Cloud-Native Development with the Toolkit \u00b6 In the previous section you got hands on experience of using the Cloud-Native Toolkit to take a starter kit through the pipeline, to build and test it, package it and deploy it. This section will go a little deeper and explain some of the key activities being performed. The next few pages allow you to explore some of the key concepts: DevOps : the concepts of DevOps and GitOps are key to understanding the Cloud-Native Toolkit Testing : having a good test strategy to find issues as soon as possible Static Analysis : using static analysis tools can also help identify vulnerabilities and bugs Pipeline : The pipeline tool is what drives the automation of building and delivering applications Image Signing : Image signing with Container Image Security Enforcement enables you to enforce trusted workloads within your Kubernetes deployments.","title":"Cloud-Native Development with the Toolkit"},{"location":"learning/in-depth/#cloud-native-development-with-the-toolkit","text":"In the previous section you got hands on experience of using the Cloud-Native Toolkit to take a starter kit through the pipeline, to build and test it, package it and deploy it. This section will go a little deeper and explain some of the key activities being performed. The next few pages allow you to explore some of the key concepts: DevOps : the concepts of DevOps and GitOps are key to understanding the Cloud-Native Toolkit Testing : having a good test strategy to find issues as soon as possible Static Analysis : using static analysis tools can also help identify vulnerabilities and bugs Pipeline : The pipeline tool is what drives the automation of building and delivering applications Image Signing : Image signing with Container Image Security Enforcement enables you to enforce trusted workloads within your Kubernetes deployments.","title":"Cloud-Native Development with the Toolkit"},{"location":"learning/pipeline/","text":"Continuous Integration Pipeline \u00b6 The pipeline technology is what ties all the different tools and activities into you CI/CD workflow. Tekton in the underlying technology in OpenShift Pipelines, which is the default pipeline technology now used by the Cloud-Native Toolkit. Jenkins support is still available, but OpenShift Pipelines is the preferred technology. There are a number of pipelines included with the Cloud-Native toolkit. The pipeline content is based on the programming language of the project. A pipeline orchestrates the activities defined by a number of tasks. There is a collection of useful tasks installed by the Cloud-Native Toolkit. The source for the pipelines and tasks installed by the Toolkit can be found here . As an example the Node.js pipeline performs the following tasks: check out the latest code from git run all the automated tests defined in the project check the Dockerfile using hadolint build the container tag the container with the version number then push to the registry create a release in the source code repository for this version of the app scan the container image for vulnerabilities create the helm chart, version it to match the container version and push it to the helm repository (Artifactory) Update the gitops repository with the latest version of the app As you adopt the Cloud-Native Toolkit you may want to create your own pipelines and tasks to customize the set of activities that make up your own CI/CD process. The integration of Pipelines into OpenShift adds a set of user interface capabilities so you can explore the pipelines and tasks, create and edit them, initiate pipeline runs and explore the output logs of each of the task runs. There is also a command line tool if you prefer to use that. The Cloud-Native Toolkit extension provides an easy way to add a pipeline to a project (you completed this task in the fast-start learning). The pipeline command section of the CLI reference outlines the actions performed by the CLI to enable a pipeline for a git hosted project. Note Red Hat also provide a set of tasks for OpenShift Pipelines which you can incorporate into your pipelines.","title":"Pipeline"},{"location":"learning/pipeline/#continuous-integration-pipeline","text":"The pipeline technology is what ties all the different tools and activities into you CI/CD workflow. Tekton in the underlying technology in OpenShift Pipelines, which is the default pipeline technology now used by the Cloud-Native Toolkit. Jenkins support is still available, but OpenShift Pipelines is the preferred technology. There are a number of pipelines included with the Cloud-Native toolkit. The pipeline content is based on the programming language of the project. A pipeline orchestrates the activities defined by a number of tasks. There is a collection of useful tasks installed by the Cloud-Native Toolkit. The source for the pipelines and tasks installed by the Toolkit can be found here . As an example the Node.js pipeline performs the following tasks: check out the latest code from git run all the automated tests defined in the project check the Dockerfile using hadolint build the container tag the container with the version number then push to the registry create a release in the source code repository for this version of the app scan the container image for vulnerabilities create the helm chart, version it to match the container version and push it to the helm repository (Artifactory) Update the gitops repository with the latest version of the app As you adopt the Cloud-Native Toolkit you may want to create your own pipelines and tasks to customize the set of activities that make up your own CI/CD process. The integration of Pipelines into OpenShift adds a set of user interface capabilities so you can explore the pipelines and tasks, create and edit them, initiate pipeline runs and explore the output logs of each of the task runs. There is also a command line tool if you prefer to use that. The Cloud-Native Toolkit extension provides an easy way to add a pipeline to a project (you completed this task in the fast-start learning). The pipeline command section of the CLI reference outlines the actions performed by the CLI to enable a pipeline for a git hosted project. Note Red Hat also provide a set of tasks for OpenShift Pipelines which you can incorporate into your pipelines.","title":"Continuous Integration Pipeline"},{"location":"learning/testing/","text":"Automated testing \u00b6 Automated testing is an essential part of cloud-native development. In traditional development, where a typical development cycle consists of a number of months, there is a period of time set aside at the end of the development cycle to run testing. With cloud-native development the approach to software development is very different with the adoption of continuous integration and continuous delivery. There is no longer the opportunity to run a large testing phase at the end of the delivery cycle, as each service and application can have its own development cadence, which can be between a few day to a few weeks. Cloud-native development shifts the bulk of the testing effort to as early as possible in the development cycle. Testing needs to be mostly automated to fit with the continuous integration and continuous development practices adopted. Ideally a bug should be detected as it is first checked in to the source control system. Development practices such as test-driven development use a programming technique requiring all code creation to be driven by automated tests that need to be passed - the test is always written before the code. The Cloud-Native Toolkit starter kits have testing enabled. Each programming language has a number of automated testing tools available, so for the Java starter kits you will see JUnit is used and in the JavaScript and TypeScript based starter kits you will find jest is the test framework used. There are other frameworks and when adopting the Cloud-Native Toolkit you may want to create your own starter kits using your preferred testing framework Code coverage \u00b6 A good practice when running any automated testing is to check the code coverage of tests. Ideally this should be as close to 100% as possible. You may even choose to fail a build if the code coverage of automated testing falls below a given threshold, but this is not currently enabled in the starter kits. Many of the testing frameworks have code coverage built in, so they can be configured to report code coverage in addition to the results of the tests being executed. The code coverage information can be written to a file, then tools such as SonarQube will read the coverage data and report it as part of their analysis output. The starter kits use this mechanism, so you can find the results of code coverage in the SonarQube report. If you completed the fast-start learning, you can go and look at the SonarQube report to see the code coverage results. Todo Add screen capture of SonarQube report showing test coverage result Contract Testing \u00b6 When creating multiple applications and services that need to communicate with each other it is important to verify that both the client (requester) and the server (responder) both stick to the defined API for the communication. In traditional testing this would have been verified during integration and systems testing, where the end-to-end functionality of a system is tested. In Cloud-Native development contract-driven testing can be used to validate that all parties are complying with service APIs. In the Cloud-Native Toolkit the Pact is used. The tests need to be written by a developer and integrated into the project automated testing. You can see an example of Pact in use in the inventory management svc solution . The code repository can be found here . This example is part of one of the Learning Journey suggested additional learning in the resources section","title":"Automated Testing"},{"location":"learning/testing/#automated-testing","text":"Automated testing is an essential part of cloud-native development. In traditional development, where a typical development cycle consists of a number of months, there is a period of time set aside at the end of the development cycle to run testing. With cloud-native development the approach to software development is very different with the adoption of continuous integration and continuous delivery. There is no longer the opportunity to run a large testing phase at the end of the delivery cycle, as each service and application can have its own development cadence, which can be between a few day to a few weeks. Cloud-native development shifts the bulk of the testing effort to as early as possible in the development cycle. Testing needs to be mostly automated to fit with the continuous integration and continuous development practices adopted. Ideally a bug should be detected as it is first checked in to the source control system. Development practices such as test-driven development use a programming technique requiring all code creation to be driven by automated tests that need to be passed - the test is always written before the code. The Cloud-Native Toolkit starter kits have testing enabled. Each programming language has a number of automated testing tools available, so for the Java starter kits you will see JUnit is used and in the JavaScript and TypeScript based starter kits you will find jest is the test framework used. There are other frameworks and when adopting the Cloud-Native Toolkit you may want to create your own starter kits using your preferred testing framework","title":"Automated testing"},{"location":"learning/testing/#code-coverage","text":"A good practice when running any automated testing is to check the code coverage of tests. Ideally this should be as close to 100% as possible. You may even choose to fail a build if the code coverage of automated testing falls below a given threshold, but this is not currently enabled in the starter kits. Many of the testing frameworks have code coverage built in, so they can be configured to report code coverage in addition to the results of the tests being executed. The code coverage information can be written to a file, then tools such as SonarQube will read the coverage data and report it as part of their analysis output. The starter kits use this mechanism, so you can find the results of code coverage in the SonarQube report. If you completed the fast-start learning, you can go and look at the SonarQube report to see the code coverage results. Todo Add screen capture of SonarQube report showing test coverage result","title":"Code coverage"},{"location":"learning/testing/#contract-testing","text":"When creating multiple applications and services that need to communicate with each other it is important to verify that both the client (requester) and the server (responder) both stick to the defined API for the communication. In traditional testing this would have been verified during integration and systems testing, where the end-to-end functionality of a system is tested. In Cloud-Native development contract-driven testing can be used to validate that all parties are complying with service APIs. In the Cloud-Native Toolkit the Pact is used. The tests need to be written by a developer and integrated into the project automated testing. You can see an example of Pact in use in the inventory management svc solution . The code repository can be found here . This example is part of one of the Learning Journey suggested additional learning in the resources section","title":"Contract Testing"},{"location":"learning-journey-slides/","text":"Learning Journey Slides \u00b6 Day 1 \u00b6 What is Cloud Native? Cloud Native Applications Containers OpenShift Overview CI/CD Day 2 \u00b6 Code Analysis Container Registry Artifact Management Day 3 \u00b6 Inventory Backend Inventory Backend-for-Frontend Inventory UI","title":"Learning Journey Slides"},{"location":"learning-journey-slides/#learning-journey-slides","text":"","title":"Learning Journey Slides"},{"location":"learning-journey-slides/#day-1","text":"What is Cloud Native? Cloud Native Applications Containers OpenShift Overview CI/CD","title":"Day 1"},{"location":"learning-journey-slides/#day-2","text":"Code Analysis Container Registry Artifact Management","title":"Day 2"},{"location":"learning-journey-slides/#day-3","text":"Inventory Backend Inventory Backend-for-Frontend Inventory UI","title":"Day 3"},{"location":"overview/overview/","text":"What is the Cloud-Native Toolkit? \u00b6 Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift and Kubernetes. It embodies IBM Garage Method principles and practices for consistently developed applications, incorporating best practices that increase developer velocity for efficient delivery of business value. Cloud-Native Toolkit objectives \u00b6 There are a number of objectives behind providing the Cloud-Native Toolkit. The three main goals of the Toolkit are provided below: 1. Accelerate time to business value \u00b6 One goal of the Cloud-Native Toolkit is to prepare the development environment quickly and allow the development team to start delivering business function on day one of the first iteration using enterprise cloud-native practices. After all, the point of cloud-native development is to deliver business value to end users and the development and operations infrastructure are provided in service to that goal. Through the automation provided by the Cloud-Native Toolkit we can provision an environment in minutes through automation that is fully configured and ready for a development team to start working immediately. With the other components of the Toolkit, developers can begin with a rich DevOps framework with a focus on \"build to manage\" techniques to help build production-ready applications. 2. Reduce risk through consistent delivery models from start to production \u00b6 The Cloud-Native Toolkit encapsulates many of the available best practices for cloud-native development including DevOps and \"Build to Manage\" practices. They have been provided through the Toolkit in this way so that developers and site reliability engineers (SREs) can benefit from these practices without requiring any additional effort and so that they can be applied consistently from project to project. 3. Quickly ramp up development teams on Red Hat OpenShift and Kubernetes \u00b6 Containerized platforms like Red Hat OpenShift and Kubernetes provide a great deal of functionality and flexibility for application teams. However, these platforms can at time seem unapproachable for developers and SREs new to the environment given all the different concepts and components. The Cloud-Native Toolkit aims to help with the learning curve in two different ways: Provide tools and techniques to \"round off the corners\" of the more complex aspects of working in a containerized environment Provide a learning journey that incrementally introduces the concepts of a containerized environment in terms of practical scenarios, not abstract theory Guided walk-though \u00b6 If you'd like to have a guided walk through of what the Cloud-Native Toolkit provides, check out this video demonstration of the Toolkit in action: Components of the Cloud-Native Toolkit \u00b6 As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life-cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Guides - this set of documentation that weaves the various toolkit components together with a perspective on how to apply cloud-native practices to deliver business solutions Infrastructure as Code - Terraform scripts and GitOps configuration to provision and manage the environment CLI - a simple node-based CLI that installs as a plugin to the kubectl and oc CLIs and provides commands to simplify common Developer Dashboard - Dashboard component and Red Hat OpenShift console extensions to simplify common developer activities DevOps pipelines - continuous integration pipelines for Tekton and Jenkins Starter Kits and Code Patterns - software repositories that can be used to quickly get started building applications using common patterns, or to serve as a reference to enhance existing patterns Learning Journey - activation material to teach practitioners how to apply cloud-native practices in real-world scenarios using the Toolkit Cloud-Native Toolkit Developer Environment \u00b6 The Cloud-Native Toolkit Developer Environment includes several features that support IBM Garage Method best practices for consistent and rapid development of cloud-native applications: Cluster : A Red Hat OpenShift or Kubernetes cluster that both hosts the tools and itself is a deployment target for application builds Software Development Life Cycle (SDLC) : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools Starter Kits : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : A centralized console to help developers use the environment's capabilities Typically a Cloud System Admin installs and sets up a new Developer Environment after the inception workshop , providing a place for the developers to start developing the minimum viable product (MVP) . The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform scripts structured as modular components so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development. Red Hat Open Innovation Labs CI/CD components embodies a very similar approach to how they deliver success with OpenShift. Environment components \u00b6 After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift or IBM Cloud Kubernetes Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. Logo Usage Reference Logo Usage Reference Artifactory is an Open Source product maintained by JFrog Jenkins Open Source project Jenkins SonarQube Open Source project maintained by SonarSource Nexus Repository Open Source project maintained by SonaType Trivy Open Source project maintained by Aqua InteliJ IDE from JetBrains VSCode Free IDE maintained by Microsoft Jaeger Open Source tool maintained by Jaeger Community ArgoCD Open Source tool maintained by ArgoCD Community OpenShift and CodeReady Workspaces are products from Red Hat LogDNA IBM Cloud service supplied by LogDNA Sysdig IBM Cloud service supplied by Sysdig The tools to provision an environment using the Cloud-Native Toolkit can the customized to provision a particular set of tools fit for the environment. The Toolkit provides a default installation to provision a Developer Environment as a starting point. Any of the available components listed on the Terraform modules page can be used to prepare the environment. Development cluster \u00b6 The heart of the Developer Environment is a cluster: An IBM Cloud-managed Kubernetes or Red Hat OpenShift cluster Cluster namespace that encapsulates the tooling installed in the cluster: tools A collection of SRE tools and services Continuous delivery tools \u00b6 The following best-of-breed open source software tools are installed in the cluster's tools namespace: Capability Tool Bitnami Description Continuous Integration Jenkins CI Yes Jenkins is a common tool for Continuous Integration Continuous Integration Tekton CI Tekton is an emerging tool for Continuous Integration with Kubernetes and OpenShift API Contract Testing Pact Pact enables API contract testing Code Analysis SonarQube Yes SonarQube can scan code and display the results in a dashboard Container Image Registry IBM Cloud Container Registry Stores container images to be deployed Artifact Management Artifactory Yes Artifactory is an artifact storage and Helm chart repository Continuous Delivery ArgoCD ArgoCD support Continuous Delivery with GitOps Web IDE Code Ready Workspace IDE for editing and managing code in a web browser","title":"What is the Cloud Native Toolkit"},{"location":"overview/overview/#what-is-the-cloud-native-toolkit","text":"Cloud-Native Toolkit is an open-source collection of assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift and Kubernetes. It embodies IBM Garage Method principles and practices for consistently developed applications, incorporating best practices that increase developer velocity for efficient delivery of business value.","title":"What is the Cloud-Native Toolkit?"},{"location":"overview/overview/#cloud-native-toolkit-objectives","text":"There are a number of objectives behind providing the Cloud-Native Toolkit. The three main goals of the Toolkit are provided below:","title":"Cloud-Native Toolkit objectives"},{"location":"overview/overview/#1-accelerate-time-to-business-value","text":"One goal of the Cloud-Native Toolkit is to prepare the development environment quickly and allow the development team to start delivering business function on day one of the first iteration using enterprise cloud-native practices. After all, the point of cloud-native development is to deliver business value to end users and the development and operations infrastructure are provided in service to that goal. Through the automation provided by the Cloud-Native Toolkit we can provision an environment in minutes through automation that is fully configured and ready for a development team to start working immediately. With the other components of the Toolkit, developers can begin with a rich DevOps framework with a focus on \"build to manage\" techniques to help build production-ready applications.","title":"1. Accelerate time to business value"},{"location":"overview/overview/#2-reduce-risk-through-consistent-delivery-models-from-start-to-production","text":"The Cloud-Native Toolkit encapsulates many of the available best practices for cloud-native development including DevOps and \"Build to Manage\" practices. They have been provided through the Toolkit in this way so that developers and site reliability engineers (SREs) can benefit from these practices without requiring any additional effort and so that they can be applied consistently from project to project.","title":"2. Reduce risk through consistent delivery models from start to production"},{"location":"overview/overview/#3-quickly-ramp-up-development-teams-on-red-hat-openshift-and-kubernetes","text":"Containerized platforms like Red Hat OpenShift and Kubernetes provide a great deal of functionality and flexibility for application teams. However, these platforms can at time seem unapproachable for developers and SREs new to the environment given all the different concepts and components. The Cloud-Native Toolkit aims to help with the learning curve in two different ways: Provide tools and techniques to \"round off the corners\" of the more complex aspects of working in a containerized environment Provide a learning journey that incrementally introduces the concepts of a containerized environment in terms of practical scenarios, not abstract theory","title":"3. Quickly ramp up development teams on Red Hat OpenShift and Kubernetes"},{"location":"overview/overview/#guided-walk-though","text":"If you'd like to have a guided walk through of what the Cloud-Native Toolkit provides, check out this video demonstration of the Toolkit in action:","title":"Guided walk-though"},{"location":"overview/overview/#components-of-the-cloud-native-toolkit","text":"As the name suggests, the Cloud-Native Toolkit provides a collection of tools that can be used in part or in whole to support the activities of software development life-cycle. The following provides a listing of the assets that make up the Cloud-Native Toolkit: Guides - this set of documentation that weaves the various toolkit components together with a perspective on how to apply cloud-native practices to deliver business solutions Infrastructure as Code - Terraform scripts and GitOps configuration to provision and manage the environment CLI - a simple node-based CLI that installs as a plugin to the kubectl and oc CLIs and provides commands to simplify common Developer Dashboard - Dashboard component and Red Hat OpenShift console extensions to simplify common developer activities DevOps pipelines - continuous integration pipelines for Tekton and Jenkins Starter Kits and Code Patterns - software repositories that can be used to quickly get started building applications using common patterns, or to serve as a reference to enhance existing patterns Learning Journey - activation material to teach practitioners how to apply cloud-native practices in real-world scenarios using the Toolkit","title":"Components of the Cloud-Native Toolkit"},{"location":"overview/overview/#cloud-native-toolkit-developer-environment","text":"The Cloud-Native Toolkit Developer Environment includes several features that support IBM Garage Method best practices for consistent and rapid development of cloud-native applications: Cluster : A Red Hat OpenShift or Kubernetes cluster that both hosts the tools and itself is a deployment target for application builds Software Development Life Cycle (SDLC) : Deployment target environments that support the application development lifecycle: dev , test , and staging Backend services : Cloud services commonly required by cloud-native applications for monitoring, security, and persistence CI/CD : A prebuilt, ready-to-run continuous delivery pipeline incorporating best-of-breed open source software tools Starter Kits : Prebuilt code templates for common application components and tasks incorporating best practices that developers can add to their codebase as needed Dashboard : A centralized console to help developers use the environment's capabilities Typically a Cloud System Admin installs and sets up a new Developer Environment after the inception workshop , providing a place for the developers to start developing the minimum viable product (MVP) . The objective is to reduce the time required for a team to configure and prepare their development environment. The key benefit is to make the end-to-end CI/CD development lifecycle consistent across each platform and make the out-of-the-box developer experience as simple as possible. The installation is performed using Terraform scripts structured as modular components so unneeded tools can be easily disabled or new tools added. The combination of tools selected are proven in the industry to deliver real value for modern cloud-native development. Red Hat Open Innovation Labs CI/CD components embodies a very similar approach to how they deliver success with OpenShift.","title":"Cloud-Native Toolkit Developer Environment"},{"location":"overview/overview/#environment-components","text":"After installation, the environment consists of the following components and developer tools: A Red Hat OpenShift or IBM Cloud Kubernetes Service development cluster A collection of continuous delivery tools deployed into the cluster A set of backend services This diagram illustrates the environment: The diagram shows the components in the environment: the cluster, the deployment target environments, the cloud services, and the tools. Logo Usage Reference Logo Usage Reference Artifactory is an Open Source product maintained by JFrog Jenkins Open Source project Jenkins SonarQube Open Source project maintained by SonarSource Nexus Repository Open Source project maintained by SonaType Trivy Open Source project maintained by Aqua InteliJ IDE from JetBrains VSCode Free IDE maintained by Microsoft Jaeger Open Source tool maintained by Jaeger Community ArgoCD Open Source tool maintained by ArgoCD Community OpenShift and CodeReady Workspaces are products from Red Hat LogDNA IBM Cloud service supplied by LogDNA Sysdig IBM Cloud service supplied by Sysdig The tools to provision an environment using the Cloud-Native Toolkit can the customized to provision a particular set of tools fit for the environment. The Toolkit provides a default installation to provision a Developer Environment as a starting point. Any of the available components listed on the Terraform modules page can be used to prepare the environment.","title":"Environment components"},{"location":"overview/prerequisites/","text":"Assumed Knowledge \u00b6 The Cloud-Native Toolkit has been developed to help developers adopt best practices when developing enterprise cloud native applications and services. It aims to teach developers how to adopt Continuous Integration and Continuous Delivery best practices, but assumes a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Prerequisites"},{"location":"overview/prerequisites/#assumed-knowledge","text":"The Cloud-Native Toolkit has been developed to help developers adopt best practices when developing enterprise cloud native applications and services. It aims to teach developers how to adopt Continuous Integration and Continuous Delivery best practices, but assumes a basic working knowledge of the following technologies: Git version control system Containers as the means of delivering applications and services Kubernetes as the container orchestration platform Helm as the kubernetes package manager Links to additional learning resources can be found in the resources section .","title":"Assumed Knowledge"},{"location":"overview/whats-new/","text":"What's new \u00b6 May 25, 2021 \u00b6 The Workshop got updated with a new Artificial Intelligence New experimental support to deploy via GitOps the Cloud Native Toolkit and IBM Cloud Pak App Connect Jan 28, 2021 \u00b6 Cloud Native Toolkit Workshop released. The workshop in a box environment is easy and quick to setup with hands on labs including videos. Check them out at cloudnativetoolkit.dev/workshop . More hands on labs for the workshop coming soon. Jan 6, 2021 \u00b6 CLI \u00b6 v1.11.1 , v1.11.0 , v1.10.2 , v1.10.1 , v1.10.0 , v1.9.0 , v1.8.1 , v1.8.0 , v1.7.1 , v1.7.0 , v1.6.0 , v1.5.0 Many usability changes, particularly for the pipeline command: Reduces required permissions At the start of every command that needs access to the kube api, the cli checks that a connection is available. Previously it did that by trying to list all the pods in the cluster (e.g. the equivalent of kubectl get pods -A ). Unfortunately, that command needs a great deal of access to succeed. The check was changed to run a command that requires much less permission. Before creating the webhook triggers, the pipeline command would read the Tekton version number from annotations on the operator deployment in the openshift-operators namespace. This check required a great deal of permissions to be able to read the deployment in that namespace. Instead, the pipeline command has been changed to resort to a brute force check - it assumes v0.6.0 and if it fails tries again with v0.4.0. Usability updates for pipeline command Allows the repo url to be passed in so it is not necessary to clone the repository first Creates a single event listener per namespace/project instead of a new event listener for each repo Detects the runtime of the repository and filters the tekton pipelines based on the runtime Reads params from tekton pipeline and prompts for values The input arguments have been cleaned up to remove conflicts and to use values that make more sense for the input parameters. Dec 11, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.5.0 Updates ibm-container-platform module to v1.18.3 to provision ocp 4.5 clusters properly Adds option of storage class for Artifactory Updates to point releases of terraform modules with updated workflows to generate module catalogs argocd v2.10.1 artifactory v1.10.0 dashboard v1.10.4 ibm-image-registry v1.2.3 ocp-image-registry v1.2.2 k8s-image-registry v1.1.5 k8s-source-control v1.2.1 jenkins v1.4.3 pactbroker v1.4.2 sonarqube v1.9.2 swaggereditor v1.4.1 tekton v2.0.2 tekton-resources v2.2.0 ibm-logdna v2.4.3 ibm-sysdig v2.3.3 CLI \u00b6 v1.4.2 , v1.4.1 Prints next steps to the console after calling the pipeline command Pipeline run started: memcached-operator-catalog-1762ff0a6d7 Next steps: Tekton cli: View PipelineRun info - tkn pr describe memcached-operator-catalog-1762ff0a6d7 View PipelineRun logs - tkn pr logs memcached-operator-catalog-1762ff0a6d7 OpenShift console: View PipelineRun - https://console-openshift-console.garage-dev-ocp45-vpc-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud/k8s/ns/operator-dev/tekton.dev~v1beta1~PipelineRun/memcached-operator-catalog-1762ff0a6d7 Registers the gitops command as a plugin to kubectl and oc clis Tekton tasks \u00b6 v2.2.3 , v2.2.2 , v2.2.1 , v2.2.0 , v2.1.27 Updates tasks to use images hosted in quay.io instead of docker.io to avoid rate limiting issue Adds workflow to mirror required images from docker.io to quay.io on a nightly schedule Adds pipelines for operator and operator catalog builds Nov 20, 2020 \u00b6 CLI \u00b6 v1.4.0 , v1.3.0 , v1.2.2 , v1.2.1 Refactors Git server interaction logic to make more extensible Adds support to pipeline command for Gogs git server running in cluster Adds support to pipeline command for Bitbucket along with existing support for GitHub, GitHub Enterprise, and GitLab Nov 13, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.4.0 Adds image-registry and source-control modules Updates numbering for generated tiles Updates default settings when installing from iZero Updates underlying module versions ibm-container-platform v1.18.0 artifactory v1.9.2 dashboard v1.10.0 ibm-image-registry v1.2.0 ocp-image-registry v1.2.0 tools-tekton-resources v2.1.9 k8s-source-control v1.2.0 tools-swagger-editor v1.4.0 Tekton tasks \u00b6 v2.1.26 , v2.1.25 , v2.1.24 , v2.1.23 , v2.1.24 , v2.1.21 , v2.1.20 Updates tekton tasks to support Gogs git server running in cluster Fixes setup task to handle different characters in git url Updates the task order in the pipelines to release the helm chart after the scan Defaults to using the internal OCP registry if none is defined Nov 6, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.3.9 , v2.3.8 , v2.3.7 , v2.3.6 , v2.3.5 , v2.3.4 , v2.3.3 , v2.3.2 , v2.3.1 , v2.3.0 , v2.2.2 Prints the elapsed time for the Toolkit installation process Updates tile definition to include README.md in long description and update input parameters Updates module versions ibm-container-platform v1.18.0 ibm-object-storage v2.0.1 CLI \u00b6 v1.2.0 , v1.1.0 Simplifies the logic used to determine cluster type by using intrinsic information within the cluster. This expands the number of commands that can be run against a cluster that doesn't have the toolkit installed Updates the git secret logic to support older versions of the git cli (which allows the CLI to be run in the IBM OpenLab environment) Oct 30, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.2.1 , v2.2.0 Adds quick install option with Terraform job running within the cluster Updates Tekton Resources module to v2.1.8 Update terraform modules to latest dashboard v1.9.0 ocp-cluster v2.3.5 pactbroker v1.4.0 CLI \u00b6 v1.0.3 , v1.0.2 Fixes bug that causes the endpoint command to fail due to a missing import Fixes bug with the credentials command that caused the internal urls to be displayed instead of the external ones Tekton tasks \u00b6 v2.1.19 , v2.1.18 , v2.1.17 , v2.1.16 , v2.1.15 , v2.1.14 , v2.1.13 , v2.1.12 , v2.1.11 , v2.1.10 , v2.1.9 , v2.1.8 , v2.1.7 , v2.1.6 , v2.1.5 , v2.1.4 , v2.1.3 Fixes bug in deploy task when the Git hash has an \"e\" in it (tries to convert to an exponential number) Combines Trivy and IBM VA scan into one task Uses internal endpoints for tools hosted within the cluster (like artifactory and sonarqube) Updates from helm v2 to v3 for the pipeline logic Fix the health url check logic Uses registry-access to get image registry information instead of ibmcloud-config Adds logic to wait for Vulnerability Advisor to complete before testing the result Sept 25, 2020 \u00b6 Tekton tasks \u00b6 v2.1.2 , v2.1.1 , v2.1.0 , Introduces image vulnerability scan with Aquasec Trivy Fixes trivy scan logic to check for PERFORM_SCAN flag in setup and execute steps Sept 11, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.1.0 , v2.0.2 , v2.0.1 Introduced Key Protect ArgoCD plugin in argocd module to v2.9.0 to generate kubernetes secrets from key material in Key Protect Updates namespace module to v2.6.0 to remove use of previously deprecated, now removed --export flag Aug 25, 2020 \u00b6 IasC (IZero and Terraform modules) \u00b6 v2.0.0 Updates Tekton module and resources to support the Red Hat Tekton operator and related versions Simplifies the process to install the Cloud-Native Toolkit on a Red Hat OpenShift provisioned anywhere Provide Private Catalog tile for install with Schematics Improves the handling of LogDNA and Sysdig in the cluster Automates the post-install configuration steps for Artifactory Automates the post-install configuration steps for SonarQube CLI \u00b6 v1.0.1 Updates tekton pipeline handling to create the webhook Adds git , gitops and console commands Tekton tasks \u00b6 v2.0.3 Refactors tasks and pipelines to support v1beta1 schema and remove dependency on PipelineResources Streamlines CI process in pipelines to be more modular and reusable Tasks for Vulnerability scanning with IBM Image Registry","title":"What's New"},{"location":"overview/whats-new/#whats-new","text":"","title":"What's new"},{"location":"overview/whats-new/#may-25-2021","text":"The Workshop got updated with a new Artificial Intelligence New experimental support to deploy via GitOps the Cloud Native Toolkit and IBM Cloud Pak App Connect","title":"May 25, 2021"},{"location":"overview/whats-new/#jan-28-2021","text":"Cloud Native Toolkit Workshop released. The workshop in a box environment is easy and quick to setup with hands on labs including videos. Check them out at cloudnativetoolkit.dev/workshop . More hands on labs for the workshop coming soon.","title":"Jan 28, 2021"},{"location":"overview/whats-new/#jan-6-2021","text":"","title":"Jan 6, 2021"},{"location":"overview/whats-new/#dec-11-2020","text":"","title":"Dec 11, 2020"},{"location":"overview/whats-new/#nov-20-2020","text":"","title":"Nov 20, 2020"},{"location":"overview/whats-new/#nov-13-2020","text":"","title":"Nov 13, 2020"},{"location":"overview/whats-new/#nov-6-2020","text":"","title":"Nov 6, 2020"},{"location":"overview/whats-new/#oct-30-2020","text":"","title":"Oct 30, 2020"},{"location":"overview/whats-new/#sept-25-2020","text":"","title":"Sept 25, 2020"},{"location":"overview/whats-new/#sept-11-2020","text":"","title":"Sept 11, 2020"},{"location":"overview/whats-new/#aug-25-2020","text":"","title":"Aug 25, 2020"},{"location":"reference/cli/","text":"Cloud Native Toolkit - Command Line Interface \u00b6 Invoking the CLI \u00b6 When the CLI is installed , it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: $ igc --help IBM Garage Cloud Native Toolkit CLI (https://cloudnativetoolkit.dev) Usage: igc <command> [args] Commands: igc console Launch the IKS or OpenShift admin console igc create-webhook Create a git webhook for a given Jenkins pipeline igc credentials Lists the urls and credentials for the tools deployed to the cluster igc dashboard Open the Developer Dashboard in the default browser igc enable Enable the current repository with pipeline logic igc endpoints List the current ingress hosts for deployed apps in a namespace [aliases: ingress, endpoint, ingresses] igc git-secret [name] Create a kubernetes secret that contains the url, username, and personal access token for a git repo igc git [remote] Launches a browser to the git repo url specified by the remote. If not provided remote defaults to origin igc gitops Registers the git repository in the kubernetes cluster as the gitops repository for the given namespace igc sync [namespace] Create a namespace (if it does not exist) and prepare it with the necessary configuration [aliases: project, namespace] igc pull-secret [namespace] Copy pull secrets into the provided project from the template namespace igc pipeline [gitUrl] Register a pipeline for the current code repository igc tool-config [name] Create the config map and secret for a tool configured in the environment igc vlan Print out the vlan values igc yq <command> lightweight yaml command-line processor that addresses deficiencies with the existing `yq` command Options: --version Show version number [boolean] --help Show help [boolean] Info As of v0.5.1, the Toolkit CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all of the following are equivalent: igc pipeline kubectl pipeline oc pipeline Prerequisite tools \u00b6 Some of the commands provided by the Toolkit CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools , in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud) Available commands \u00b6 dashboard \u00b6 Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established. Command flags \u00b6 -n : the namespace where the dashboard has been deployed; the default is tools Usage \u00b6 CLI The command is used in the following way: igc dashboard OpenShift The following commands would have the same result on OpenShift: HOST = $( oc get routes/dashboard -n tools -o jsonpath = '{.spec.host}' ) open \"https:// $HOST \" Kubernetes The following commands would have the same result on Kubernetes: HOST = $( kubectl get ingress/developer-dashboard -n tools -o jsonpath = '{.spec.rules[0].host}' ) open \"https:// $HOST \" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command console \u00b6 Opens the IKS or OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established. Usage \u00b6 CLI The command is used in the following way: igc console OpenShift The following commands would have the same result on OpenShift: open $( oc whoami --show-console ) Kubernetes The following commands would have the same result on Kubernetes: REGION = \"...\" CLUSTER_NAME = \"...\" CLUSTER_ID = $( ibmcloud ks cluster get --cluster ${ CLUSTER_NAME } | grep -E \"^ID\" | sed -E \"s/ID: +([^ ]+)/\\\\1/g\" ) open \"https:// ${ REGION } .containers.cloud.ibm.com/kubeproxy/clusters/ ${ CLUSTER_ID } /service/#/overview?namespace=default\" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command git \u00b6 Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out. Usage \u00b6 CLI The command is used in the following way: igc git If you have multiple remotes and would like to open one other than origin : igc git origin-fork Manual The following commands would have the same result with shell commands: alias gh = \"open https://github. $( git config remote.origin.url | cut -f2 -d. | tr ':' / ) \" Related commands \u00b6 credentials : shows information about the same tools shown in the dashboard from the command-line tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command credentials \u00b6 Lists the endpoints, user names, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established. Command flags \u00b6 -n : the namespace where the tools have been deployed; the default is tools Usage \u00b6 CLI The command is used in the following way: igc credentials The credential output is JSON format like this Credentials: { argocd: { user: 'admin' , password: '12345678' , url: 'https://argocd-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . dashboard: { url: 'https://dashboard-tools.mycluster.us-east.containers.appdomain.cloud' } , . . . } OpenShift or Kubernetes The following commands have the same result (note the dependency on jq ): # config maps kubectl get configmap -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-config\").data]' # secrets kubectl get secret -n tools -l grouping = garage-cloud-native-toolkit -o json | \\ jq '[.items[] | select(.metadata.name != \"ibmcloud-apikey\").data | with_entries(.value |= @base64d)]' Related commands \u00b6 dashboard : displays the url of the Developer Dashboard and launches the default browser tool-config : allows configuration for additional tools to be added to the cluster, making them available to the dashboard and credentials command endpoints \u00b6 Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established. Command flags \u00b6 -n : the namespace from which the endpoints will be read; the value will be read from the current context if not provided Usage \u00b6 CLI The command is used in the following way: igc endpoints OpenShift The following commands list the route and ingress endpoints: # routes kubectl get route -n tools # ingress kubectl get ingress -n tools Kubernetes The following commands list the ingress endpoints: kubectl get ingress -n tools sync \u00b6 Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. The command can also add additional privileges to the tekton pipeline service account. These privileges are needed to run the buildah task in OpenShift 4.7 Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] -p, --tekton flag indicating the tekton pipeline service account should be given privileged scc --verbose flag to produce more verbose logging [boolean] Usage \u00b6 CLI Create a dev namespace for development igc sync -p myapp-dev OpenShift Create a dev namespace for development oc sync -p myapp-dev Kubernetes Create a dev namespace for development kubectl sync myapp-dev Manual ConfigMap and Secret setup The following steps will copy the ConfigMaps and Secrets from a template namespace to a target namespace: export TEMPLATE_NAMESPACE = \"tools\" export NAMESPACE = \"NAMESPACE\" kubectl get configmap -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get configmap ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done kubectl get secret -l group = catalyst-tools -n ${ TEMPLATE_NAMESPACE } -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | \\ while read cm ; do kubectl get secret ${ cm } --namespace ${ TEMPLATE_NAMESPACE } --export -o yaml | \\ kubectl apply --namespace $NAMESPACE -f - done The -p or --tekton flag performs the same function as command: oc adm policy add-scc-to-user privileged -z pipeline pull-secret \u00b6 Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean] Usage \u00b6 CLI Copy the pull secret from default namespace into myapp-test namespace and add to serviceAccount default igc pull-secret myapp-test -t default -z default Manual pull secret setup The following commands will copy the pull secret(s) from the default namespace and add them to the service account: export NAMESPACE = \"myapp-test\" export SERVICE_ACCOUNT = \"default\" if [[ $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ .metadata.name }{ \"\\n\" }{ end }' | grep icr | wc -l | xargs ) -eq 0 ]] ; then echo \"*** Copying pull secrets from default namespace to ${ NAMESPACE } namespace\" kubectl get secrets -n default | grep icr | sed \"s/\\([A-Za-z-]*\\) *.*/\\1/g\" | while read default_secret ; do kubectl get secret ${ default_secret } -n default -o yaml --export | sed \"s/name: default-/name: /g\" | kubectl -n ${ NAMESPACE } create -f - done else echo \"*** Pull secrets already exist on ${ NAMESPACE } namespace\" fi EXISTING_SECRETS = $( kubectl get serviceaccount/ ${ SERVICE_ACCOUNT } -n \" ${ NAMESPACE } \" -o json | tr '\\n' ' ' | sed -E \"s/.*imagePullSecrets.: \\[([^]]*)\\].*/\\1/g\" | grep icr | wc -l | xargs ) if [[ ${ EXISTING_SECRETS } -eq 0 ]] ; then echo \"*** Adding secrets to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" PULL_SECRETS = $( kubectl get secrets -n \" ${ NAMESPACE } \" -o jsonpath = '{ range .items[*] }{ \"{\\\"name\\\": \\\"\"}{ .metadata.name }{ \"\\\"}\\n\" }{ end }' | grep icr | grep -v \" ${ NAMESPACE } \" | paste -sd \",\" - ) kubectl patch -n \" ${ NAMESPACE } \" serviceaccount/ ${ SERVICE_ACCOUNT } -p \"{\\\"imagePullSecrets\\\": [ ${ PULL_SECRETS } ]}\" else echo \"*** Pull secrets already applied to serviceaccount/ ${ SERVICE_ACCOUNT } in ${ NAMESPACE } namespace\" fi pipeline \u00b6 Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url. Repository location \u00b6 The pipeline command supports registering a CI pipeline for a repository that has been cloned locally or using the remote repository url. Local repository \u00b6 If you are registering a local repository then you must run the command from within the directory of your local clone of the Git repo. When registering a local repository, the pipeline will use the branch that is currently checked out. Remote repository \u00b6 To register a remote repository, pass the repo url as an argument to the pipeline command. For example: oc pipeline \"https://github.com/my-org/my-repo\" You can optionally provide the branch name with the url using a hash ( # ): oc pipeline \"https://github.com/my-org/my-repo#my-branch\" Note When registering a remote git repo, if the branch is not provided then the default branch will be used. Pipeline type \u00b6 The pipeline command supports registering pipelines with either Tekton or Jenkins. The pipeline can be specified from the command-line with either the --tekton or --jenkins flags. If a flag is not provided then you will be prompted to select the pipeline. Git credentials \u00b6 The command will prompt for the username and password/personal access token to access the Git repository, unless those are already stored in a secret in the cluster namespace or provided as command-line parameters. The username and password can be provided with the -u and -p flags. If you want to change the credentials that have already been stored in the cluster namespace, the -g argument an be provided and you will be prompted for the credentials. Tekton template pipeline \u00b6 If a Tekton pipeline will be used, a template pipeline must be selected for the new repository pipeline. The command reads the template pipelines available in the template namespace. The template namespace can be provided with the -t argument and will default to tools if not provided. The command will also filter the list of pipelines based on the runtime determined from the given repository. If there is more than one template pipeline available then you will be prompted to pick one. The template pipeline can also be provided on the command-line using the --pipeline argument. If the name doesn't match an available pipeline then you will be prompted to select one. Pipeline parameters \u00b6 Once the pipeline template is selected, you will be prompted to provide values for the defined pipeline parameters. The values can also be provided from the command-line using the -p argument. The name of the parameter is listed at the beginning of the prompt message. Multiple parameters can be provided by repeating the -p argument. For example: oc pipeline --tekton \"https://github.com/my-org/my-repo\" -p scan-image = false -p edge = false Optional arguments \u00b6 -u : the username for accessing the Git repo -P : the password or personal access token for accessing the Git repo -g : ignore existing git-credentials secret and prompt to update the values -p : provide parameters for the pipeline --jenkins : deploy using a Jenkins pipeline --tekton : deploy using a Tekton pipeline --pipeline : the name of the Tekton pipeline -n : the deployment namespace; if not provided the namespace from the current context will be used -t : the template namespace; if not provided the value will default to tools Usage CLI Create a Jenkins pipeline in the current namespace and prompt for the Git credentials oc pipeline --jenkins Create a Tekton pipeline in the my-dev namespace, using the Git credentials gituser and gitpat oc pipeline -n my-dev -u gituser -P gitpat --tekton Manual Steps for Tekton The following is the list of steps required to manually configure a Tekton pipeline with your development cluster. Set the current namespace/project OpenShift oc project { namespace } Kubernetes kubectl config set-context --current --namespace ={ namespace } Copy the tasks from the tools namespace into the current namespace kubectl get tasks -o json -n tools | \\ jq 'del(.items[].metadata.uid) | del(.items[].metadata.selfLink) | del(.items[].metadata.resourceVersion) | del(.items[].metadata.namespace) | del(.items[].metadata.creationTimestamp) | del(.items[].metadata.generation) | del(.items[].metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - List the available pipeline templates in the tools namespace and select the one to use for your project. kubectl get pipelines -n tools Clone the selected pipeline from the tools namespace into the current namespace kubectl get pipeline ${ TEMPLATE_NAME } -o json -n tools | \\ jq --arg PIPELINE_NAME ${ PIPELINE_NAME } '.metadata.name = $PIPELINE_NAME | del(.metadata.uid) | del(.metadata.selfLink) | del(.metadata.resourceVersion) | del(.metadata.namespace) | del(.metadata.creationTimestamp) | del(.metadata.generation) | del(.metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' | \\ kubectl apply -f - where: - TEMPLATE_NAME is the name of the pipeline selected in the previous step - PIPELINE_NAME is the name of the pipeline for your project Start the pipeline \u00b6 The Tekton pipeline does not automatically start when it is first created. After the webhook is created in the subsequent steps the pipeline will start when changes are pushed to the repository but before that, we can manually trigger the build to start using the CLI. (The pipeline can also be started through the OpenShift Console.) Kick off the pipeline using the Tekton CLI tkn pipeline start { PIPELINE_NAME } -s pipeline -p git-url ={ GIT_REPO } -p git-revision ={ GIT_BRANCH } To create a new PipelineRun with the same parameters from a previous PipelineRun you can do the following tkn pipeline start { PIPELINE_NAME } --use-pipelinerun { PIPELINE_RUN_NAME } Create a Git Webhook** \u00b6 Create the event listener and triggers \u00b6 In order for a Tekton pipeline to be triggered by a webhook notification, several resources need to be created: TriggerTemplate - defines how to create the PipelineRun and any other required resources when a webhook notification is received. TriggerBinding - provides a mapping for the information available in the webhook payload into the TriggerTemplate EventListener - makes the connection between the Pipeline, TriggerBinding, and TriggerTemplate together that will be created when a webhook is triggered Create a file named tekton-trigger.yaml and paste in the following contents: apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerTemplate metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_TEMPLATE_NAME } spec : params : - description : The git revision name : gitrevision - description : The git repository url name : gitrepositoryurl resourcetemplates : - apiVersion : tekton.dev/v1beta1 kind : PipelineRun metadata : generateName : { PIPELINE_NAME } - spec : params : - name : git-url value : $(params.gitrepositoryurl) - name : git-revision value : $(params.gitrevision) - name : scan-image value : \"false\" pipelineRef : name : { PIPELINE_NAME } --- apiVersion : triggers.tekton.dev/v1alpha1 kind : TriggerBinding metadata : labels : app : { PIPELINE_NAME } name : { TRIGGER_BINDING_NAME } spec : params : - name : gitrevision value : $(body.head_commit.id) - name : gitrepositoryurl value : $(body.repository.url) --- apiVersion : triggers.tekton.dev/v1alpha1 kind : EventListener metadata : labels : app : { PIPELINE_NAME } name : { EVENT_LISTENER_NAME } spec : serviceAccountName : pipeline triggers : - bindings : - kind : TriggerBinding name : { TRIGGER_BINDING_NAME } interceptors : - cel : filter : header.match('X-GitHub-Event', 'push') && body.ref == 'refs/heads/{BRANCH_NAME}' name : { PIPELINE_NAME } template : name : { TRIGGER_TEMPLATE_NAME } Replace the place holder values with the appropriate values: where: - {PIPELINE_NAME} is the name of your Pipeline resource from the previous section. - {TRIGGER_TEMPLATE_NAME} is the name of the TriggerTemplate. This can be the same as the {PIPELINE_NAME} . - {TRIGGER_BINDING_NAME} is the name of the TriggerBinding. This can be the same as the {PIPELINE_NAME} . - {EVENT_LISTENER_NAME} is the name of the EventListener. This can be el-{PIPELINE_NAME} if the EventListeners will be configured one-to-one with the Pipelines or the instance can be shared across the project. - {BRANCH_NAME} is the name of the branch from which webhook events should trigger the build to start Apply the trigger resources to the cluster, in the same namespace where the Pipeline was created kubectl apply -f tekton-trigger.yaml In order for the Git repository to trigger the build with a webhook, an endpoint needs to be available. Expose the EventListener service with a route to provide that endpoint. oc expose service ${ EVENT_LISTENER_NAME } --name = ${ EVENT_LISTENER_NAME } Register the webhook url with your Git repository \u00b6 The particular steps will vary to create the Webhook depending on the flavor of hosted Git you are using (GitHub, GitHub Enterprise, GitLab, BitBucket, etc) but the general flow will remain the same. Get the host name for the route created in the previous step oc get route ${ EVENT_LISTENER_NAME } -o jsonpath = '{.spec.host}' Create a webhook in your hosted Git repository using the https url of the host name from the previous step that is triggered by the desired events (e.g. push, pull request, release) Manual steps for Jenkins on OpenShift Provision Jenkins ephemeral \u00b6 Jenkins ephemeral provides a kubernetes native version of Jenkins that dynamically provisions build agents on-demand. It's ephemeral meaning it doesn't allocate any persistent storage in the cluster. Set the project/namespace oc project { NAMESPACE } where: - {NAMESPACE} is the development namespace where the pipelines will run Run the following command to provision the Jenkins instance in your namespace oc new-app jenkins-ephemeral Open the OpenShift console as described in the login steps above Select Workloads -> Pods from the left-hand menu At the top of the page select your project/namespace from the drop-down list to see the Jenkins instance running Give the jenkins service account privileged access \u00b6 All of the Cloud-Native Toolkit pipelines use buildah to build and push the container image to the registry. Unfortunately, the buildah container must run as root. By default, OpenShift does not allow containers to run as the root user and special permission is required for the pipeline to run. With the Jenkins build engine, all the build processes run as the jenkins service account. In order for the pipeline container to run as root on OpenShift we will need to give the privileged security context constraint (scc) to jenkins service account with the following command: oc project { NAMESPACE } oc adm policy add-scc-to-user privileged -z jenkins where: - {NAMESPACE} should be the name you claimed in the box note prefixed to -dev (e.g. user01-dev) Create a secret with git credentials \u00b6 In order for Jenkins to have access to the git repository, particularly if it is a private repository, a Kubernetes secret needs to be added that contains the git credentials. Create a personal access token (if you don't already have one) using the prereq instructions Copy the following into a file called gitsecret.yaml and update the {Git-Username}, and {Git-PAT} apiVersion : v1 kind : Secret metadata : annotations : build.openshift.io/source-secret-match-uri-1 : https://github.com/* labels : jenkins.io/credentials-type : usernamePassword name : git-credentials type : kubernetes.io/basic-auth stringData : username : { Git-Username } password : { Git-PAT } where: - Git-Username is the username that has access to the git repo - Git-PAT is the personal access token of the git user After logging into the cluster, create the secret by running the following: oc project { NAMESPACE } oc create -f gitsecret.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run Create the build config \u00b6 On OpenShift 4.3, Jenkins is built into the OpenShift console and the build pipelines can be managed using Kubernetes custom resources. The following steps will create one by hand to create the build pipeline for the new application. Copy the following into a file called buildconfig.yaml and update the {Name}, {Secret}, {Git-Repo-URL}, and {Namespace} apiVersion : v1 kind : BuildConfig metadata : name : { Name } spec : triggers : - type : GitHub github : secret : my-secret-value source : git : uri : { Git-Repo-URL } ref : master strategy : jenkinsPipelineStrategy : jenkinsfilePath : Jenkinsfile env : - name : CLOUD_NAME value : openshift - name : NAMESPACE value : { NAMESPACE } where: - Name is in the name of your pipeline - Git-Repo-URL is the https url to the git repository - {NAMESPACE} is the development namespace where the pipelines will run Assuming you are still logged into the cluster, create the buildconfig resource in the cluster oc project { NAMESPACE } oc create -f buildconfig.yaml where: - {NAMESPACE} is the development namespace where the pipelines will run View the pipeline in the OpenShift console \u00b6 Open the OpenShift console for the cluster Select Builds -> Build Config Select your project/namespace (i.e. {NAMESPACE} ) from the top The build pipeline that was created in the previous step should appear Manually trigger the pipeline by selecting Start Build the menu button on the right side of the row Create the webhook \u00b6 Run the following to get the webhook details from the build config oc project { NAMESPACE } oc describe bc { Name } where: - {Name} is the name used in the previous step for the build config - {NAMESPACE} is the development namespace where the pipelines will run The webhook url will have a structure similar to: http://{openshift_api_host:port}/oapi/v1/namespaces/{namespace}/buildconfigs/{name}/webhooks/{secret}/generic In this case {secret} will be my-secret-value Open a browser to the GitHub repo deployed in the previous step in the build config Select Settings then Webhooks . Press Add webhook Paste the webhook url from the previous step into the Payload url Set the content-type to application/json and leave the rest of the values as the defaults Press Add webhook to create the webhook Press the button to test the webhook to ensure that everything was done properly Go back to your project code and push a change to one of the files Go to the Build pipeline page in the OpenShift console to see that the build was triggered Manual steps for Jenkins on Kubernetes TBD enable \u00b6 Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Jenkinsfile This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run igc pipeline to connect your repo to a pipeline in the environment. Command flags \u00b6 --repo : the set of pipelines to choose from; the default is https://github.com/ibm-garage-cloud/garage-pipelines -p : the name of the pipeline that should be installed; if not provided then you will be prompted -b : the branch from which the pipeline should be installed; the default is stable r : the version number of the pipeline that should be installed; the default is latest Usage \u00b6 CLI Before running the command, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Apply the pipeline updates using the CLI command igc enable Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them Manual steps The follow provides the manual steps equivalent to the igc enable command: Before updating the pipelines, make sure you have a clean repository with no unstaged changes. Either commit any changes or stash them temporarily with git stash . It is particularly important that any changes to the pipeline be dealt with. Download the index.yaml file containing the available pipeline versions curl -O https://ibm-garage-cloud.github.io/garage-pipelines/index.yaml Look through the index.yaml file to identify the url for the desired pipeline branch and version With the PIPELINE_URL from the previous step, run the following to download the pipeline tar-ball curl -O ${ PIPELINE_URL } Extract the tar-ball into your repository directory. You will be prompted to overwrite files. Overwrite as appropriate tar xzf ${ PIPELINE_FILE } Review the changes using git diff and revert any application-specific changes that should remain (e.g. customization to the Jenkins pipeline in the Jenkinsfile , specific values added to values.yaml , customizations to the templates in the helm chart ) Commit the changes when you are happy with them git-secret \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed. Command flags \u00b6 [positional] : overwrites the name of the config map -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage \u00b6 ===\"CLI\" The following gives an example of using the git-secret command to set up the config map and secret in the dev namespace ```shell igc git-secret -n dev ``` Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap showcase.myrepo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master gitops \u00b6 Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed. Command flags \u00b6 -n : the namespace where the secret and config map should be created. Defaults to the currently selected project/namespace -d : the directory where the cloned repository is located. Defaults to the current working directory -u : the username for the git secret, If not provided the value will be collected from a prompt -p : the personal access token, If not provided the value will be collected from a prompt --values : an optional yaml file that contains additional attributes to add to the secret --replace : flag indicating that the secret should be replaced/updated if it already exists Usage \u00b6 CLI The following gives an example of using the gitops command to set up the config map and secret in the dev namespace igc gitops -n dev Manual The following gives the equivalent commands to create the config map and secret for a git repository in the dev namespace Create the git-credentials secret kubectl create secret generic git-credentials -n dev \\ --from-literal = username ={ git username } \\ --from-literal = password ={ git personal access token } \\ --dry-run --output = yaml | \\ kubectl label -f - --local --dry-run --output = yaml \\ \"jenkins.io/credentials-type=usernamePassword\" | \\ kubectl annotate -f - --local --dry-run --output = yaml \\ \"build.openshift.io/source-secret-match-uri-1=https://github.com/*\" \\ \"tekton.dev/git-0=https://github.com\" | \\ kubectl apply -f - Create the config map for a git repo located at https://github.com/showcase/myrepo kubectl create configmap github-repo -n dev \\ --from-literal = host = github.com \\ --from-literal = org = showcase \\ --from-literal = repo = myrepo \\ --from-literal = url = https://github.com/showcase/myrepo \\ --from-literal = branch = master tool-config \u00b6 Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard. Command flags \u00b6 The name for the tool -n : the tools namespace; the default is tools --url : the endpoint for accessing the tool, usually its dashboard --username : (optional) the user name for logging into to tool --password : (optional) the password for logging into to tool Usage \u00b6 CLI The following gives an example of using the tool-config command to set up a tool named my-tool with its dashboard's endpoint and credentials igc tool-config my-tool \\ --url https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --username admin \\ --password password Manual install with helm The following gives an example of using helm directly to do the equivalent (using helm 3): helm install my-tool tool-config \\ --repo https://ibm-garage-cloud.github.io/toolkit-charts/ \\ --set url = https://mytool-dashboard.mycluster.us-east.containers.appdomain.cloud \\ --set username = admin \\ --set password = password vlan \u00b6 Lists the VLANs for a particular IBM Cloud region. This information is useful for preparing Terraform cluster creation steps. The command reads all the data centers in the region and allows you to select the appropriate data center for the vlan. This command requires that the terminal is already logged in to the cloud region. It does NOT need to be logged in to a cluster. Usage \u00b6 CLI List a pair of public/private VLANs for a new environment to use igc vlan Manual steps List the zones for the region ibmcloud ks zones --region-only --provider classic Select the desired zone from the listing provided by the previous command and run the following to list the vlans for that zone ibmcloud ks vlans --zone ${ zone }","title":"Command Line Tool"},{"location":"reference/cli/#cloud-native-toolkit-command-line-interface","text":"","title":"Cloud Native Toolkit - Command Line Interface"},{"location":"reference/cli/#invoking-the-cli","text":"When the CLI is installed , it adds an executable named igc to the PATH. Running igc --help will list the available commands. The output text will be similar to the following: $ igc --help IBM Garage Cloud Native Toolkit CLI (https://cloudnativetoolkit.dev) Usage: igc <command> [args] Commands: igc console Launch the IKS or OpenShift admin console igc create-webhook Create a git webhook for a given Jenkins pipeline igc credentials Lists the urls and credentials for the tools deployed to the cluster igc dashboard Open the Developer Dashboard in the default browser igc enable Enable the current repository with pipeline logic igc endpoints List the current ingress hosts for deployed apps in a namespace [aliases: ingress, endpoint, ingresses] igc git-secret [name] Create a kubernetes secret that contains the url, username, and personal access token for a git repo igc git [remote] Launches a browser to the git repo url specified by the remote. If not provided remote defaults to origin igc gitops Registers the git repository in the kubernetes cluster as the gitops repository for the given namespace igc sync [namespace] Create a namespace (if it does not exist) and prepare it with the necessary configuration [aliases: project, namespace] igc pull-secret [namespace] Copy pull secrets into the provided project from the template namespace igc pipeline [gitUrl] Register a pipeline for the current code repository igc tool-config [name] Create the config map and secret for a tool configured in the environment igc vlan Print out the vlan values igc yq <command> lightweight yaml command-line processor that addresses deficiencies with the existing `yq` command Options: --version Show version number [boolean] --help Show help [boolean] Info As of v0.5.1, the Toolkit CLI will now install the commands as plugins to the kubectl and oc CLIs. For example, all of the following are equivalent: igc pipeline kubectl pipeline oc pipeline","title":"Invoking the CLI"},{"location":"reference/cli/#prerequisite-tools","text":"Some of the commands provided by the Toolkit CLI orchestrate interactions between other CLIs. To get started please install the prerequisite tools , in particular: The Kubernetes CLI The Red Hat OpenShift CLI The IBM Cloud CLI - used to interact with IBM Cloud vlans (not needed if tools will not run on IBM Cloud)","title":"Prerequisite tools"},{"location":"reference/cli/#available-commands","text":"","title":"Available commands"},{"location":"reference/cli/#dashboard","text":"Opens the Developer Dashboard in the default browser. If a default browser has not been configured, then the URL to the Dashboard will be printed out. The dashboard displays the Cloud-Native Toolkit tools configured within the cluster along with links to activation content and links to Starter Kits to start a project quickly. This command requires that the login context for the cluster has already been established.","title":"dashboard"},{"location":"reference/cli/#console","text":"Opens the IKS or OpenShift admin console in the default browser. If a default browser has not been configured, then the URL to the console will be printed out. This command requires that the login context for the cluster has already been established.","title":"console"},{"location":"reference/cli/#git","text":"Opens the Git repo in the default browser for the current working directory. If a default browser has not been configured, then the URL to the repo will be printed out.","title":"git"},{"location":"reference/cli/#credentials","text":"Lists the endpoints, user names, and passwords for the tools configured in the environment. This is the easiest way to get the login credentials for each of the installed tools. Ideally all of the tools would be accessible via SSO at which point this command will be obsolete. The command works by reading information available in the cluster. When each tool is installed by the toolkit, a config map and secret are created to store the url and credential for the tool. That information is used in a number of different ways within the environment: Provide configuration information to the pipelines Populate the tiles on the Developer Dashboard Populate the results of the credentials command This command requires that the login context for the cluster has already been established.","title":"credentials"},{"location":"reference/cli/#endpoints","text":"Lists the ingress and/or route URLs for the applications in a given namespace. An attempt will be made to get the namespace from the current context if one is not provided as an argument. Results of the command are provided in an interactive menu. If one of the endpoints is selected, it will display the URL and launch it in the default browser. Selecting Exit will print the full list of endpoints and exit. This command requires that the login context for the cluster has already been established.","title":"endpoints"},{"location":"reference/cli/#sync","text":"Creates a Kubernetes namespace or OpenShift project (if it doesn't already exist) and sets it up so that the namespace can be used as a target for application deployments and/or to host the environment. The command synchronize the ConfigMaps and Secrets from a template namespace (ie tools ) to create a \"development\" namespace. After the command has run successfully it will set the provided namespace in the current context (e.g. equivalent to oc project X ) This command copies the relevant ConfigMaps and Secrets into the namespace that are needed for development activities. Managing resources across namespaces (particularly ConfigMaps and Secrets ) is a common challenge in Kubernetes environments. We have provided the command at this time to simplify the steps required to get everything ready. Ultimately, this problem seems like an ideal one for an Operator to solve and when one is available (either from the Toolkit or elsewhere) this command will be retired or transitioned. The command will setup the \"development\" namespace where DevOps pipelines can be run (e.g. myapp-dev) The \"development\" namespace will have the ConfigMaps and Secrets copied over. The command can also add additional privileges to the tekton pipeline service account. These privileges are needed to run the buildah task in OpenShift 4.7 Positionals: namespace The namespace that will be created and/or prepared Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] -p, --tekton flag indicating the tekton pipeline service account should be given privileged scc --verbose flag to produce more verbose logging [boolean]","title":"sync"},{"location":"reference/cli/#pull-secret","text":"Copy pull secrets into the provided project from the template namespace for the IBM Container Registry. Set up a service account in the namespace with the pull secret(s) for the IBM Container Registry that are copied. The pull secret(s) are required in order for pods to pull images that are stored in the IBM Container Registry. When the cluster is created in IBM Cloud, a pull secret is provided in the default namespace. In order for a pod in another namespace to use it, the secret must first be copied into the namespace. After that, the pod either needs to reference the pull secret directly or the service account used by the resource needs to have a reference to the secret. The CLI copies the pull secret over and adds it to the service account so the pod can take either approach. This command should be use to set up \"release\" namespaces where applications can be deployed (e.g. test, staging) Positionals: namespace The namespace into which the pull-secret(s) will be created Options: --version Show version number [boolean] --help Show help [boolean] -t, --templateNamespace the template namespace that will be the source of the config [string] [default: \"tools\"] -z, --serviceAccount the service account that will be used within the namespace [string] [default: \"default\"] --dev flag to indicate this is a development namespace and that development artifacts should be created[boolean] --verbose flag to produce more verbose logging [boolean]","title":"pull-secret"},{"location":"reference/cli/#pipeline","text":"Connects a branch in a Git repo to a either a Jenkins or Tekton CI pipeline in the environment and triggers an initial build. A webhook is also created so that when a new commit is added to the branch, the pipeline is triggered to start the process to rebuild and redeploy the app using the new code. Currently, webhook creation is supported for repositories hosted on Gitlab, Github, Github Enterprise, Bitbucket, and Gogs. This command can either be used to register a git repository that has previously been cloned to the local filesystem OR using the remote repo url.","title":"pipeline"},{"location":"reference/cli/#enable","text":"Adds DevOps artifacts to a Git repo that the environment uses to deploy the app. The command displays a list of available pipelines and applies the one you select to your code repo. The DevOps files added to your repo include (but are not limited to): Helm chart Jenkinsfile This command DOES NOT require that the terminal is already logged in to an IBM Cloud account nor the cluster. It DOES require that the terminal's current directory is the repository directory for your local copy of the Git repo. The command will add files to the local repo. You should commit these new files and push them to the server repo. Then run igc pipeline to connect your repo to a pipeline in the environment.","title":"enable"},{"location":"reference/cli/#git-secret","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named {git org}.{git repo} . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated it won't be updated unless the --replace argument is passed.","title":"git-secret"},{"location":"reference/cli/#gitops","text":"Creates a kubernetes secret that contains the username and personal access token for a git repo and a config map that contains the url, host, org, repo, and branch for the git repository. The secret is always named git-credentials and the config map is named gitops-repo . The config map and secret will be created in the currently selected namespace/project, unless a value is passed with the -n flag. If the git-credentials secret already exists then it won't be replaced/updated unless the --replace argument is passed.","title":"gitops"},{"location":"reference/cli/#tool-config","text":"Configures a new tool in the environment. After deploying the tool, use this command to add the tool to the list of credentials so that it will be displayed in the dashboard.","title":"tool-config"},{"location":"reference/cli/#vlan","text":"Lists the VLANs for a particular IBM Cloud region. This information is useful for preparing Terraform cluster creation steps. The command reads all the data centers in the region and allows you to select the appropriate data center for the vlan. This command requires that the terminal is already logged in to the cloud region. It does NOT need to be logged in to a cluster.","title":"vlan"},{"location":"reference/dashboard/","text":"Developer Dashboard \u00b6 Explore the resources at your fingertips provided by the Cloud-Native Toolkit Developer Dashboard The Developer Dashboard is one of the tools running in your developer environment. It is designed to help you navigate to the installed tools and provide a simple way to perform common developer tasks, such as: Dashboard : Navigate to the tools installed in the cluster Activation : Links to education to help you learn cloud-native development and deployment using IBM Cloud Kubernetes Service and Red Hat OpenShift on IBM Cloud Starter Kits : Links to templates that will help accelerate your project With the release of Red Hat OpenShift 4.x**, it is now even easier for developers to integrate the SDLC tools into the OpenShift console. This Developer Dashboard is mainly focused on providing a simple navigation experience when the Cloud-Native Toolkit is installed into IBM Cloud Kubernetes Service. Here are some recent improvements: More tools can be added to the dashboard using a simple igc tool-config command Prefix (shown above as \"IBM\") and Title (shown above as \"Cloud Native Toolkit\") can be customized to you own names The IBM Cloud link can be overridden to support links to other cloud vendors when OpenShift is running on Azure, AWS, Google Cloud, or VMWare The tools view is split into more columns to enable more reuse of the screen The Cluster information is now available when you click on \"Developer Dashboard\" title The Toolkit CLI now installs an alias into the oc and kubectl so it is now possible to open the dashboard quickly using oc dashboard and kubectl dashboard Tools configured with OpenShift console \u00b6 When the administrator configures your developer environment, they can customize a set of short cut links to common tools you often use as a developer. You can see how these tools are configured by reading the tools configuration guide Opening the Dashboard \u00b6 You can open the Dashboard from a terminal. Open a terminal Make sure your terminal is logged into your cluster In the IBM Cloud console, navigate to your cluster's overview Switch to the Access tab Follow the instructions to log in from the command line Use the Toolkit CLI ( igc ) to open the Dashboard in your environment OpenShift oc dashboards Kubernetes kubectl dashboard Toolkit CLI igc dashboard The command should print the url to the dashboard then open the default browser to the url. Opening the Kubernetes/OpenShift console \u00b6 Use the Toolkit CLI console command to open the IKS or OpenShift console: OpenShift oc console Kubernetes kubectl console Toolkit CLI igc console This command will determine the type of cluster (IKS or OpenShift), get the url for the console, and launch the url in the default browser. If the default browser is not available then the url will be printed to the screen. Access the URLs to endpoints in the cluster \u00b6 Use the Toolkit CLI endpoints command to list the endpoints for a given namespace/project: OpenShift oc endpoints -n tools Kubernetes kubectl endpoints -n tools Toolkit CLI igc endpoints -n tools This will return the route and ingress URLs for the tools namespace where the DevOps tools have been installed in the cluster: ? Endpoints in the 'tools' namespace. Select an endpoint to launch the default browser or 'Exit'. 1) Exit 2) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud 3) argocd-server - https://argocd-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 4) artifactory - https://artifactory-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 5) dashboard - https://dashboard-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 6) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud (Move up and down to reveal more choices) Answer: You can then select the URL and launch it in the default browser. Credentials \u00b6 In the future, the tools in the Dashboard will be linked using a single sign-on (SSO) service. In the meantime, the CLI includes a command to list the tools' logins. Use the Toolkit CLI credentials command to list the endpoints and credentials for the tools OpenShift oc credentials Kubernetes kubectl credentials Toolkit CLI igc credentials The command lists the userid and password for each tool installed. You can use these credentials to log into each of the installed tools. More of the tools in Red Hat OpenShift will be integrated into the OpenShift console login process Note If you are using OpenShift, the Jenkins credential will be undefined because the OpenShift built in Jenkins service uses IBM cloud single sign-on mechanism.","title":"Developer Dashboard"},{"location":"reference/dashboard/#developer-dashboard","text":"Explore the resources at your fingertips provided by the Cloud-Native Toolkit Developer Dashboard The Developer Dashboard is one of the tools running in your developer environment. It is designed to help you navigate to the installed tools and provide a simple way to perform common developer tasks, such as: Dashboard : Navigate to the tools installed in the cluster Activation : Links to education to help you learn cloud-native development and deployment using IBM Cloud Kubernetes Service and Red Hat OpenShift on IBM Cloud Starter Kits : Links to templates that will help accelerate your project With the release of Red Hat OpenShift 4.x**, it is now even easier for developers to integrate the SDLC tools into the OpenShift console. This Developer Dashboard is mainly focused on providing a simple navigation experience when the Cloud-Native Toolkit is installed into IBM Cloud Kubernetes Service. Here are some recent improvements: More tools can be added to the dashboard using a simple igc tool-config command Prefix (shown above as \"IBM\") and Title (shown above as \"Cloud Native Toolkit\") can be customized to you own names The IBM Cloud link can be overridden to support links to other cloud vendors when OpenShift is running on Azure, AWS, Google Cloud, or VMWare The tools view is split into more columns to enable more reuse of the screen The Cluster information is now available when you click on \"Developer Dashboard\" title The Toolkit CLI now installs an alias into the oc and kubectl so it is now possible to open the dashboard quickly using oc dashboard and kubectl dashboard","title":"Developer Dashboard"},{"location":"reference/dashboard/#tools-configured-with-openshift-console","text":"When the administrator configures your developer environment, they can customize a set of short cut links to common tools you often use as a developer. You can see how these tools are configured by reading the tools configuration guide","title":"Tools configured with OpenShift console"},{"location":"reference/dashboard/#opening-the-dashboard","text":"You can open the Dashboard from a terminal. Open a terminal Make sure your terminal is logged into your cluster In the IBM Cloud console, navigate to your cluster's overview Switch to the Access tab Follow the instructions to log in from the command line Use the Toolkit CLI ( igc ) to open the Dashboard in your environment OpenShift oc dashboards Kubernetes kubectl dashboard Toolkit CLI igc dashboard The command should print the url to the dashboard then open the default browser to the url.","title":"Opening the Dashboard"},{"location":"reference/dashboard/#opening-the-kubernetesopenshift-console","text":"Use the Toolkit CLI console command to open the IKS or OpenShift console: OpenShift oc console Kubernetes kubectl console Toolkit CLI igc console This command will determine the type of cluster (IKS or OpenShift), get the url for the console, and launch the url in the default browser. If the default browser is not available then the url will be printed to the screen.","title":"Opening the Kubernetes/OpenShift console"},{"location":"reference/dashboard/#access-the-urls-to-endpoints-in-the-cluster","text":"Use the Toolkit CLI endpoints command to list the endpoints for a given namespace/project: OpenShift oc endpoints -n tools Kubernetes kubectl endpoints -n tools Toolkit CLI igc endpoints -n tools This will return the route and ingress URLs for the tools namespace where the DevOps tools have been installed in the cluster: ? Endpoints in the 'tools' namespace. Select an endpoint to launch the default browser or 'Exit'. 1) Exit 2) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud 3) argocd-server - https://argocd-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 4) artifactory - https://artifactory-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 5) dashboard - https://dashboard-tools.gsi-learning-ocp311-clu-7ec5d722a0ab3f463fdc90eeb94dbc70-0001.eu-gb.containers.appdomain.cloud 6) developer-dashboard - http://dashboard.garage-dev-ocp4-c-518489-0143c5dd31acd8e030a1d6e0ab1380e3-0000.us-east.containers.appdomain.cloud (Move up and down to reveal more choices) Answer: You can then select the URL and launch it in the default browser.","title":"Access the URLs to endpoints in the cluster"},{"location":"reference/dashboard/#credentials","text":"In the future, the tools in the Dashboard will be linked using a single sign-on (SSO) service. In the meantime, the CLI includes a command to list the tools' logins. Use the Toolkit CLI credentials command to list the endpoints and credentials for the tools OpenShift oc credentials Kubernetes kubectl credentials Toolkit CLI igc credentials The command lists the userid and password for each tool installed. You can use these credentials to log into each of the installed tools. More of the tools in Red Hat OpenShift will be integrated into the OpenShift console login process Note If you are using OpenShift, the Jenkins credential will be undefined because the OpenShift built in Jenkins service uses IBM cloud single sign-on mechanism.","title":"Credentials"},{"location":"reference/git/","text":"Cloud Native Toolkit Git Repositories \u00b6 The Cloud Native Toolkit is developed in the open, with all content being managed in github repositories. This page provides a guide to the key repositories. The primary organization is https://github.com/cloud-native-toolkit . The most important repositories are pinned at the top of the organization page. Planning \u00b6 Work on the Cloud-Native Toolkit is managed using ZenHub . Install the plugin into your browser then access the planning github repository Installation \u00b6 The Toolkit can be installed using a number of different approaches, but they all end up using the Iteration-zero scripts and the terraform modules Note Some terraform modules have been broken out into their own repository Documentation \u00b6 The source for this site can be found in the developer guide repository Command Line Interface \u00b6 The CLI source can be found in the cli repository Starter Kits \u00b6 The Starter Kit source code is not maintained in the Cloud-Native Toolkit github organization, as they are widely used, so are located in the IBM main repository. You can find details in the Starter Kit section Tekton Pipelines and Tasks \u00b6 The pipeline source code, like the Starter Kit source code, is not maintained in the Cloud-Native Toolkit github organization. You can find details in the Pipeline section","title":"Git repositories"},{"location":"reference/git/#cloud-native-toolkit-git-repositories","text":"The Cloud Native Toolkit is developed in the open, with all content being managed in github repositories. This page provides a guide to the key repositories. The primary organization is https://github.com/cloud-native-toolkit . The most important repositories are pinned at the top of the organization page.","title":"Cloud Native Toolkit Git Repositories"},{"location":"reference/git/#planning","text":"Work on the Cloud-Native Toolkit is managed using ZenHub . Install the plugin into your browser then access the planning github repository","title":"Planning"},{"location":"reference/git/#installation","text":"The Toolkit can be installed using a number of different approaches, but they all end up using the Iteration-zero scripts and the terraform modules Note Some terraform modules have been broken out into their own repository","title":"Installation"},{"location":"reference/git/#documentation","text":"The source for this site can be found in the developer guide repository","title":"Documentation"},{"location":"reference/git/#command-line-interface","text":"The CLI source can be found in the cli repository","title":"Command Line Interface"},{"location":"reference/git/#starter-kits","text":"The Starter Kit source code is not maintained in the Cloud-Native Toolkit github organization, as they are widely used, so are located in the IBM main repository. You can find details in the Starter Kit section","title":"Starter Kits"},{"location":"reference/git/#tekton-pipelines-and-tasks","text":"The pipeline source code, like the Starter Kit source code, is not maintained in the Cloud-Native Toolkit github organization. You can find details in the Pipeline section","title":"Tekton Pipelines and Tasks"},{"location":"reference/reference/","text":"Toolkit Reference \u00b6 This section provides the technical reference for the components that make up the Cloud-Native Toolkit. Area Description Command Line Tool The Cloud-Native Toolkit Command Line Interface (CLI) Developer Dashboard The web application deployed with the Toolkit to provide easy access to Toolkit developer resources Git Repositories The source code for the Toolkit Starter Kits The projects that provide a starting point for new applications and services using the Toolkit Toolkit Components The Open Source projects that form the foundation of the Toolkit Pipeline reference Details of the pipelines and pipeline tasks provided by the Toolkit Iteration Zero The Toolkit installer. These assets use Terraform to install and configure the Cloud-Native Toolkit","title":"Reference"},{"location":"reference/reference/#toolkit-reference","text":"This section provides the technical reference for the components that make up the Cloud-Native Toolkit. Area Description Command Line Tool The Cloud-Native Toolkit Command Line Interface (CLI) Developer Dashboard The web application deployed with the Toolkit to provide easy access to Toolkit developer resources Git Repositories The source code for the Toolkit Starter Kits The projects that provide a starting point for new applications and services using the Toolkit Toolkit Components The Open Source projects that form the foundation of the Toolkit Pipeline reference Details of the pipelines and pipeline tasks provided by the Toolkit Iteration Zero The Toolkit installer. These assets use Terraform to install and configure the Cloud-Native Toolkit","title":"Toolkit Reference"},{"location":"reference/iteration-zero/iteration-zero/","text":"Iteration Zero \u00b6 Iteration Zero is the name of the process of installing and setting up the toolkit. The toolkit uses infrastructure as code using Terraform as the primary means of coordinating and performing the many component installs needed by the toolkit. The fast-start install uses all the default values provided by the modules to deliver a generic install of the Toolkit, but as you adopt the Toolkit you may want to customize the Iteration Zero process. The Iteration Zero logic is stored in two repositories: garage-terraform-modules The modules that provide logic to provision individual components of the infrastructure. These modules cover one of three different categories: infrastructure (e.g. create a kubernetes cluster), cloud service, or software deployed and configured into a cluster. https://github.com/cloud-native-toolkit/garage-terraform-modules ibm-garage-iteration-zero The logic that makes use of the modules with specific configuration parameters used to deliver an entire solution. https://github.com/cloud-native-toolkit/ibm-garage-iteration-zero This guide will walk through the various files that make up the Infrastructure as Code components and how to customize them. The Installation Overview walks through how to perform an install with the Iteration Zero scripts. Iteration Zero terraform scripts \u00b6 The Iteration Zero terraform scripts make use of the modules to provision and prepare an environment. The logic is provided as stages that can be removed and added as needed. Stages \u00b6 The files in the stages and stages-crc folders provide Terraform files that make use of external Terraform modules to provision resources. The different resources are logically grouped with stage numbers and names for the resource provided. All of the stages are processed by the Terraform apply at the same time and Terraform works out the sequencing of execution based on the dependencies between the modules. The Iteration Zero application comes with a pre-defined set of software and services that will be provisioned. For more advanced situations, that set of modules can be easily customized. Removing a stage To remove a stage, simply delete or move a file out of the stages directory Adding a stage To add a stage, define a new stage file and reference the desired module. Any necessary variables can be referenced from the base variables or the output from the other modules. Modifying a stage Any of the values for the variables in variables.tf or in the stage files can be updated to change the results of what is built Environment configuration \u00b6 There a number of files used to provide the overall configuration for the environment that will be provisioned. credentials.template Template file for the credentials.properties credentials.properties File containing the API key and Classic Infrastructure credentials needed to run the scripts terraform/settings/environment.tfvars General configuration values for the environment, like region , resource group and cluster type terraform/settings/vlan.tfvars Configuration values for the IBM Cloud vlan settings needed for the creation of a new cluster terraform/stages/variables.tf or terraform/stages-crc/variables.tf Defined variables for the various stages and, in some cases, default values. Scripts \u00b6 launch.sh Launches a container image from the Docker Hub registry that contains all the tools necessary to run the terraform scripts and opens into a shell where the Terraform logic can be run terraform/runTerraform.sh Based on the values configured in environment.tfvars , this script creates the terraform/workspace directory, copies the appropriate Terraform files into that directory, then applies the Terraform scripts terraform/scripts/apply.sh Applies the Terraform scripts. This script is copied into the terraform/workspace directory during the runTerraform.sh logic. It is then available to rerun the Terraform logic without having to set the terraform/workspace directory up again. terraform/scripts/destroy-cluster.sh Helper script that destroys the IBM Cloud cluster to clean up the environment terraform/scripts/destroy-services.sh Helper script that destroys services that have been provisioned in IBM Cloud. It works against the resource group that has been configured in the environment.tfvars file. Any values passed in as arguments will be used to do a regular expression match to exclude services from the list of those that will be destroyed. Terraform Modules \u00b6 The terraform modules project contains the building block components that can be used to create a provisioned environment. The modules are organized into one of three major categories: cloud-managed Modules that provision infrastructure (cluster and/or services) into a managed cloud environment self-managed Modules that provision infrastructure into a self-managed environment (e.g. software deployed into a cluster) generic Modules that can be applied independent of the environment (e.g. software that is installed into a running kubernetes environment) A listing of the modules is shown below:","title":"Iteration Zero"},{"location":"reference/iteration-zero/iteration-zero/#iteration-zero","text":"Iteration Zero is the name of the process of installing and setting up the toolkit. The toolkit uses infrastructure as code using Terraform as the primary means of coordinating and performing the many component installs needed by the toolkit. The fast-start install uses all the default values provided by the modules to deliver a generic install of the Toolkit, but as you adopt the Toolkit you may want to customize the Iteration Zero process. The Iteration Zero logic is stored in two repositories: garage-terraform-modules The modules that provide logic to provision individual components of the infrastructure. These modules cover one of three different categories: infrastructure (e.g. create a kubernetes cluster), cloud service, or software deployed and configured into a cluster. https://github.com/cloud-native-toolkit/garage-terraform-modules ibm-garage-iteration-zero The logic that makes use of the modules with specific configuration parameters used to deliver an entire solution. https://github.com/cloud-native-toolkit/ibm-garage-iteration-zero This guide will walk through the various files that make up the Infrastructure as Code components and how to customize them. The Installation Overview walks through how to perform an install with the Iteration Zero scripts.","title":"Iteration Zero"},{"location":"reference/iteration-zero/iteration-zero/#iteration-zero-terraform-scripts","text":"The Iteration Zero terraform scripts make use of the modules to provision and prepare an environment. The logic is provided as stages that can be removed and added as needed.","title":"Iteration Zero terraform scripts"},{"location":"reference/iteration-zero/iteration-zero/#stages","text":"The files in the stages and stages-crc folders provide Terraform files that make use of external Terraform modules to provision resources. The different resources are logically grouped with stage numbers and names for the resource provided. All of the stages are processed by the Terraform apply at the same time and Terraform works out the sequencing of execution based on the dependencies between the modules. The Iteration Zero application comes with a pre-defined set of software and services that will be provisioned. For more advanced situations, that set of modules can be easily customized. Removing a stage To remove a stage, simply delete or move a file out of the stages directory Adding a stage To add a stage, define a new stage file and reference the desired module. Any necessary variables can be referenced from the base variables or the output from the other modules. Modifying a stage Any of the values for the variables in variables.tf or in the stage files can be updated to change the results of what is built","title":"Stages"},{"location":"reference/iteration-zero/iteration-zero/#environment-configuration","text":"There a number of files used to provide the overall configuration for the environment that will be provisioned. credentials.template Template file for the credentials.properties credentials.properties File containing the API key and Classic Infrastructure credentials needed to run the scripts terraform/settings/environment.tfvars General configuration values for the environment, like region , resource group and cluster type terraform/settings/vlan.tfvars Configuration values for the IBM Cloud vlan settings needed for the creation of a new cluster terraform/stages/variables.tf or terraform/stages-crc/variables.tf Defined variables for the various stages and, in some cases, default values.","title":"Environment configuration"},{"location":"reference/iteration-zero/iteration-zero/#scripts","text":"launch.sh Launches a container image from the Docker Hub registry that contains all the tools necessary to run the terraform scripts and opens into a shell where the Terraform logic can be run terraform/runTerraform.sh Based on the values configured in environment.tfvars , this script creates the terraform/workspace directory, copies the appropriate Terraform files into that directory, then applies the Terraform scripts terraform/scripts/apply.sh Applies the Terraform scripts. This script is copied into the terraform/workspace directory during the runTerraform.sh logic. It is then available to rerun the Terraform logic without having to set the terraform/workspace directory up again. terraform/scripts/destroy-cluster.sh Helper script that destroys the IBM Cloud cluster to clean up the environment terraform/scripts/destroy-services.sh Helper script that destroys services that have been provisioned in IBM Cloud. It works against the resource group that has been configured in the environment.tfvars file. Any values passed in as arguments will be used to do a regular expression match to exclude services from the list of those that will be destroyed.","title":"Scripts"},{"location":"reference/iteration-zero/iteration-zero/#terraform-modules","text":"The terraform modules project contains the building block components that can be used to create a provisioned environment. The modules are organized into one of three major categories: cloud-managed Modules that provision infrastructure (cluster and/or services) into a managed cloud environment self-managed Modules that provision infrastructure into a self-managed environment (e.g. software deployed into a cluster) generic Modules that can be applied independent of the environment (e.g. software that is installed into a running kubernetes environment) A listing of the modules is shown below:","title":"Terraform Modules"},{"location":"reference/iteration-zero/terraform/","text":"Terraform Reference \u00b6 Todo Add a summary of the terraform content here Each terraform module should have a reference page in this section. Clearly setting out what the module does, any input needed (such as secrets or config maps pre-defined by previous modules, variables used to configure the module, etc.). The reference should also detail what config/secrets/CRDs the module creates that could be used in subsequent modules","title":"Terraform modules"},{"location":"reference/iteration-zero/terraform/#terraform-reference","text":"Todo Add a summary of the terraform content here Each terraform module should have a reference page in this section. Clearly setting out what the module does, any input needed (such as secrets or config maps pre-defined by previous modules, variables used to configure the module, etc.). The reference should also detail what config/secrets/CRDs the module creates that could be used in subsequent modules","title":"Terraform Reference"},{"location":"reference/starter-kit/starter-kit/","text":"Starter Kits \u00b6 Overview \u00b6 To complement the IBM Cloud-Native Toolkit this project provides a set of Starter Kits . These have been designed to support build to manage and give a production ready entry point for cloud-native development. While the use of the Starter Kits is not required to be able to use the development tools, their use is highly recommended to get up and running quickly. There are a small number of Starter Kit git repositories that provide support for different architecture layers of your cloud native solution these includes: User Interfaces Backend for Frontends Microservices Edge Apps You can pick the Starter Kit that best meets your requirements and it will seamlessly integrate with the installed development tools. You can work with more Starter Kits from developer.ibm.com . These patterns can be \"enabled\" using the Bring your own code approach on Starter Kits Docs . Even though they are \"enabled\" they are not optimized for production usage and do not include a number of key IBM Garage best practices. They are fully open source and they can be enhanced overtime. You can also work with the Starter Kits provided from the IBM Cloud IBM Cloud Starter Kits for the server side patterns you can \"enable\" these using the enable command on Cloud-Native Command Line Interface There is more information below on what is included in the Starter Kits . Why another Starter Kit \u00b6 As teams have built out production solutions using IBM Kubernetes Services and Red Hat OpenShift on the IBM Cloud it has became clear that starting with a hello-world style code slows down an agile team. In cases where you are learning the basic principles of cloud-native development with a specific language it does help to start with the basic principles of hello-world but when you are building production code to be used by real users it takes a lot of effort to industrialize this code ready for production. The objective of these Starter Kits is to kick off a project quickly so they can reach maximum development velocity in the least amount of time. To not contain any business logic and to have a set of proven opinionated frameworks that are commonly used in the industry this can include Express , React and testing frameworks. Why Starter Kits ? As the approach to cloud native microservice development has evolved a number of language frameworks have come into play, runtime configuration technologies and best practices to improve quality and robustness. It becomes very time consuming to create, manage and maintain these elements. If we look at a typical cloud-native app, they require a number of supporting files, similar to the ones found in the outer ring of this diagram. They take time to create and are only often needed for the initial seed of the project. Its also never clear where the documentation is for these elements and how they are work together. Some typical examples : Dockerfile Helm chart CI pipeline ( Jenkins , Tekton , etc.) TDD Frameworks Tracing Code Analysis Monitoring / Logging Support Cloud Service bindings and credentials User Case logic UI, BFF, Microservice The Starter Kits provide a key number of these supporting files and meta data and are tested on every code change to make sure they work with IBM Kubernetes and IBM Red Hat OpenShift managed clusters environments. Starter Kit Template \u00b6 The purpose of the Starter Kit is to provide the scaffold code for the elements outside of your working use case business logic. This will then allow a developer to get started a lot quicker and allow you to push code regularly into your CI environment. This code is optimized for build to manage scenarios where where logging, health are fully integrated. What constitutes a good Starter Kit \u00b6 If you want to contribute a Starter Kit have a look at the examples that are provided. They all have most of the features listed below. They must be UBI ( Universal Base Image ) based so they can run in Red Hat OpenShift as well as Kubernetes. They must have a Dockerfile that uses that UBI. To make them work with continuous integration, you can provide a Jenkins or Tekton pipeline. In some cases for Edge they base images may be different as they are optimized for size. Make sure there is a good use case that is repeatable in your solution architecture. Make sure the code is documented and includes a `README.md'. There is good code coverage for tests and you have integrated SonarQube code scanning on the build process. Finally them put in open source so other developers can enhance, improve or consume. README License Package.json Use case example source code for example APIs, UIs, Dashboards, Machine learning models Unit test framework Pact test framework Integration testing including User Experience tests SonarQube scan integration Dockerfile using Universal Base Image Helm chart that is production ready Jenkinsfile or Tekton pipeline that is production ready Make it Open Source","title":"Starter Kit overview"},{"location":"reference/starter-kit/starter-kit/#starter-kits","text":"","title":"Starter Kits"},{"location":"reference/starter-kit/starter-kit/#overview","text":"To complement the IBM Cloud-Native Toolkit this project provides a set of Starter Kits . These have been designed to support build to manage and give a production ready entry point for cloud-native development. While the use of the Starter Kits is not required to be able to use the development tools, their use is highly recommended to get up and running quickly. There are a small number of Starter Kit git repositories that provide support for different architecture layers of your cloud native solution these includes: User Interfaces Backend for Frontends Microservices Edge Apps You can pick the Starter Kit that best meets your requirements and it will seamlessly integrate with the installed development tools. You can work with more Starter Kits from developer.ibm.com . These patterns can be \"enabled\" using the Bring your own code approach on Starter Kits Docs . Even though they are \"enabled\" they are not optimized for production usage and do not include a number of key IBM Garage best practices. They are fully open source and they can be enhanced overtime. You can also work with the Starter Kits provided from the IBM Cloud IBM Cloud Starter Kits for the server side patterns you can \"enable\" these using the enable command on Cloud-Native Command Line Interface There is more information below on what is included in the Starter Kits .","title":"Overview"},{"location":"reference/starter-kit/starter-kit/#why-another-starter-kit","text":"As teams have built out production solutions using IBM Kubernetes Services and Red Hat OpenShift on the IBM Cloud it has became clear that starting with a hello-world style code slows down an agile team. In cases where you are learning the basic principles of cloud-native development with a specific language it does help to start with the basic principles of hello-world but when you are building production code to be used by real users it takes a lot of effort to industrialize this code ready for production. The objective of these Starter Kits is to kick off a project quickly so they can reach maximum development velocity in the least amount of time. To not contain any business logic and to have a set of proven opinionated frameworks that are commonly used in the industry this can include Express , React and testing frameworks. Why Starter Kits ? As the approach to cloud native microservice development has evolved a number of language frameworks have come into play, runtime configuration technologies and best practices to improve quality and robustness. It becomes very time consuming to create, manage and maintain these elements. If we look at a typical cloud-native app, they require a number of supporting files, similar to the ones found in the outer ring of this diagram. They take time to create and are only often needed for the initial seed of the project. Its also never clear where the documentation is for these elements and how they are work together. Some typical examples : Dockerfile Helm chart CI pipeline ( Jenkins , Tekton , etc.) TDD Frameworks Tracing Code Analysis Monitoring / Logging Support Cloud Service bindings and credentials User Case logic UI, BFF, Microservice The Starter Kits provide a key number of these supporting files and meta data and are tested on every code change to make sure they work with IBM Kubernetes and IBM Red Hat OpenShift managed clusters environments.","title":"Why another Starter Kit"},{"location":"reference/starter-kit/starter-kit/#starter-kit-template","text":"The purpose of the Starter Kit is to provide the scaffold code for the elements outside of your working use case business logic. This will then allow a developer to get started a lot quicker and allow you to push code regularly into your CI environment. This code is optimized for build to manage scenarios where where logging, health are fully integrated.","title":"Starter Kit Template"},{"location":"reference/starter-kit/starter-kit/#what-constitutes-a-good-starter-kit","text":"If you want to contribute a Starter Kit have a look at the examples that are provided. They all have most of the features listed below. They must be UBI ( Universal Base Image ) based so they can run in Red Hat OpenShift as well as Kubernetes. They must have a Dockerfile that uses that UBI. To make them work with continuous integration, you can provide a Jenkins or Tekton pipeline. In some cases for Edge they base images may be different as they are optimized for size. Make sure there is a good use case that is repeatable in your solution architecture. Make sure the code is documented and includes a `README.md'. There is good code coverage for tests and you have integrated SonarQube code scanning on the build process. Finally them put in open source so other developers can enhance, improve or consume. README License Package.json Use case example source code for example APIs, UIs, Dashboards, Machine learning models Unit test framework Pact test framework Integration testing including User Experience tests SonarQube scan integration Dockerfile using Universal Base Image Helm chart that is production ready Jenkinsfile or Tekton pipeline that is production ready Make it Open Source","title":"What constitutes a good Starter Kit"},{"location":"reference/starter-kit/starter-kits/","text":"Starter kits \u00b6 Overview \u00b6 In the Overview section we describe why Start Kits provide a perfect on ramp to help projects get started. The Developer Tools project is providing a set of seed templates that can be used to get projects moving quickly and focusing on use case business logic. Read up on why you should start with a Starter Kit and if you have not already tried to deploy your first app to show you how easy it is to get started Here are links to the base set of Starter Kits provided for the Cloud-Native Toolkit . To use the Starter Kits, click on the link and then the Template button to create a version in your own git organization. Then clone it onto your local machine and then use igc pipeline to register it with your Jenkins server. oc login oc get pods git clone <code pattern> | cd <code pattern> oc sync <project> --dev oc pipeline If you are bringing your own code run the oc enable command to add a Helm chart and make sure you code exposes a /health endpoint and the values.yaml is exposing the correct port for your application. Git Repo Links \u00b6 Title Subtitle Link React UI Patterns Carbon based UI to help with common patterns using React framework https://github.com/IBM/template-node-react Angular UI Patterns Carbon based UI to help with common patterns using Angular framework https://github.com/IBM/template-node-angular Typescript Microservice Node.js TypeScript Microservice offering OpenAPI endpoints https://github.com/IBM/template-node-typescript Typescript GraphQL Node.js TypeScript GraphQL Backend for Frontend https://github.com/IBM/template-graphql-typescript Spring Boot Microservice Spring Boot Java Microservice https://github.com/IBM/template-java-spring Go Gin Microservice Go Gin Microservice/Bff/Edge https://github.com/IBM/template-go-gin Quarkus Microservice Quarkus Java Microservice https://github.com/IBM/template-quarkus Liberty Microservice Open Liberty Java Microservice https://github.com/IBM/template-liberty Artificial Intelligence (AI) Microservice Deep Learning Model: Locate and identify multiple objects in a single image https://github.com/IBM/MAX-Object-Detector ArgoCD GitOps Template configuration to help to set up GitOps with ArgoCD https://github.com/IBM/template-argocd-gitops","title":"Available Starter Kits"},{"location":"reference/starter-kit/starter-kits/#starter-kits","text":"","title":"Starter kits"},{"location":"reference/starter-kit/starter-kits/#overview","text":"In the Overview section we describe why Start Kits provide a perfect on ramp to help projects get started. The Developer Tools project is providing a set of seed templates that can be used to get projects moving quickly and focusing on use case business logic. Read up on why you should start with a Starter Kit and if you have not already tried to deploy your first app to show you how easy it is to get started Here are links to the base set of Starter Kits provided for the Cloud-Native Toolkit . To use the Starter Kits, click on the link and then the Template button to create a version in your own git organization. Then clone it onto your local machine and then use igc pipeline to register it with your Jenkins server. oc login oc get pods git clone <code pattern> | cd <code pattern> oc sync <project> --dev oc pipeline If you are bringing your own code run the oc enable command to add a Helm chart and make sure you code exposes a /health endpoint and the values.yaml is exposing the correct port for your application.","title":"Overview"},{"location":"reference/starter-kit/starter-kits/#git-repo-links","text":"Title Subtitle Link React UI Patterns Carbon based UI to help with common patterns using React framework https://github.com/IBM/template-node-react Angular UI Patterns Carbon based UI to help with common patterns using Angular framework https://github.com/IBM/template-node-angular Typescript Microservice Node.js TypeScript Microservice offering OpenAPI endpoints https://github.com/IBM/template-node-typescript Typescript GraphQL Node.js TypeScript GraphQL Backend for Frontend https://github.com/IBM/template-graphql-typescript Spring Boot Microservice Spring Boot Java Microservice https://github.com/IBM/template-java-spring Go Gin Microservice Go Gin Microservice/Bff/Edge https://github.com/IBM/template-go-gin Quarkus Microservice Quarkus Java Microservice https://github.com/IBM/template-quarkus Liberty Microservice Open Liberty Java Microservice https://github.com/IBM/template-liberty Artificial Intelligence (AI) Microservice Deep Learning Model: Locate and identify multiple objects in a single image https://github.com/IBM/MAX-Object-Detector ArgoCD GitOps Template configuration to help to set up GitOps with ArgoCD https://github.com/IBM/template-argocd-gitops","title":"Git Repo Links"},{"location":"reference/tasks/pipelines/","text":"Pipelines \u00b6 The toolkit provides a number of pipelines for a number of programming languages. Each pipeline is composed of several tasks. A set of Tekton Pipelines and Pipeline tasks are imported to your cluster when the Cloud-Native Toolkit is installed. The source for these pipelines and tasks can be found in GitHub Todo Add a breakdown of the pipelines and tasks provided by the Toolkit. Each pipeline and task should have a separate reference page","title":"Pipelines"},{"location":"reference/tasks/pipelines/#pipelines","text":"The toolkit provides a number of pipelines for a number of programming languages. Each pipeline is composed of several tasks. A set of Tekton Pipelines and Pipeline tasks are imported to your cluster when the Cloud-Native Toolkit is installed. The source for these pipelines and tasks can be found in GitHub Todo Add a breakdown of the pipelines and tasks provided by the Toolkit. Each pipeline and task should have a separate reference page","title":"Pipelines"},{"location":"reference/tasks/placeholder/","text":"Task Placeholder page \u00b6 Todo Complete this content for each Pipeline and Task provided in the Toolkit Template content \u00b6 For each task the content should be: Description \u00b6 What is the purpose of the task Requirements \u00b6 For the task to complete successfully what are the assumptions, prereqs, ..... Source \u00b6 Link to github repo source","title":"placeholder"},{"location":"reference/tasks/placeholder/#task-placeholder-page","text":"Todo Complete this content for each Pipeline and Task provided in the Toolkit","title":"Task Placeholder page"},{"location":"reference/tasks/placeholder/#template-content","text":"For each task the content should be:","title":"Template content"},{"location":"reference/tasks/placeholder/#description","text":"What is the purpose of the task","title":"Description"},{"location":"reference/tasks/placeholder/#requirements","text":"For the task to complete successfully what are the assumptions, prereqs, .....","title":"Requirements"},{"location":"reference/tasks/placeholder/#source","text":"Link to github repo source","title":"Source"},{"location":"reference/tools/argocd/","text":"Argo CD (OpenShift GitOps) \u00b6 Use GitOps to continuously deliver application changes Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo. Configuring GitOps with Argo CD \u00b6 Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution Set up the GitOps repo \u00b6 Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD template used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD template . Click Use this template - If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test Hook the CI pipeline to the CD pipeline \u00b6 The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the repositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc). If you used the IGC CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the IGC CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the IGC CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD. Configure Release namespaces \u00b6 ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Create a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets Register the GitOps repo in ArgoCD \u00b6 Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops Create a project in Argo CD \u00b6 In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note: Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note The --dest and --src arguments can be provided multiple times if there are multiple destinations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) shell script kubectl apply -f project.yaml -n tools Add an application in Argo CD for each application component \u00b6 Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. ==== \"Argo CD UI\" 1. Log into Argo CD user interface 1. Press New Application and provide the following values: - application name - The name of the application. It is recommend to use the format of {namespace}-{image name} - project - The ArgoCD project with which the application should be included - sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended - repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) - revision - The Git branch where the configuration for this instance is stored - path - The path within the repository where the application config is located (should be the application name) - destination cluster - The cluster url for the deployment - destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) 1. Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools Managing secrets in Argo CD \u00b6 The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation . Prepare the Key Protect instance \u00b6 As the name suggests, the Argo CD Key Protect plugin ) leverages the capabilities of the Key Protect service to manage the protected information. The details for setting up and managing the Key Protect instance can be found in Secret management with Key Protect . From those instructions you can find the information required for the subsequent steps. Create the secret configuration \u00b6 The input to the plugin is a directory that contains one or more yaml \"secret templates\". In this case the \"secret template\" provides the structure of the desired secret with placeholders for the values that will be pulled from the key management system. Create a directory to contain the secret configuration. The Argo CD template repository has a template in templates/secrets-plugin that can be copied as a starting point Update the values in the yaml file for the secret that will be created apiVersion : keymanagement.ibm/v1 kind : SecretTemplate metadata : name : mysecret annotations : key-manager : key-protect key-protect/instanceId : instance-id key-protect/region : us-east spec : labels : {} annotations : {} values : - name : url value : https://ibm.com - name : username b64value : dGVhbS1jYXA= - name : password keyId : 36397b07-d98d-4c0b-bd7a-d6c290163684 The metadata.annotations value is optional. key-manager - the only value supported currently is key-protect key-protect/instanceId - the instance id of the key protect instance. If not provided then the instance-id value from the key-protect-access secret will be used. key-protect/region - the region where the key protect instance has been provisioned. If not provided then the region value from the key-protect-access secret will be used. The metadata.name value given will be used as the name for the Secret that will be generated. The information in spec.labels and spec.annotations will be copied over as the labels and annotations in the Secret that is generated The spec.values section contains the information that should be provided in the data section of the generated Secret. There are three possible ways the values can be provided: value - the actual value can be provided directly as clear text. This would be appropriate for information that is not sensitive but is required in the secret b64value - a base64 encoded value can be provided to the secret. This can be used for large values that might present formatting issues or for information that is not sensitive but that might be obfuscated a bit (like a username) keyId - the id (not the name) of the Standard Key that has been stored in Key Protect. The value stored in Key Protect can be anything Commit the changes to the GitOps repository Add the secret application in Argo CD \u00b6 Once the configuration has been added to the GitOps repository, Argo CD needs to be configured to deploy the secrets. === \"Argo CD UI 1. Log into Argo CD user interface 1. Press New Application and provide the following values: - application name - The name of the application. It is recommend to use the format of {namespace}-{image name} - project - The project with which the application should be included - sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended - repository url - The Git url where the configuration is stored - revision - The branch where the configuration for this instance is stored - path - The path within the repository where the application config is located (should be the application name) - destination cluster - The cluster url for the deployment - destination namespace - The namespace where the application should be deployed - Plugin - In the last section of the UI select Plugin from the dropdown - key-protect-secret - Click in the name field and select key-protect-secret from the dropdown 1. Repeat that step for each secret application and environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when SSO authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } \\ --config-management-plugin key-protect-secret where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named secret-application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } plugin : name : key-protect-secret syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f secret-application.yaml -n tools Configure another cluster as an Argo CD deployment target \u00b6 Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"ArgoCD"},{"location":"reference/tools/argocd/#argo-cd-openshift-gitops","text":"Use GitOps to continuously deliver application changes Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The deployment environment is a namespace in a container platform like Kubernetes or Red Hat OpenShift. Argo CD models a collection of applications as a project and uses a Git repository to store the application's desired state. Argo CD is flexible in the structure of the application configuration represented in the Git repository. Argo CD supports defining Kubernetes manifests in a number of ways: helm charts kustomize ksonnet jsonnet plain directory of yaml/json manifests custom plugins Argo CD compares the actual state of the application in the cluster with the desired state defined in Git and determines if they are out of sync. When it detects the environment is out of sync, Argo CD can be configured to either send out a notification to kick off a separate reconciliation process or Argo CD can automatically synchronize the environments to ensure they match. Note Confidential information like passwords and security tokens should not be checked into the Git repository. Managing secrets in Argo CD provides information on how to handle confidential information in the GitOps repo.","title":"Argo CD (OpenShift GitOps)"},{"location":"reference/tools/argocd/#configuring-gitops-with-argo-cd","text":"Terminology: Argo CD uses a number of terms to refer to the components Application - A deployable unit In the context of the environment, an application is one Helm chart that contains one container image that was produced by one CI pipeline. While Helm charts and images could certainly be combined to make more sophisticated applications in more advanced scenarios, we will be using this simple definition here. Project - A collection of applications that make up a solution","title":"Configuring GitOps with Argo CD"},{"location":"reference/tools/argocd/#set-up-the-gitops-repo","text":"Argo CD uses a Git repo to express the desired state of the Kubernetes environment. The basic setup uses one repository to represent one project . Within that repository, each application that makes up the project will be described in its own folder. The repository will also contain a branch for each destination (i.e. cluster and namespace) into which we want to deploy the applications. Note There is nothing special about a git repository used for git-ops. All that is required at a minimum is a hosted git repository that is accessible from by the Argo CD instance. The Argo CD template used in the following steps is optional and provides some application templates to help simplify some configuration activities. Create a new repo from the Argo CD template . Click Use this template - If you see a 404 error when you click on the link, you need to sign in to github. Clone the project to your machine git clone ${ GIT_URL_GITOPS } navigate into the directory cd ${ GIT_DIRECTORY } Create and push test branch git checkout -b test git push -u origin test","title":"Set up the GitOps repo"},{"location":"reference/tools/argocd/#hook-the-ci-pipeline-to-the-cd-pipeline","text":"The last stage in the CI pipeline updates a GitOps repository with the updated application metadata from the build. In order to do that, the CI pipeline needs to know which repository should be used and needs the credentials to push changes to that repository. As with other configuration within the pipeline, this is handled with config maps and secrets: A secret named git-credentials holds the credentials the CI pipeline uses to access all the repositories in the Git host (e.g. GitHub, GitLab, BitBucket, etc). If you used the IGC CLI to register the pipeline then this secret has already been created. A config map named gitops-repo holds the url and branch for the gitops repository. Fortunately the IGC CLI provides a gitops command to simplify this step. Information on how to use the command as well as the alternative manual steps can be found in the IGC CLI gitops command section. Make sure to switch context to the project/namespace CI namespace oc project ${ DEV_NAMESPACE } Run the gitops command to create the config map and secret in the CI namespace igc gitops Note For the secret to be available to the CI pipeline, the secret needs to be created in the same namespace where the pipeline is running. The value provided for branch is the one the pipeline will use to when committing changes to trigger the CD pipeline. As of v2.0.0 of the Tekton tasks and the Jenkins pipelines, the CI pipeline will create a folder and the initial configuration for an application deployment if it doesn't already exist. This means, there is no other manual configuration required to set up the repository. Now run a new Pipeline and make sure a directory for the application is created on the gitops git repository. This is required before configuring ArgoCD.","title":"Hook the CI pipeline to the CD pipeline"},{"location":"reference/tools/argocd/#configure-release-namespaces","text":"ArgoCD will deploy the application into the \"releases\" namespace such as ${TEST_NAMESPACE} or ${STAGING_NAMESPACE} Create a release namespace where ArgoCD will deploy the application oc new-project ${ TEST_NAMESPACE } The release namespaces need pull secrets for the application container images to be pull. OpenShift Image Registry If you are using the OpenShift Image Registry then you need to give permission to the services accounts in the \"release\" namespaces to be able to pull images from the \"development\" namespaces. Grant access to service accounts in the new test or staging namespace to pull the container image from the dev namespace oc policy add-role-to-group system:image-puller system:serviceaccounts: { TEST_NAMESPACE } -n { DEV_NAMESPACE } IBM Container Registry If you are using the IBM Container Registry then you need to copy the pull secret all-icr-io from the default namespace and then add this secret to the service account used by your application (ie default service account) Use the Toolkit CLI to copy the secret and setup the service account igc pull-secret ${ TEST_NAMESPACE } -t default -z default Other Container Registries If you are using an external image registry from a 3rd party provider like quay.io or dockerhub.io then you need to create the pull secret of type \"docker-registry\" and add it to the service account. Check the OpenShift Documentation Using pull secrets","title":"Configure Release namespaces"},{"location":"reference/tools/argocd/#register-the-gitops-repo-in-argocd","text":"Now that the repository has been created, we need to tell ArgoCD where it is. Log into ArgoCD Click on the gear icon on the left menu to access the Settings options Select the Repositories option Click either the Connect Repo using HTTPS or Connect Repo using SSH button at the top and provide the information for the GitOps repo you just created. For HTTPS you can use the access token you used when you ran igc gitops","title":"Register the GitOps repo in ArgoCD"},{"location":"reference/tools/argocd/#create-a-project-in-argo-cd","text":"In Argo CD terms, each deployable component is an application and applications are grouped into projects. Projects are not required for Argo CD to be able to deploy applications, but it helps to organize applications and provide some restrictions on what can be done for applications that make up a project. Argo CD UI Log into the Argo CD user interface Click on the gear icon on the left menu to access the Settings options Select the Projects option Press the New Project button at the top of the page Specify the properties for the new project Name - Provide the name for the project Description - A brief description of the project Source - Press Add source and pick the Git repository from the list that was added previously Destinations Add https://kubernetes.default.svc for the cluster url and ${TEST_NAMESPACE} for the namespace Add https://kubernetes.default.svc for the cluster url and ${STAGING_NAMESPACE} for the namespace Press Create Note: Initially, the only cluster that is available is the one in which Argo CD is - https://kubernetes.default.svc . By adding the two destinations we have allowed the project to be deployed to both the ${TEST_NAMESPACE} and ${STAGING_NAMESPACE} namespaces within the current cluster. Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create the project argocd proj create { PROJECT } --dest { CLUSTER_HOST } , { TEST_NAMESPACE } --src { GIT_REPO } where: {PROJECT} is the name you want to give to the project. {CLUSTER_HOST} is the url for the cluster server to which the project applications can be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed. * can also be used to allow deployments to any server. {TEST_NAMESPACE} is the namespace in the cluster where the applications can be deployed. * can be used to indicate any namespace. {GIT_REPO} is the url of the git repository where the gitops config will be located or * if you want to allow any. Note The --dest and --src arguments can be provided multiple times if there are multiple destinations and/or sources that should be configured for the project Argo CD custom resource Log in to the cluster from the command line Create a file named project.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : AppProject metadata : name : { PROJECT_NAME } spec : sourceRepos : - '*' destinations : - namespace : '*' server : https://kubernetes.default.svc where: {PROJECT NAME} is the name of the project Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) shell script kubectl apply -f project.yaml -n tools","title":"Create a project in Argo CD"},{"location":"reference/tools/argocd/#add-an-application-in-argo-cd-for-each-application-component","text":"Warning Before continuing to setup ArgoCD, please verify that the CI Pipeline run created the directory for the application on the gitops git repository and the directory container the helm related files including requirements.yaml The last step in the process is to define the application(s) within Argo CD that should be managed. This consists of connecting the config within the Git repo to the cluster and namespace. ==== \"Argo CD UI\" 1. Log into Argo CD user interface 1. Press New Application and provide the following values: - application name - The name of the application. It is recommend to use the format of {namespace}-{image name} - project - The ArgoCD project with which the application should be included - sync-policy - The manner with which Argo CD will use to manage the deployed artifacts. Automatic is recommended - repository url - The Git url where the configuration is stored (restricted to git urls configured in the Argo Project) - revision - The Git branch where the configuration for this instance is stored - path - The path within the repository where the application config is located (should be the application name) - destination cluster - The cluster url for the deployment - destination namespace - The namespace where the application should be deployed (restricted to namespaces configured in the Argo Project) 1. Repeat that step for each application and each environment Argo CD CLI Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Create an application argocd app create { APP_NAME } \\ --project { PROJECT } \\ --repo { GIT_REPO } \\ --path { APP_FOLDER } \\ --dest-namespace { NAMESPACE } \\ --dest-server { SERVER_URL } where: {APP_NAME} is the name you want to give the application {PROJECT} is the name of the project created above or \"default\" {GIT_REPO} is the url of the git repository where the gitops config is be located {APP_FOLDER} is the path to the configuration for the application in the gitops repo {DEST_NAMESPACE} is the target namespace in the cluster where the application will be deployed {SERVER_URL} is the url of the cluster where the application will be deployed. Use https://kubernetes.default.svc to reference the same cluster where Argo CD has been deployed Argo CD custom resource Log in to the cluster from the command line Create a file named application.yaml with the following contents apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : { APPLICATION NAME } spec : destination : namespace : { TARGET NAMESPACE } server : { TARGET CLUSTER } project : { PROJECT NAME } source : path : { APPLICATION PATH } repoURL : { REPO URL } targetRevision : { REPO BRANCH } helm : valueFiles : - values.yaml syncPolicy : automated : prune : true selfHeal : true where: {APPLICATION NAME} is the name of the application {PROJECT NAME} is the name of the project (the one created in the previous step or default ) {TARGET NAMESPACE} is the namespace where the application should be deployed {TARGET CLUSTER} is the server api url for the cluster where the application should be deployed. Use https://kubernetes.default.svc for the same cluster where Argo CD has been deployed {APPLICATION PATH} is the path to the folder where the secret template was created {REPO URL} is the url to the git repository {REPO BRANCH} is the branch in the git repository that contains the application configuration Apply the yaml to the cluster in the namespace where Argo CD has been deployed (e.g. tools ) kubectl apply -f application.yaml -n tools","title":"Add an application in Argo CD for each application component"},{"location":"reference/tools/argocd/#managing-secrets-in-argo-cd","text":"The following provides the steps to handle protected information in a GitOps repository using the Argo CD Key Protect plugin . The plugin takes yaml configuration as input, looks up the credentials in Key Protect, and generates a secret in the cluster from the results. Note If Argo CD was installed via the Toolkit then the plugin should already be installed and ready to use. If not, the instructions for installing the plugin can be found in the plugin documentation .","title":"Managing secrets in Argo CD"},{"location":"reference/tools/argocd/#configure-another-cluster-as-an-argo-cd-deployment-target","text":"Argo CD supports deploying applications into clusters other than the one into which it has been installed. To do that, the target cluster must first be registered with Argo CD. After that, the deployment target within an application configuration. The following describes the steps required to add another cluster as a deployment target: Log in to the Argo CD CLI argocd login { GRPC_INGRESS_HOST } --grpc-web [ --sso ] where: GRPC_INGRESS_HOST is the host name of the grpc endpoint. On Red Hat OpenShift this is the same as the UI host --sso is an optional flag used when sso authentication is enabled. If not using SSO then you will be prompted for the username and password The grpc url and credentials can be retrieved from the igc credentials command Log into the target cluster from the command line Run the Argo CD command to list the available clusters argocd cluster add The currently selected cluster will be the one with the asterisk next to it. Copy the cluster id from the table Add the cluster to Argo CD argocd cluster add { CLUSTER ID } where: {CLUSTER ID} is the id of the target cluster from the previous step Confirm the configuration by listing the known clusters argocd cluster list","title":"Configure another cluster as an Argo CD deployment target"},{"location":"reference/tools/artifactory/","text":"Artifactory \u00b6 Use Artifactory to store artifacts such as Helm charts and maven assets In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. The environment uses Artifactory as an artifact repository manager, which it uses to host its Helm repository. What is Artifactory \u00b6 Artifactory is an artifact management repository. An artifact repository manager hosts multiple binary repositories, like a database management system for artifacts. The binary repository compliments the source code repository: the code from an SCM is the input to the build process, whereas a binary repository stores the output of the build process, often called artifacts. The artifacts are often individual application components that can later be assembled into a full product. An artifact repository manager is an integral part of a CI/CD solution, a companion to the pipeline. As the pipeline builds artifacts, they're stored in the repositories. When the pipeline later needs artifacts that have already been built, they're retrieved from the repositories. This enables a build to be broken into smaller stages with well-defined inputs and outputs and provides better tracking of each stage's results. Often a failed pipeline can restart in the middle using artifacts that were already built and stored. An artifact repository often serves as the storage for a package manager, which assembles an application from artifacts. Here are some common package managers and their repositories: Maven : Builds Java artifacts (such as Jar, War, Ear, etc.) and projects stored in Maven repositories such as Maven Central npm : Assembles programs from JavaScript packages stored in npm-registries such as the public npm registry PIP : Installs Python packages from index repositories such as the Python Package Index (PyPI) Helm : Deploys applications to Kubernetes using charts stored in Helm repositories such as the Helm Hub catalog of repositories Docker is not a package manager, but its architecture includes an artifact repository: Docker : Stores images in Docker registries such as Docker Hub Note that you do not need a very large team to start reaping benefits from an artifact repository manager. The initial investment is not very large and the benefits are felt immediately. Artifact management in the Pipeline \u00b6 Note: Be sure to set up Artifactory before using it in the Development Tools environment, if you are using external Artifactory that was not set up as part of the toolkit installation. The environment will eventually be extended to store a number of artifact types in Artifactory. Thus far, the CI and CD pipelines exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: Docker images : The Developer Tools Image Registry Helm charts : A Helm repository in Artifactory The templates have also been configured to store their Helm charts in Artifactory. Artifactory is part of the environment's complete CI/CD solution: Continuous Integration [Continuous Delivery]../../learning/fast-cd.md){: target=_blank} Artifactory dashboard \u00b6 Use the Artifactory dashboard to browse the repositories and their artifacts. Open the Artifactory web UI for your environment. Use the Developer Dashboard to open the Artifactory dashboard Browse the Helm repository. Go to the Artifact Repository Browser page Expand the tree for the generic-local repository, which is the Helm repository Expand the branch for your environment's cluster, such as showcase-dev-iks The artifacts in the cluster's branch follow the Helm chart repository structure: index.yaml : Helm's index of all of the charts in the repository charts : The tgz files named for the application they deploy Browse the artifacts to see how a Helm repository is organized. Select the index.yaml file and View it to see its contents Expand a chart's branch to see that the tgz file contains the chart directory structure Notice that each chart has its own URL in Artifactory, and index lists the URL for a chart. Conclusion \u00b6 The environment includes an artifact repository manager called Artifactory, which it uses to host a Helm repository. As the CI pipeline builds the Helm chart for an application, it stores the chart in the Artifactory repository. When the ArgoCD pipeline deploys an application, it retrieves the chart from the Artifactory repository.","title":"Artifactory"},{"location":"reference/tools/artifactory/#artifactory","text":"Use Artifactory to store artifacts such as Helm charts and maven assets In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. The environment uses Artifactory as an artifact repository manager, which it uses to host its Helm repository.","title":"Artifactory"},{"location":"reference/tools/artifactory/#what-is-artifactory","text":"Artifactory is an artifact management repository. An artifact repository manager hosts multiple binary repositories, like a database management system for artifacts. The binary repository compliments the source code repository: the code from an SCM is the input to the build process, whereas a binary repository stores the output of the build process, often called artifacts. The artifacts are often individual application components that can later be assembled into a full product. An artifact repository manager is an integral part of a CI/CD solution, a companion to the pipeline. As the pipeline builds artifacts, they're stored in the repositories. When the pipeline later needs artifacts that have already been built, they're retrieved from the repositories. This enables a build to be broken into smaller stages with well-defined inputs and outputs and provides better tracking of each stage's results. Often a failed pipeline can restart in the middle using artifacts that were already built and stored. An artifact repository often serves as the storage for a package manager, which assembles an application from artifacts. Here are some common package managers and their repositories: Maven : Builds Java artifacts (such as Jar, War, Ear, etc.) and projects stored in Maven repositories such as Maven Central npm : Assembles programs from JavaScript packages stored in npm-registries such as the public npm registry PIP : Installs Python packages from index repositories such as the Python Package Index (PyPI) Helm : Deploys applications to Kubernetes using charts stored in Helm repositories such as the Helm Hub catalog of repositories Docker is not a package manager, but its architecture includes an artifact repository: Docker : Stores images in Docker registries such as Docker Hub Note that you do not need a very large team to start reaping benefits from an artifact repository manager. The initial investment is not very large and the benefits are felt immediately.","title":"What is Artifactory"},{"location":"reference/tools/artifactory/#artifact-management-in-the-pipeline","text":"Note: Be sure to set up Artifactory before using it in the Development Tools environment, if you are using external Artifactory that was not set up as part of the toolkit installation. The environment will eventually be extended to store a number of artifact types in Artifactory. Thus far, the CI and CD pipelines exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: Docker images : The Developer Tools Image Registry Helm charts : A Helm repository in Artifactory The templates have also been configured to store their Helm charts in Artifactory. Artifactory is part of the environment's complete CI/CD solution: Continuous Integration [Continuous Delivery]../../learning/fast-cd.md){: target=_blank}","title":"Artifact management in the Pipeline"},{"location":"reference/tools/artifactory/#artifactory-dashboard","text":"Use the Artifactory dashboard to browse the repositories and their artifacts. Open the Artifactory web UI for your environment. Use the Developer Dashboard to open the Artifactory dashboard Browse the Helm repository. Go to the Artifact Repository Browser page Expand the tree for the generic-local repository, which is the Helm repository Expand the branch for your environment's cluster, such as showcase-dev-iks The artifacts in the cluster's branch follow the Helm chart repository structure: index.yaml : Helm's index of all of the charts in the repository charts : The tgz files named for the application they deploy Browse the artifacts to see how a Helm repository is organized. Select the index.yaml file and View it to see its contents Expand a chart's branch to see that the tgz file contains the chart directory structure Notice that each chart has its own URL in Artifactory, and index lists the URL for a chart.","title":"Artifactory dashboard"},{"location":"reference/tools/artifactory/#conclusion","text":"The environment includes an artifact repository manager called Artifactory, which it uses to host a Helm repository. As the CI pipeline builds the Helm chart for an application, it stores the chart in the Artifactory repository. When the ArgoCD pipeline deploys an application, it retrieves the chart from the Artifactory repository.","title":"Conclusion"},{"location":"reference/tools/container-image-security-enforcement/","text":"Container Image Security Enforcement \u00b6 Container Image Security Enforcement enables trust within your container workloads. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. Container Image security is applied on a per-cluster basis. This can be enabled in for a cluster in IBM Cloud in a single command using the IBM Cloud CLI . ibmcloud oc cluster image-security enable -c <cluster name> This command will configure the cluster to use Container Image Security, which installs Portieris in the cluster, and will also create ClusterImagePolicy instances to enforce signatures for IBM Cloud images. Portieris is a Kubernetes admission controller for the enforcement of image security policies. With Portieris you can create image security policies either for each Kubernetes namespace or at the cluster level, and enforce different rules for different images. Portieris will block deployment for any image that fails signature validation as defined by the image policies. Thus ensuring that the images running inside of your cluster are unmodified from their original source. Note Portieris can be used in clusters that are not running on IBM Cloud by installing via the helm chart . Enabling Policy Enforcement \u00b6 Portieris uses RedHat Signatures to sign container images. To take advantage of Portieris and policy enforcement, you need 3 things: A GnuPG key to sign container images, stored in a vault A process to sign container images using the key from the credentials vault An ImagePolicy or ClusterImagePolicy that can instruct Portieris to apply enforcement rules The following steps are based on signing images for trusted content . Getting started quickly \u00b6 A script that demonstrates how to easily create a GPG key, publish it to a vault, setup cluster secrets, and setup a default ClusterImagePolicy (as described below) is available at https://github.com/IBM/ibm-garage-tekton-tasks/blob/image-signing/utilities/setup-image-signing-keys.sh The toolkit's 8-image-release.yaml tekton task has also been updated to accept the output of this script and enforce signatures during the image release phase. Create an Image Signing Key \u00b6 An image signing key must be created to sign the container images. This can be done by executing the following command: gpg --generate-key This will create a public and private key combination that can be used to sign and verify container images. The output of the gpg --generate-key command will show the fingerprint of the newly generated key. This fingerprint will be needed in the following steps. Saving the private key in a vault \u00b6 Once your key has been generated, the private key should be stored within a credentials vault, such as Key Protect or IBM HyperProtect Crypto Services . The private key should be placed in the vault as a base64 encoded string, which can be accessed by your Tekton pipeline during the image building task. To place the private key in a vault ENCODED_PRIVATE_KEY = $( gpg --export-secret-key <KEY_FINGERPRINT> | base64 ) curl -X POST https://<region>.kms.cloud.ibm.com/api/v2/keys \\ -H 'authorization: Bearer <IAM_token>' \\ -H 'bluemix-instance: <instance_ID>' \\ -H 'content-type: application/vnd.ibm.kms.key+json' \\ -d \"{ \\\"metadata\\\": { \\\"collectionType\\\": \\\"application/vnd.ibm.kms.key+json\\\", \\\"collectionTotal\\\": 1 }, \\\"resources\\\": [ { \\\"type\\\": \\\"application/vnd.ibm.kms.key+json\\\", \\\"name\\\": \\\"image-signing-key\\\", \\\"aliases\\\": [], \\\"description\\\": \\\"Private key for signing container images\\\", \\\"payload\\\": \\\" $ENCODED_PRIVATE_KEY \\\", \\\"extractable\\\": true } ] }\" Both Key Protect and Hyper Protect Crypto Services have an identical API, so the previous steps are identical except that a different API endpoint is used. Saving the public key \u00b6 The public key needs to be made available to the cluster for verifying container image signatures by either creating a secret within the cluster, or making the public key available through Artifactory . Use the following commands to make the public key available for policy enforcement by creating a secret within the cluster: gpg --export --armour <KEY_FINGERPRINT> > key.pubkey oc create secret generic image-signing-public-key --from-file = key = key.pubkey Signing container images \u00b6 Container image policy enforcement will reject images that are not signed, so you need to sign images either when they are pushed to the container registry. This can be done using either the skopeo copy command or buildah push command, depending when you want to sign your images. Extracting the private key for signing \u00b6 The following commands can be used to access your private key from the vault, and import it into gpg for use in signing. This would be used inside of your pipeline: echo \"Getting private key from keystore for image signing\" curl -s -o payload \\ https://<region>.kms.cloud.ibm.com/api/v2/keys/<key_ID_or_alias> \\ -H \"Authorization: Bearer <IAM_token>\" \\ -H \"Content-Type: application/json\" \\ -H \"bluemix-instance: <instance_ID>\" ENCODEDKEY = $( jq \".resources[0].payload\" -r payload ) echo $ENCODEDKEY > encodedkey base64 -d encodedkey > decodedkey echo \"Importing key\" gpg --import decodedkey Once the key is imported, then the image can be signed. If the image is being signed at build time, the signature can be specified by the --sign-by parameter to the buildah command: buildah --sign-by <KEY_FINGERPRINT> --storage-driver = overlay push --digestfile ./image-digest ${ APP_IMAGE } docker:// ${ APP_IMAGE } If the image is being signed at copy-time, it can be specified as a parameter to the skopeo command: skopeo --sign-by <KEY_FINGERPRINT> copy ${ IMAGE_FROM_CREDS } docker:// ${ IMAGE_FROM } docker:// ${ IMAGE_TO } Note On Linux\u00ae and macOS: The default configuration for the signing tools is to store the signatures locally. Storing signatures locally can lead to signature verification failure because the signature is not in the registry. To fix this problem, you can modify or delete the configuration file. On Linux\u00ae, the configuration is saved in /etc/containers/registries.d/default.yaml. On macOS, the configuration file is saved in /usr/local/etc/containers/registries.d/default.yaml. If you sign images in your container registry, yet your deployments are failing with the message policy denied the request: A signature was required, but no signature exists , then the default configuration is likely saving your image signatures locally instead of pushing the signature to the registry API server and you need to modify the tools configuration. Create image policies \u00b6 Finally, image policies need to be created to instruct Portieris which keys should be used to sign images from specific container registries. These policies can be applied globally to the entire cluster using a ClusterImagePolicy , or to a specific namespace using an ImagePolicy resource. In those policies, rules can be defined for enforcement for specific container registries/namespaces, or globally to all container registries used by the cluster. For example, the following ClusterImagePolicy enforces a policy that all images in the container registry private.us.icr.io/mynamespace/* must be signed by the public key that was earlier created and placed into the image-signing-public-key cluster secret. This policy should be updated for your own registry namespace and images. apiVersion : portieris.cloud.ibm.com/v1 kind : ClusterImagePolicy metadata : name : mynamespace-cluster-image-policy spec : repositories : - name : \"private.us.icr.io/mynamespace/*\" policy : mutateImage : false simple : requirements : - type : \"signedBy\" keySecret : image-signing-public-key This policy also uses the mutateImage:false configuration so that the GitOps operations using ArgoCD do not enter an infinite loop due to mutated image paths. More information about policies and enforcement and image mutation can be found in the Portieris Policies documentation. Tekton tasks \u00b6 A script that demonstrates how to easily create a GPG key, publish it to a vault, setup cluster secrets, and setup a default ClusterImagePolicy is available at IBM/ibm-garage-tekton-tasks/setup-image-signing-keys.sh The toolkit's 8-image-release.yaml tekton task has also been updated to accept the output of this script and enforce signatures during the image release phase. Impact to Kubernetes yaml or helm charts \u00b6 The Portieris image signing tools require an explicit specification which image pull secrets should be used to retrieve the signature/trust data. You deployment must specify an imagePullSecret value, or else the trust/verification will fail. Additional Information \u00b6 Additional information on trusted content and policy enforcement can be found at: Signing images for trusted content Gnu Privacy Guard (GPG) RedHat Signatures Portieris Portieris Policies Key Protect API Docs Hyper Protect API Docs","title":"Container Image Security Enforcement"},{"location":"reference/tools/container-image-security-enforcement/#container-image-security-enforcement","text":"Container Image Security Enforcement enables trust within your container workloads. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. Container Image security is applied on a per-cluster basis. This can be enabled in for a cluster in IBM Cloud in a single command using the IBM Cloud CLI . ibmcloud oc cluster image-security enable -c <cluster name> This command will configure the cluster to use Container Image Security, which installs Portieris in the cluster, and will also create ClusterImagePolicy instances to enforce signatures for IBM Cloud images. Portieris is a Kubernetes admission controller for the enforcement of image security policies. With Portieris you can create image security policies either for each Kubernetes namespace or at the cluster level, and enforce different rules for different images. Portieris will block deployment for any image that fails signature validation as defined by the image policies. Thus ensuring that the images running inside of your cluster are unmodified from their original source. Note Portieris can be used in clusters that are not running on IBM Cloud by installing via the helm chart .","title":"Container Image Security Enforcement"},{"location":"reference/tools/container-image-security-enforcement/#enabling-policy-enforcement","text":"Portieris uses RedHat Signatures to sign container images. To take advantage of Portieris and policy enforcement, you need 3 things: A GnuPG key to sign container images, stored in a vault A process to sign container images using the key from the credentials vault An ImagePolicy or ClusterImagePolicy that can instruct Portieris to apply enforcement rules The following steps are based on signing images for trusted content .","title":"Enabling Policy Enforcement"},{"location":"reference/tools/container-image-security-enforcement/#getting-started-quickly","text":"A script that demonstrates how to easily create a GPG key, publish it to a vault, setup cluster secrets, and setup a default ClusterImagePolicy (as described below) is available at https://github.com/IBM/ibm-garage-tekton-tasks/blob/image-signing/utilities/setup-image-signing-keys.sh The toolkit's 8-image-release.yaml tekton task has also been updated to accept the output of this script and enforce signatures during the image release phase.","title":"Getting started quickly"},{"location":"reference/tools/container-image-security-enforcement/#create-an-image-signing-key","text":"An image signing key must be created to sign the container images. This can be done by executing the following command: gpg --generate-key This will create a public and private key combination that can be used to sign and verify container images. The output of the gpg --generate-key command will show the fingerprint of the newly generated key. This fingerprint will be needed in the following steps.","title":"Create an Image Signing Key"},{"location":"reference/tools/container-image-security-enforcement/#signing-container-images","text":"Container image policy enforcement will reject images that are not signed, so you need to sign images either when they are pushed to the container registry. This can be done using either the skopeo copy command or buildah push command, depending when you want to sign your images.","title":"Signing container images"},{"location":"reference/tools/container-image-security-enforcement/#create-image-policies","text":"Finally, image policies need to be created to instruct Portieris which keys should be used to sign images from specific container registries. These policies can be applied globally to the entire cluster using a ClusterImagePolicy , or to a specific namespace using an ImagePolicy resource. In those policies, rules can be defined for enforcement for specific container registries/namespaces, or globally to all container registries used by the cluster. For example, the following ClusterImagePolicy enforces a policy that all images in the container registry private.us.icr.io/mynamespace/* must be signed by the public key that was earlier created and placed into the image-signing-public-key cluster secret. This policy should be updated for your own registry namespace and images. apiVersion : portieris.cloud.ibm.com/v1 kind : ClusterImagePolicy metadata : name : mynamespace-cluster-image-policy spec : repositories : - name : \"private.us.icr.io/mynamespace/*\" policy : mutateImage : false simple : requirements : - type : \"signedBy\" keySecret : image-signing-public-key This policy also uses the mutateImage:false configuration so that the GitOps operations using ArgoCD do not enter an infinite loop due to mutated image paths. More information about policies and enforcement and image mutation can be found in the Portieris Policies documentation.","title":"Create image policies"},{"location":"reference/tools/container-image-security-enforcement/#tekton-tasks","text":"A script that demonstrates how to easily create a GPG key, publish it to a vault, setup cluster secrets, and setup a default ClusterImagePolicy is available at IBM/ibm-garage-tekton-tasks/setup-image-signing-keys.sh The toolkit's 8-image-release.yaml tekton task has also been updated to accept the output of this script and enforce signatures during the image release phase.","title":"Tekton tasks"},{"location":"reference/tools/container-image-security-enforcement/#impact-to-kubernetes-yaml-or-helm-charts","text":"The Portieris image signing tools require an explicit specification which image pull secrets should be used to retrieve the signature/trust data. You deployment must specify an imagePullSecret value, or else the trust/verification will fail.","title":"Impact to Kubernetes yaml or helm charts"},{"location":"reference/tools/container-image-security-enforcement/#additional-information","text":"Additional information on trusted content and policy enforcement can be found at: Signing images for trusted content Gnu Privacy Guard (GPG) RedHat Signatures Portieris Portieris Policies Key Protect API Docs Hyper Protect API Docs","title":"Additional Information"},{"location":"reference/tools/ibm-cloud-container-registry/","text":"IBM Cloud Container Registry \u00b6 The CI/CD pipeline stores container images in an image registry In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. When hosted in IBM Cloud, the environment uses the IBM Cloud Container Registry for storing container images. What is the IBM Cloud Container Registry \u00b6 IBM Cloud Container Registry is a private, multi-tenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: Developer builds the image; ideally it is automated as part of a CI pipeline Docker private registry stores the image that was built UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image Accessing the registry \u00b6 There are two ways to work with an IBM Cloud registry : Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. When you installed the prerequisites , part of installing the IBM Cloud CLI installed several other tools and plug-ins including the container-registry plug-in. To use the container-registry plug-in, or even to push an image into the registry using a local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: ibmcloud login ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images. Registry organization \u00b6 Like the directories and file names in a file system, a Docker registry is a single collection of images that is cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: docker tag <image> <namesapce>/<repo-name>:<tag> docker push <namesapce>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: docker push <domain>/<namesapce>/<repo-name>:<tag> Registry organization in an IBM Cloud account \u00b6 IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multi-tenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1.0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1.0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1.0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1.0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images. IBM Cloud Container Registry features \u00b6 IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it adds several features to the registry service. The registry in each region is private, multi-tenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Running containers are not scanned , just the images in the registry. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using RedHat Signatures . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account manager can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" By using IAM to control access to the registry, a manager can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The managers can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace. Image registry in the Pipeline \u00b6 The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: Docker images : This Developer Tools Image Registry Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers. Give it a try \u00b6 Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the template named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, the share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1.0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1.0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found was configuration issues Scroll down to see the list of configuration issues Conclusion \u00b6 We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multi-tenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"IBM Container Registry"},{"location":"reference/tools/ibm-cloud-container-registry/#ibm-cloud-container-registry","text":"The CI/CD pipeline stores container images in an image registry In IBM Garage Method, one of the Develop practices is to automate continuous delivery through a delivery pipeline , in part by using an artifact repository for storing output of the build stage. When hosted in IBM Cloud, the environment uses the IBM Cloud Container Registry for storing container images.","title":"IBM Cloud Container Registry"},{"location":"reference/tools/ibm-cloud-container-registry/#what-is-the-ibm-cloud-container-registry","text":"IBM Cloud Container Registry is a private, multi-tenant Docker registry built into IBM Cloud for storing OCI images . Each IBM Cloud region hosts its own highly available registry. When deploying an application to a Kubernetes or OpenShift cluster, the cluster creates containers using the images in the registry. To package an application for deployment, the runtime must be built into an image that is stored in the registry. In this standard Docker diagram, the acme.com domain is effectively an IBM Cloud region and the Docker private registry is the instance of IBM Cloud Container Registry in that region. The diagram shows these components and their relationships: Developer builds the image; ideally it is automated as part of a CI pipeline Docker private registry stores the image that was built UAT , Promote , and Production are deployment environments, such as Kubernetes clusters or namespaces, that run the containers based on the image","title":"What is the IBM Cloud Container Registry"},{"location":"reference/tools/ibm-cloud-container-registry/#accessing-the-registry","text":"There are two ways to work with an IBM Cloud registry : Web UI : In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry CLI : Use the container-registry CLI plug-in in the IBM Cloud CLI Of the two approaches, the CLI is much more powerful. The console is mainly for looking at your registry. When you installed the prerequisites , part of installing the IBM Cloud CLI installed several other tools and plug-ins including the container-registry plug-in. To use the container-registry plug-in, or even to push an image into the registry using a local Docker install, you must first log into IBM Cloud and then log into the region's IBM Cloud Container Registry: ibmcloud login ibmcloud cr login The logins determine the namespaces you can access . Other accounts for other tenants also have namespaces in the registry, but IBM Cloud doesn't let you see them and prevents you from accessing their images.","title":"Accessing the registry"},{"location":"reference/tools/ibm-cloud-container-registry/#registry-organization","text":"Like the directories and file names in a file system, a Docker registry is a single collection of images that is cataloged with hierarchical names. A Docker registry such as Docker Hub (or, as we'll see, a registry in IBM Cloud) stores an image with a hierarchical name: namespace, repository, and tag. This path is specified when tagging and pushing the image: docker tag <image> <namesapce>/<repo-name>:<tag> docker push <namesapce>/<repo-name>:<tag> To tag and push an image to any registry other than Docker Hub, you have to specify its domain as part of the path: docker push <domain>/<namesapce>/<repo-name>:<tag>","title":"Registry organization"},{"location":"reference/tools/ibm-cloud-container-registry/#registry-organization-in-an-ibm-cloud-account","text":"IBM's registry organizes images in this same hierarchical structure. Domain : Each region in IBM Cloud (e.g. Dallas, London, Sydney, etc.) has its own multi-tenant registry instance with its own domain name (such as us.icr.io for Dallas/us-south). IBM provides its public images in a global registry (domain icr.io (no region)). Namespace : A namespace is associated with an IBM Cloud account and groups the account's images. Every user in the account can view and work with all images in the namespace, but users outside of the account cannot access images in the account's namespaces. An account may use multiple namespaces to organize images for groupings such as development vs. production or approved vs. experimental. Each namespace must have a name that is unique within a region for all accounts (not just your account). Repository : A repo is often thought of as the name of the image, but technically the same image can have different names (but the same image ID). Within a registry, different images with the same name stored in the same namespace will be stored in the same repo as long as they have different tags. Tag : Optional; if a command does not specify it, the default tag is latest . Two different tags enable a namespace to store two images with different image IDs but the same repository name. The tag typically specifies a different build of an image with a different image ID. Two builds usually package two different versions of an application's code, but Docker does not enforce that. The two builds could just be the same Dockerfile run twice with the same inputs (and therefore equivalent), or two completely unrelated sets of software. But two builds usually run software that is similar but at least slightly different, such as a new version of an application's code, but could be simply an alternative implementation of the code (such as for A/B testing). Therefore, when adding an image to a registry in IBM Cloud , the push command specifies the image's hierarchical path like this: docker push <region>.icr.io/<my_namespace>/<image_repo>:<tag> You can see this structure when you show the registry using the CLI: ibmcloud cr image-list Listing images... Repository Tag Digest Namespace Created Size Security status us.icr.io/showcase-dev-iks/template-graphql-typescript 1.0.0 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-graphql-typescript 1.0.0-5 6b3575c122e9 showcase-dev-iks 6 hours ago 303 MB 5 Issues us.icr.io/showcase-dev-iks/template-java-spring 1.0.0 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues us.icr.io/showcase-dev-iks/template-java-spring 1.0.0-14 24f3cdf69605 showcase-dev-iks 7 hours ago 213 MB No Issues Notice the columns labeled Repository (which is domain/namespace/repo) and Tag --those comprise the path for finding each image. And Namespace is specified again in its own column, which is useful for filtering searches. You can also see that the first two items in the list are not two separate images but really the same image with two tags: the image ID (a.k.a. digest) is the same, so the two tags are two different ways to look up the same image. Likewise, the last two images are really the same image with two tags. The registry in the IBM Cloud console shows the same images: It shows more clearly that each image has two tags, rather than being two different images.","title":"Registry organization in an IBM Cloud account"},{"location":"reference/tools/ibm-cloud-container-registry/#ibm-cloud-container-registry-features","text":"IBM Cloud Container Registry is not only a Docker registry hosted in IBM Cloud, it adds several features to the registry service. The registry in each region is private, multi-tenant, and highly available--properties that a simple registry doesn't have. Here are some other features and capabilities. Vulnerability Advisor scans images in the registry to search for known security issues and generates reports with advice on how to fix your images and better secure your environment. Lists of the vulnerabilities scanned for are available in Vulnerable packages . An administrator can specify exemptions that should not be reported. Running containers are not scanned , just the images in the registry. In the image lists shown above, the Security status column shows the number of issues found; the report will explain them in greater detail. In the console, click on the number of issues for details. Trusted content technology : IBM Cloud Container Registry supports images signed using RedHat Signatures . The signature confirms who built the image, such as the CI tool. The push and pull commands maintain image signatures. Container Image Security Enforcement verifies container images before deploying them to a cluster. You can control where images are deployed from, enforce Vulnerability Advisor policies, and ensure that content trust is properly applied to the image. If an image does not meet your policy requirements, the pod is not deployed to your cluster or updated. User authorization : While all users in an account have access to that account's namespaces in the registry, an account manager can use IAM to manage the access for different users. For example, a common customer concern is: \"How can an administrator control which images can be downloaded from Docker Hub and deployed into production?\" By using IAM to control access to the registry, a manager can disable the ability to push, build, or delete images, then create a policy to allow these actions and only assign it to certain trusted users such as the CI pipeline's service ID. These privileged users should only add approved images into the registry, thereby limiting the containers that the developers can deploy. The managers can likewise limit what the CD pipeline can possibly deploy to production by creating a namespace that only includes images approved for production and restricting the CD pipeline to deploy from that namespace.","title":"IBM Cloud Container Registry features"},{"location":"reference/tools/ibm-cloud-container-registry/#image-registry-in-the-pipeline","text":"The CI and CD pipelines currently exchange two types of artifacts: Docker images and Helm charts. The CI pipeline ( Jenkins , Tekton , etc.) builds these artifacts and ArgoCD deploys them. To store and share the artifacts, the pipeline uses two repositories: Docker images : This Developer Tools Image Registry Helm charts : A Helm repository in Artifactory In the CI pipeline, the Build image stage creates the Docker image and stores it in the registry. Then the Deploy to DEV env stage specifies the image's path in the Helm chart's values file, which the chart will use to deploy the app. Likewise, the CD pipeline specifies the image's registry path in the values files for the other deployments. When the Helm chart runs, it and Kubernetes read the image from the registry and start the containers.","title":"Image registry in the Pipeline"},{"location":"reference/tools/ibm-cloud-container-registry/#give-it-a-try","text":"Let's take a look at using the registry. If you haven't already, deploy your first app For example, deploy the template named Typescript Microservice I deployed my in a project named dev-guide-example-bw In the IBM Cloud console, navigate to either Kubernetes or OpenShift and then Registry It doesn't matter whether you navigate via the Kubernetes panel or the OpenShift panel, the share the same registry The registry is the IBM Cloud Container Registry for your region In the registry, search for the image named for your project In the CLI, run the corresponding command ibmcloud cr image-list | grep dev-guide-example-bw us.icr.io/mooc-team-one/dev-guide-example-bw 1.0.0 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues us.icr.io/mooc-team-one/dev-guide-example-bw 1.0.0-1 a2138c3025ac mooc-team-one 4 hours ago 286 MB 5 Issues Back in the console, click on the image to see its details Under Security Status , click on 5 issues to see the issues that Vulnerability Advisor found in this image What it found was configuration issues Scroll down to see the list of configuration issues","title":"Give it a try"},{"location":"reference/tools/ibm-cloud-container-registry/#conclusion","text":"We've seen that the CI pipeline packages an app as a Docker image and stores it in the registry, and also builds a Helm chart that references the image in the registry so that the chart can deploy containers built from the image. In IBM Cloud, the registry is the IBM Cloud Container Registry. The registry displays all of the namespaces in your account but not those in other tenants' accounts. In addition to basic registry functions, IBM Cloud adds additional features: private, multi-tenant, highly available, Vulnerability Advisor, Trusted content technology, Container Image Security Enforcement, and User authorization. Using the registry, you can see your list of images, details about the image, and details about any issues that Vulnerability Advisor found.","title":"Conclusion"},{"location":"reference/tools/jenkins/","text":"Tekton (OpenShift pipelines) \u00b6 Information Openshift Pipelines (Tekton) is the preferred pipeline technology used by the Toolkit. Use Jenkins to automate your continuous integration process Jenkins is a self-contained, open source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. It is a perfect tool for helping manage continuous integration tasks for a wide range of software components. Jenkins Pipeline (or simply \"Pipeline\") is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins. A continuous delivery pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Jenkins Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code.\" The definition of a Jenkins Pipeline is typically written into a text file (called a Jenkinsfile ){: target=_blank}) that in turn is checked into a project\u2019s source control repository. Pipelines \u00b6 Pipelines offer a set of stages or steps that can be chained together to allow a level of software automation. This automation can be tailored to the specific project requirements. You can read more information about Jenkins Pipelines here ){: target=_blank} Stages \u00b6 Pipelines are defined in a Jenkinsfile that sits in the root of your application code. It defines a number of stages. Each of the templates ){: target=_blank} includes a Jenkinsfile that offers a number of stages. The stages have been configured to complete the build, test, package, and deploy of the application code. Each stage can use the defined defined secrets and config maps that were previously configured during the installation of Development cluster setup. Developer Tools Pipeline \u00b6 To enable application compatibility between Kubernetes and OpenShift, the Jenkinsfile is consistent between pipeline registration with both platforms. Also, the Docker images are built from UBI images ){: target=_blank} so that their containers can run on both platforms. These are the stages in the pipeline and a description of what each stage does. The bold stage names indicate the stages that are required; the italics stage names indicate optional stages that can be deleted or will be ignored if the tool supporting the stage is not installed. These stages represent a typical production pipeline flow for a cloud-native application. Setup : Clones the code into the pipeline Build : Runs the build commands for the code Test : Validates the unit tests for the code Publish pacts : Publishes any pact contracts that have been defined Sonar scan : Runs a sonar code scan of the source code and publishes the results to SonarQube Verify environment : Validates the OpenShift or IKS environment configuration is valid Build image : Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env : Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check : Validates the Health Endpoint of the deployed application Package Helm Chart : Stores the tagged version of the Helm chart in Artifactory Trigger CD Pipeline : This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test Registering Pipelines \u00b6 The templates ){: target=_blank} are a good place to start to see how Jenkinsfile and Dockerfile should be configured for use in a Jenkins CI pipeline. To register your git repo, use the IGC CLI ){: target=_blank}. This command automates a number of manual steps you would have to do with Jenkins, including: managing secrets, webhooks, and pipeline registration in the Jenkins tools. igc pipeline By default, the pipeline will register into the dev namespace and will copy all the configMaps and secrets from the tools namespace to the dev namespace. This means the pipeline can execute, knowing it has access to the key information that enables it to integrate with both the cloud platform and the various development tools. See Cluster Configuration ){: target=_blank} for more detailed information. Registering Pipeline in new namespace \u00b6 You can use any namespace you want to register a pipeline. If you add -n or namespace to your igc pipeline command, it will create a new namespace if it doesn't already exist. It will copy the necessary secrets and configMaps into that namespace and configure the build agents pods to run in that namespace. igc pipeline -n team-one This is good if you have various squads, teams, pairs or students working in the same Development Tools environment. Continuous deployment \u00b6 In addition to continuous integration, the environment also supports continuous delivery using Artifactory and ArgoCD: Artifact Management with Artifactory Continuous Delivery with ArgoCD","title":"Jenkins"},{"location":"reference/tools/jenkins/#tekton-openshift-pipelines","text":"Information Openshift Pipelines (Tekton) is the preferred pipeline technology used by the Toolkit. Use Jenkins to automate your continuous integration process Jenkins is a self-contained, open source automation server that can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. It is a perfect tool for helping manage continuous integration tasks for a wide range of software components. Jenkins Pipeline (or simply \"Pipeline\") is a suite of plugins that supports implementing and integrating continuous delivery pipelines into Jenkins. A continuous delivery pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Jenkins Pipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines \"as code.\" The definition of a Jenkins Pipeline is typically written into a text file (called a Jenkinsfile ){: target=_blank}) that in turn is checked into a project\u2019s source control repository.","title":"Tekton (OpenShift pipelines)"},{"location":"reference/tools/jenkins/#pipelines","text":"Pipelines offer a set of stages or steps that can be chained together to allow a level of software automation. This automation can be tailored to the specific project requirements. You can read more information about Jenkins Pipelines here ){: target=_blank}","title":"Pipelines"},{"location":"reference/tools/jenkins/#stages","text":"Pipelines are defined in a Jenkinsfile that sits in the root of your application code. It defines a number of stages. Each of the templates ){: target=_blank} includes a Jenkinsfile that offers a number of stages. The stages have been configured to complete the build, test, package, and deploy of the application code. Each stage can use the defined defined secrets and config maps that were previously configured during the installation of Development cluster setup.","title":"Stages"},{"location":"reference/tools/jenkins/#developer-tools-pipeline","text":"To enable application compatibility between Kubernetes and OpenShift, the Jenkinsfile is consistent between pipeline registration with both platforms. Also, the Docker images are built from UBI images ){: target=_blank} so that their containers can run on both platforms. These are the stages in the pipeline and a description of what each stage does. The bold stage names indicate the stages that are required; the italics stage names indicate optional stages that can be deleted or will be ignored if the tool supporting the stage is not installed. These stages represent a typical production pipeline flow for a cloud-native application. Setup : Clones the code into the pipeline Build : Runs the build commands for the code Test : Validates the unit tests for the code Publish pacts : Publishes any pact contracts that have been defined Sonar scan : Runs a sonar code scan of the source code and publishes the results to SonarQube Verify environment : Validates the OpenShift or IKS environment configuration is valid Build image : Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env : Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check : Validates the Health Endpoint of the deployed application Package Helm Chart : Stores the tagged version of the Helm chart in Artifactory Trigger CD Pipeline : This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test","title":"Developer Tools Pipeline"},{"location":"reference/tools/jenkins/#registering-pipelines","text":"The templates ){: target=_blank} are a good place to start to see how Jenkinsfile and Dockerfile should be configured for use in a Jenkins CI pipeline. To register your git repo, use the IGC CLI ){: target=_blank}. This command automates a number of manual steps you would have to do with Jenkins, including: managing secrets, webhooks, and pipeline registration in the Jenkins tools. igc pipeline By default, the pipeline will register into the dev namespace and will copy all the configMaps and secrets from the tools namespace to the dev namespace. This means the pipeline can execute, knowing it has access to the key information that enables it to integrate with both the cloud platform and the various development tools. See Cluster Configuration ){: target=_blank} for more detailed information.","title":"Registering Pipelines"},{"location":"reference/tools/jenkins/#registering-pipeline-in-new-namespace","text":"You can use any namespace you want to register a pipeline. If you add -n or namespace to your igc pipeline command, it will create a new namespace if it doesn't already exist. It will copy the necessary secrets and configMaps into that namespace and configure the build agents pods to run in that namespace. igc pipeline -n team-one This is good if you have various squads, teams, pairs or students working in the same Development Tools environment.","title":"Registering Pipeline in new namespace"},{"location":"reference/tools/jenkins/#continuous-deployment","text":"In addition to continuous integration, the environment also supports continuous delivery using Artifactory and ArgoCD: Artifact Management with Artifactory Continuous Delivery with ArgoCD","title":"Continuous deployment"},{"location":"reference/tools/key-protect/","text":"Secret management with Key Protect \u00b6 Key Protect is a tool that provides centralized management of encryption keys and sensitive information. Key Protect manages two different types of keys: root keys and standard keys . Root keys are used to encrypt information in other systems, like the etcd database of the cluster, or data in Object Storage, or a MongoDB database. The details of which are the subject for a different article. Standard keys are used to store any kind of protected information. The Key Protect plugin reads the contents of a standard key, identified by a given key id, and stores the key value into a secret in the cluster. Getting the Key Protect instance id \u00b6 Set the target resource group and region for the Key Protect instance. ibmcloud target -g { RESOURCE_GROUP } -r { REGION } List the available resources and find the name of the Key Protect instance. ibmcloud resource service-instances List the details for the Key Protect instance. The Key Protect instance id is listed as GUID . ibmcloud resource service-instance { INSTANCE_NAME } Creating a standard key \u00b6 Open the IBM Cloud console and navigate to the Key Protect service Within Key Protect, select the Manage Keys tab Press the Add key button to open the \"Add a new key\" dialog Select the Import your own key radio button and Standard key from the drop down Provide a descriptive name for the key and paste the base-64 encoded value of the key into the Key material field Note: A value can be encoded as base-64 from the terminal with the following command: echo -n \"{VALUE}\" | base64 If you need to encode a larger value, create the value in a file and encode the entire contents of the file with: cat { file } | base64 Click Import key to create the key Copy the value of the ID","title":"IBM Key Protect"},{"location":"reference/tools/key-protect/#secret-management-with-key-protect","text":"Key Protect is a tool that provides centralized management of encryption keys and sensitive information. Key Protect manages two different types of keys: root keys and standard keys . Root keys are used to encrypt information in other systems, like the etcd database of the cluster, or data in Object Storage, or a MongoDB database. The details of which are the subject for a different article. Standard keys are used to store any kind of protected information. The Key Protect plugin reads the contents of a standard key, identified by a given key id, and stores the key value into a secret in the cluster.","title":"Secret management with Key Protect"},{"location":"reference/tools/key-protect/#getting-the-key-protect-instance-id","text":"Set the target resource group and region for the Key Protect instance. ibmcloud target -g { RESOURCE_GROUP } -r { REGION } List the available resources and find the name of the Key Protect instance. ibmcloud resource service-instances List the details for the Key Protect instance. The Key Protect instance id is listed as GUID . ibmcloud resource service-instance { INSTANCE_NAME }","title":"Getting the Key Protect instance id"},{"location":"reference/tools/key-protect/#creating-a-standard-key","text":"Open the IBM Cloud console and navigate to the Key Protect service Within Key Protect, select the Manage Keys tab Press the Add key button to open the \"Add a new key\" dialog Select the Import your own key radio button and Standard key from the drop down Provide a descriptive name for the key and paste the base-64 encoded value of the key into the Key material field Note: A value can be encoded as base-64 from the terminal with the following command: echo -n \"{VALUE}\" | base64 If you need to encode a larger value, create the value in a file and encode the entire contents of the file with: cat { file } | base64 Click Import key to create the key Copy the value of the ID","title":"Creating a standard key"},{"location":"reference/tools/pact/","text":"PACT \u00b6 Use Pact to test your code's API In IBM Garage Method, one of the Develop practices is contract-driven testing . Pact automates contract testing and enables it to be added to a continuous integration pipeline. The environment's CI pipeline ( Jenkins , Tekton , etc.) includes a Pact stage. Simply by building your app using the CI pipeline, your code's contract gets tested, just open the Pact UI to browse the results. What is contract testing \u00b6 Contract testing is a testing discipline that ensures two applications (a consumer and a provider) have a shared understanding of the interactions or the contract between them. The Pact framework has been selected for the provided tool set. Pact is a consumer-driven contract testing framework. More details can be found here - Pact overview . The framework has been built into the templates and a Pact Broker instance is provisioned in the cluster along with the other tools. In consumer-driven contract testing it is the consumer who defines the contract in terms of the expected interactions, the data structure, and the expected responses. That contract can then be used on the consumer-side to mock the interactions and validate the consumer behavior. More importantly, the contract can be shared with the provider of the interaction so that the provider's responses can be validated to ensure the consumer's expectations are met. In the Pact framework, the contract is called a pact . A pact consists of one or more interactions . Each interaction has the following structure: Given a *state* of {state} *upon receiving* a {description} request *with request* parameters - HTTP method - path - headers - body *will respond with* values like - status - headers - body where: {state} is an optional string that describes the initial state. This value can be used by the provider during testing to make sure the preconditions are met {description} is a unique description of the interaction the request parameters can contain any values that describe the interaction the response contains the relevant information for the consumer. The response values can be exact values or using matchers for type, regular expressions, etc Consumer \u00b6 Using the Pact framework libraries in conjunction with the unit testing framework on the consumer, the pact for the interaction between the consumer and provider is generated and validated. As part of the pact test setup, a Pact server is started and configured with the expected interactions. All of the consumer service invocations are directed at the Pact server which provides mock responses based on the interactions defined by the pact . At the end of the test, if all the interactions completed successfully a file containing the pact definition is generated. The following diagram gives an overview of the consumer interactions: An example pact test on a Typescript consumer using the jest testing framework is provided below. It has been broken into several parts. Pact server config \u00b6 At the beginning of the test file, the pact server is configured and started in the beforeAll() block. The afterAll() block finalizes the pacts by writing them out to a file and stopping the pact server. const port = 1234 ; const baseUrl = `http://localhost: ${ port } ` ; let pact : Pact ; beforeAll (() => { pact = new Pact ({ consumer : consumerName , provider : providerName , logLevel : 'error' , port , }); return pact . setup (); }); afterAll (() => { return pact . finalize () . catch ( err => console . error ( 'Error finalizing pact' , err )); }); Setup the service \u00b6 Next, an instance of the component that will be tested is loaded and configured with the pact server host and port as the base url used for the interactions. In this example, the consumer is using the typescript-ioc library to inject the baseUrl config value into the service. let service : SampleApi ; beforeAll (() => { Container . bind ( MyServiceConfig ) . provider ({ get : () => ({ baseUrl })}); service = Container . get ( SampleService ); }); Define and test the interaction \u00b6 For each interaction with the provider, a test similar to the one provided below is created. In it, the Pact framework is used to define the interaction. The addInteraction() publishes the interaction to the Pact server so that it can be used to provide a mock response when the request is made. The mock response is then used to validate the behavior of the component that is being tested. The example below is simple and passes the provider response directly through the service api but in more sophisticated examples the value would be transformed. describe ( 'given createWidget()' , () => { context ( 'when called with valid request' , () => { const widgetRequest = {...}; const widgetResponse = {...}; beforeEach (() => { return pact . addInteraction ({ uponReceiving : 'a valid widget request' , withRequest : { method : 'POST' , path : '/widgets' , headers : { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, body : widgetRequest , }, willRespondWith : { status : 200 , headers : { 'Content-Type' : Matchers . regex ({ generate : 'application/json' , matcher : 'application/json.*' }), }, body : Matchers.like ( widgetResponse ), }, }); }); test ( 'then return 200 status' , async () => { expect ( await service . createWidget ( widgetRequest )). toEqual ( widgetResponse ); }); }); }); Provider \u00b6 The provider uses the Pact framework to verify the running server against the pact , either one started locally as part of the test or another instance running elsewhere. The interactions in the pact are sent to the provider by a mock consumer in the Pact framework and the results are verified against the expected results defined by the pact. As an optional configuration for the verification process, an endpoint can be provided that handles the state information in the pact in order to ensure the preconditions for the test are met. (E.g. state=\"given an empty database\"). In order for these tests to be repeatable, it is often advisable to stand up a clean backend to run the pact tests when the tests start and tear it down when the tests are completed. For example, if a provider interacts with a Cloudant database point the test provider at a new database instance for the tests. The following diagram shows the interactions for the pact provider: Pact Broker \u00b6 One of the underpinning requirements of the pact verification process is the ability to make the pact files generated by the consumer available to the provider. When the pact verification is run in an automated pipeline this is difficult without an intermediary. Within the Pact framework, the Pact Broker provides the facility for consumers and providers to share the pact definitions with minimal dependencies between the systems. Additionally, the Pact Broker provides a place to define webhooks to trigger the provider build process when the pact definition changes and a way record and visualize the results of the verification process. The high-level interaction is shown below: templates \u00b6 The templates have been built with the frameworks necessary to generate and publish pacts for api consumers and verify against pacts and publish the results for api providers. The pipelines will do all the publishing and verification against Pact Broker if an instance of Pact Broker has been configured within the cluster. You can review you pact contracts using the Pact Dashboard.app Use the Developer Dashboard to open the Pact dashboard","title":"PACT"},{"location":"reference/tools/pact/#pact","text":"Use Pact to test your code's API In IBM Garage Method, one of the Develop practices is contract-driven testing . Pact automates contract testing and enables it to be added to a continuous integration pipeline. The environment's CI pipeline ( Jenkins , Tekton , etc.) includes a Pact stage. Simply by building your app using the CI pipeline, your code's contract gets tested, just open the Pact UI to browse the results.","title":"PACT"},{"location":"reference/tools/pact/#what-is-contract-testing","text":"Contract testing is a testing discipline that ensures two applications (a consumer and a provider) have a shared understanding of the interactions or the contract between them. The Pact framework has been selected for the provided tool set. Pact is a consumer-driven contract testing framework. More details can be found here - Pact overview . The framework has been built into the templates and a Pact Broker instance is provisioned in the cluster along with the other tools. In consumer-driven contract testing it is the consumer who defines the contract in terms of the expected interactions, the data structure, and the expected responses. That contract can then be used on the consumer-side to mock the interactions and validate the consumer behavior. More importantly, the contract can be shared with the provider of the interaction so that the provider's responses can be validated to ensure the consumer's expectations are met. In the Pact framework, the contract is called a pact . A pact consists of one or more interactions . Each interaction has the following structure: Given a *state* of {state} *upon receiving* a {description} request *with request* parameters - HTTP method - path - headers - body *will respond with* values like - status - headers - body where: {state} is an optional string that describes the initial state. This value can be used by the provider during testing to make sure the preconditions are met {description} is a unique description of the interaction the request parameters can contain any values that describe the interaction the response contains the relevant information for the consumer. The response values can be exact values or using matchers for type, regular expressions, etc","title":"What is contract testing"},{"location":"reference/tools/pact/#consumer","text":"Using the Pact framework libraries in conjunction with the unit testing framework on the consumer, the pact for the interaction between the consumer and provider is generated and validated. As part of the pact test setup, a Pact server is started and configured with the expected interactions. All of the consumer service invocations are directed at the Pact server which provides mock responses based on the interactions defined by the pact . At the end of the test, if all the interactions completed successfully a file containing the pact definition is generated. The following diagram gives an overview of the consumer interactions: An example pact test on a Typescript consumer using the jest testing framework is provided below. It has been broken into several parts.","title":"Consumer"},{"location":"reference/tools/pact/#pact-server-config","text":"At the beginning of the test file, the pact server is configured and started in the beforeAll() block. The afterAll() block finalizes the pacts by writing them out to a file and stopping the pact server. const port = 1234 ; const baseUrl = `http://localhost: ${ port } ` ; let pact : Pact ; beforeAll (() => { pact = new Pact ({ consumer : consumerName , provider : providerName , logLevel : 'error' , port , }); return pact . setup (); }); afterAll (() => { return pact . finalize () . catch ( err => console . error ( 'Error finalizing pact' , err )); });","title":"Pact server config"},{"location":"reference/tools/pact/#setup-the-service","text":"Next, an instance of the component that will be tested is loaded and configured with the pact server host and port as the base url used for the interactions. In this example, the consumer is using the typescript-ioc library to inject the baseUrl config value into the service. let service : SampleApi ; beforeAll (() => { Container . bind ( MyServiceConfig ) . provider ({ get : () => ({ baseUrl })}); service = Container . get ( SampleService ); });","title":"Setup the service"},{"location":"reference/tools/pact/#define-and-test-the-interaction","text":"For each interaction with the provider, a test similar to the one provided below is created. In it, the Pact framework is used to define the interaction. The addInteraction() publishes the interaction to the Pact server so that it can be used to provide a mock response when the request is made. The mock response is then used to validate the behavior of the component that is being tested. The example below is simple and passes the provider response directly through the service api but in more sophisticated examples the value would be transformed. describe ( 'given createWidget()' , () => { context ( 'when called with valid request' , () => { const widgetRequest = {...}; const widgetResponse = {...}; beforeEach (() => { return pact . addInteraction ({ uponReceiving : 'a valid widget request' , withRequest : { method : 'POST' , path : '/widgets' , headers : { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' , }, body : widgetRequest , }, willRespondWith : { status : 200 , headers : { 'Content-Type' : Matchers . regex ({ generate : 'application/json' , matcher : 'application/json.*' }), }, body : Matchers.like ( widgetResponse ), }, }); }); test ( 'then return 200 status' , async () => { expect ( await service . createWidget ( widgetRequest )). toEqual ( widgetResponse ); }); }); });","title":"Define and test the interaction"},{"location":"reference/tools/pact/#provider","text":"The provider uses the Pact framework to verify the running server against the pact , either one started locally as part of the test or another instance running elsewhere. The interactions in the pact are sent to the provider by a mock consumer in the Pact framework and the results are verified against the expected results defined by the pact. As an optional configuration for the verification process, an endpoint can be provided that handles the state information in the pact in order to ensure the preconditions for the test are met. (E.g. state=\"given an empty database\"). In order for these tests to be repeatable, it is often advisable to stand up a clean backend to run the pact tests when the tests start and tear it down when the tests are completed. For example, if a provider interacts with a Cloudant database point the test provider at a new database instance for the tests. The following diagram shows the interactions for the pact provider:","title":"Provider"},{"location":"reference/tools/pact/#pact-broker","text":"One of the underpinning requirements of the pact verification process is the ability to make the pact files generated by the consumer available to the provider. When the pact verification is run in an automated pipeline this is difficult without an intermediary. Within the Pact framework, the Pact Broker provides the facility for consumers and providers to share the pact definitions with minimal dependencies between the systems. Additionally, the Pact Broker provides a place to define webhooks to trigger the provider build process when the pact definition changes and a way record and visualize the results of the verification process. The high-level interaction is shown below:","title":"Pact Broker"},{"location":"reference/tools/pact/#templates","text":"The templates have been built with the frameworks necessary to generate and publish pacts for api consumers and verify against pacts and publish the results for api providers. The pipelines will do all the publishing and verification against Pact Broker if an instance of Pact Broker has been configured within the cluster. You can review you pact contracts using the Pact Dashboard.app Use the Developer Dashboard to open the Pact dashboard","title":"templates"},{"location":"reference/tools/sonar-qube/","text":"SonarQube \u00b6 Use SonarQube to analyze your code's quality SonarQube performs static code analysis to evaluate code quality, using analysis rules that focus on three areas: Code Reliability : Detect bugs that will impact end-user functionality Application Security : Detect vulnerabilities and hot spots that can be exploited to compromise the program Technical Debt : Keep you codebase maintainable to increase developer velocity SonarQube plugs into the application lifecycle management (ALM) process to make continuous inspection part of continuous integration. Adding code analysis to ALM provides regular, timely feedback on the quality of the code being produced. The goal is to detect problems as soon as possible so that they can be resolved before they can impact production end users. The continuous integration (CI) server integrates SonarQube into the ALM.The SonarQube solution consists of several components: The central component is the SonarQube Server, which runs the SonarScanner, processes the resulting analysis reports, stores the reports in SonarQube Database, and displays the reports in the SonarQube UI. A CI server uses a stage/goal/task in its build automation to trigger the language-specific SonarScanner to scan the code being built. Developers can view the resulting analysis report in the SonarQube UI. Code Analysis in the Pipeline \u00b6 In the CI pipeline, the Sonar scan stage triggers the SonarScanner in SonarQube. Follow these directions to see code analysis in action: Deploy the template named Spring Boot Microservice. Follow the directions in Deploying an App Deploy the Spring Boot Microservice template Name the new repo something like sonar-java Be sure to run the CI pipeline for your project, and confirm that it runs the Sonar scan stage Examine SonarQube's analysis report for your app. Use the Developer Dashboard to open the SonarQube dashboard Go to the Projects page You should see your project in the list, such as bwoolf1.sonar-java . The project summary shows several characteristics measured in the app: The quality gate passed Several issues were found, categorized by type 2 bugs for a C rating 0 vulnerabilities for an A rating 17 code smells but an A rating None of the code was tested None of it is duplicate code It scanned 1.5k lines of code written in Java, a small program (XS, S, M, L, XL) In the Projects list, click on the project name (such as bwoolf1.sonar-java ) to open your project The project overview shows more detail about how many issues were found in the app Reliability: 2 bugs for a C rating Security: 1 security hot spot but an A rating Maintainability: 17 code smells, 2 hrs of technical debt but an A rating Coverage: 7 unit tests Duplications: 0 duplicated blocks Examine the issues \u00b6 Use the SonarQube dashboard to explore the issues that it found in your project. In the Reliability pane of the project's Overview page, click on the \"2\" to open the Issues page The Issues page, filtered for bugs, shows two issues. Both concern \"synchronized\" methods. In the Issues list, click on either issue to see where the issues appeared in the code The Issues detail shows the source code file for the Java class. The issue descriptions are embedded after the mark and reset method signatures. In either issue, press the See Rule button. SonarQube displays the details of its \"Overrides should match their parent class methods in synchronization\" rule. Now you need to investigate to figure out why the code violates this rule. Want to see if you can track down the problem before seeing the solution? What to fix is pretty obvious--the Rule explains what to do--but tracking down why takes some effort. Here's the solution: The error is not in the file's parent class, ResettableHttpServletRequest , but in its embedded class, SimpleServletInputStream , which extends javax.servlet.ServletInputStream . The Javadocs for ServletInputStream show that it extends java.io.InputStream . The original Java 1.0 Javadocs show that these methods in InputStream are indeed synchronized: public synchronized void mark ( int readlimit ) . . . public synchronized void reset () throws IOException More recent Javadocs haven't shown these signatures for years, but the compiler says the class is still defined that way. There's some debate about whether mark and reset really need to be synchronized . But SonarQube doesn't judge, it just reports: Since the superclass defined the method signatures as synchronized, SonarQube is warning that the subclass is supposed to do so as well. Examine the other issues \u00b6 Besides the bugs, SonarQube also found issues that are host spots and code smells. In the SonarQube dashboard, go back to the Issues page Click on the \"1\" above Security hot spots The issue warns to \"Make sure that command line arguments are used safely here.\" SonarQube considers any class that has a public static void main(String[] args) method to be a potential vulnerability. As the rule explains, \"Command line arguments can be dangerous just like any other user input. They should never be used without being first validated and sanitized.\" This method passes them through unchecked, which is risky. Back in the Issues page, click on \"17\" code smells SonarQube found issues such as: - Remove this unused import 'java.lang.System.lineSeparator'. - Move constants to a class or enum. - This block of commented-out lines of code should be removed. None of these break your app's functionality, but they do make the code more difficult to maintain. Give it a try \u00b6 As we saw earlier, SonarQube found two bugs in our Java app. Let's do something about that. Add a quality gate to SonarQube \u00b6 The first problem is that the quality gate says that the app passed. The default quality gate is OK with those two bugs, but we're not. Let's create a new quality gate that checks for bugs. Use the Developer Dashboard to open the SonarQube dashboard To create and install a new quality gate, first log in to SonarQube Go to the Quality Gates page Make a copy of the default gate named Sonar way , give it a name like better gate {your initials} , i.e. better gate bw Add a condition, Bugs is greater than 0 Add your project to this gate Run the pipeline again to scan the code again. Back in the OpenShift console, on the Application Console > Builds > Pipelines page, press Start Pipeline After the Sonar Scan stage completes, go back to the SonarQube dashboard and take a look at your project Good news, the quality gate is working and SonarQube fails the project now! Add a stage to Jenkins \u00b6 Error Do not complete this step for the homework task, when working on the the shared development environment, the SonarQube Plugin is not correctly configured for this environment. However, take a look at your pipeline (in the OpenShift console). It kept right on going! We don't want the pipeline to keep going and deploy the app; if the code fails the quality gate, we want the pipeline to stop. Modify the Jenkins pipeline to add a Quality Gate stage. In the local Git repo that contains your source code (e.g. sonar-app-bw ), edit the file named Jenkinsfile After the stage called Sonar Scan, insert this stage: stage(\"Quality Gate\") { timeout(time: 1, unit: 'HOURS') { // Parameter indicates whether to set pipeline to UNSTABLE if Quality Gate fails // true = set pipeline to UNSTABLE, false = don't waitForQualityGate abortPipeline: true } Push the change to the server repo, which runs the pipeline again, and the Quality Gate stage fails Good. Now, when SonarQube finds bugs in our app, the quality gate fails and the Jenkins stage fails. Fix the code \u00b6 Now that our pipeline detects problems in our code, let's fix those problems. As discussed before, the bugs are that two methods need to be marked as synchronized. Let's fix the code. Edit the class com.ibm.cloud_garage.logging.inbound.ResettableHttpServletRequest Edit the methods mark and reset to make them both synchronized public synchronized void mark ( int i ) { . . . public synchronized void reset () throws IOException { Push the change to the server repo, which runs the pipeline again, and this time the Quality Gate stage passes Check the project in SonarQube and see that it now has 0 bugs and has now passed Extra credit : The code still has 17 code smells. Go fix those! Conclusion \u00b6 It's a good idea to incorporate code analysis as part of your application development lifecycle, so you can use its findings to help enforce and improve your code quality. Here, the environment uses SonarQube, but you never had to run SonarQube. Just run the environment's build pipeline on your app and it gets scanned automatically. After running the pipeline, open the SonarQube UI and browse the findings in your app's project to figure out what code you ought to fix.","title":"SonarQube"},{"location":"reference/tools/sonar-qube/#sonarqube","text":"Use SonarQube to analyze your code's quality SonarQube performs static code analysis to evaluate code quality, using analysis rules that focus on three areas: Code Reliability : Detect bugs that will impact end-user functionality Application Security : Detect vulnerabilities and hot spots that can be exploited to compromise the program Technical Debt : Keep you codebase maintainable to increase developer velocity SonarQube plugs into the application lifecycle management (ALM) process to make continuous inspection part of continuous integration. Adding code analysis to ALM provides regular, timely feedback on the quality of the code being produced. The goal is to detect problems as soon as possible so that they can be resolved before they can impact production end users. The continuous integration (CI) server integrates SonarQube into the ALM.The SonarQube solution consists of several components: The central component is the SonarQube Server, which runs the SonarScanner, processes the resulting analysis reports, stores the reports in SonarQube Database, and displays the reports in the SonarQube UI. A CI server uses a stage/goal/task in its build automation to trigger the language-specific SonarScanner to scan the code being built. Developers can view the resulting analysis report in the SonarQube UI.","title":"SonarQube"},{"location":"reference/tools/sonar-qube/#code-analysis-in-the-pipeline","text":"In the CI pipeline, the Sonar scan stage triggers the SonarScanner in SonarQube. Follow these directions to see code analysis in action: Deploy the template named Spring Boot Microservice. Follow the directions in Deploying an App Deploy the Spring Boot Microservice template Name the new repo something like sonar-java Be sure to run the CI pipeline for your project, and confirm that it runs the Sonar scan stage Examine SonarQube's analysis report for your app. Use the Developer Dashboard to open the SonarQube dashboard Go to the Projects page You should see your project in the list, such as bwoolf1.sonar-java . The project summary shows several characteristics measured in the app: The quality gate passed Several issues were found, categorized by type 2 bugs for a C rating 0 vulnerabilities for an A rating 17 code smells but an A rating None of the code was tested None of it is duplicate code It scanned 1.5k lines of code written in Java, a small program (XS, S, M, L, XL) In the Projects list, click on the project name (such as bwoolf1.sonar-java ) to open your project The project overview shows more detail about how many issues were found in the app Reliability: 2 bugs for a C rating Security: 1 security hot spot but an A rating Maintainability: 17 code smells, 2 hrs of technical debt but an A rating Coverage: 7 unit tests Duplications: 0 duplicated blocks","title":"Code Analysis in the Pipeline"},{"location":"reference/tools/sonar-qube/#examine-the-issues","text":"Use the SonarQube dashboard to explore the issues that it found in your project. In the Reliability pane of the project's Overview page, click on the \"2\" to open the Issues page The Issues page, filtered for bugs, shows two issues. Both concern \"synchronized\" methods. In the Issues list, click on either issue to see where the issues appeared in the code The Issues detail shows the source code file for the Java class. The issue descriptions are embedded after the mark and reset method signatures. In either issue, press the See Rule button. SonarQube displays the details of its \"Overrides should match their parent class methods in synchronization\" rule. Now you need to investigate to figure out why the code violates this rule. Want to see if you can track down the problem before seeing the solution? What to fix is pretty obvious--the Rule explains what to do--but tracking down why takes some effort. Here's the solution: The error is not in the file's parent class, ResettableHttpServletRequest , but in its embedded class, SimpleServletInputStream , which extends javax.servlet.ServletInputStream . The Javadocs for ServletInputStream show that it extends java.io.InputStream . The original Java 1.0 Javadocs show that these methods in InputStream are indeed synchronized: public synchronized void mark ( int readlimit ) . . . public synchronized void reset () throws IOException More recent Javadocs haven't shown these signatures for years, but the compiler says the class is still defined that way. There's some debate about whether mark and reset really need to be synchronized . But SonarQube doesn't judge, it just reports: Since the superclass defined the method signatures as synchronized, SonarQube is warning that the subclass is supposed to do so as well.","title":"Examine the issues"},{"location":"reference/tools/sonar-qube/#examine-the-other-issues","text":"Besides the bugs, SonarQube also found issues that are host spots and code smells. In the SonarQube dashboard, go back to the Issues page Click on the \"1\" above Security hot spots The issue warns to \"Make sure that command line arguments are used safely here.\" SonarQube considers any class that has a public static void main(String[] args) method to be a potential vulnerability. As the rule explains, \"Command line arguments can be dangerous just like any other user input. They should never be used without being first validated and sanitized.\" This method passes them through unchecked, which is risky. Back in the Issues page, click on \"17\" code smells SonarQube found issues such as: - Remove this unused import 'java.lang.System.lineSeparator'. - Move constants to a class or enum. - This block of commented-out lines of code should be removed. None of these break your app's functionality, but they do make the code more difficult to maintain.","title":"Examine the other issues"},{"location":"reference/tools/sonar-qube/#give-it-a-try","text":"As we saw earlier, SonarQube found two bugs in our Java app. Let's do something about that.","title":"Give it a try"},{"location":"reference/tools/sonar-qube/#add-a-quality-gate-to-sonarqube","text":"The first problem is that the quality gate says that the app passed. The default quality gate is OK with those two bugs, but we're not. Let's create a new quality gate that checks for bugs. Use the Developer Dashboard to open the SonarQube dashboard To create and install a new quality gate, first log in to SonarQube Go to the Quality Gates page Make a copy of the default gate named Sonar way , give it a name like better gate {your initials} , i.e. better gate bw Add a condition, Bugs is greater than 0 Add your project to this gate Run the pipeline again to scan the code again. Back in the OpenShift console, on the Application Console > Builds > Pipelines page, press Start Pipeline After the Sonar Scan stage completes, go back to the SonarQube dashboard and take a look at your project Good news, the quality gate is working and SonarQube fails the project now!","title":"Add a quality gate to SonarQube"},{"location":"reference/tools/sonar-qube/#add-a-stage-to-jenkins","text":"Error Do not complete this step for the homework task, when working on the the shared development environment, the SonarQube Plugin is not correctly configured for this environment. However, take a look at your pipeline (in the OpenShift console). It kept right on going! We don't want the pipeline to keep going and deploy the app; if the code fails the quality gate, we want the pipeline to stop. Modify the Jenkins pipeline to add a Quality Gate stage. In the local Git repo that contains your source code (e.g. sonar-app-bw ), edit the file named Jenkinsfile After the stage called Sonar Scan, insert this stage: stage(\"Quality Gate\") { timeout(time: 1, unit: 'HOURS') { // Parameter indicates whether to set pipeline to UNSTABLE if Quality Gate fails // true = set pipeline to UNSTABLE, false = don't waitForQualityGate abortPipeline: true } Push the change to the server repo, which runs the pipeline again, and the Quality Gate stage fails Good. Now, when SonarQube finds bugs in our app, the quality gate fails and the Jenkins stage fails.","title":"Add a stage to Jenkins"},{"location":"reference/tools/sonar-qube/#fix-the-code","text":"Now that our pipeline detects problems in our code, let's fix those problems. As discussed before, the bugs are that two methods need to be marked as synchronized. Let's fix the code. Edit the class com.ibm.cloud_garage.logging.inbound.ResettableHttpServletRequest Edit the methods mark and reset to make them both synchronized public synchronized void mark ( int i ) { . . . public synchronized void reset () throws IOException { Push the change to the server repo, which runs the pipeline again, and this time the Quality Gate stage passes Check the project in SonarQube and see that it now has 0 bugs and has now passed Extra credit : The code still has 17 code smells. Go fix those!","title":"Fix the code"},{"location":"reference/tools/sonar-qube/#conclusion","text":"It's a good idea to incorporate code analysis as part of your application development lifecycle, so you can use its findings to help enforce and improve your code quality. Here, the environment uses SonarQube, but you never had to run SonarQube. Just run the environment's build pipeline on your app and it gets scanned automatically. After running the pipeline, open the SonarQube UI and browse the findings in your app's project to figure out what code you ought to fix.","title":"Conclusion"},{"location":"reference/tools/tekton/","text":"Tekton \u00b6 Use Tekton to automate your continuous integration process Overview \u00b6 Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. Tekton is a new emerging CI tool that has been built to support Kubernetes and OpenShift from the ground up. What is Tekton \u00b6 Tekton is a powerful yet flexible Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems. It lets you build, test, and deploy across multiple cloud providers or on-premises systems by abstracting away the underlying implementation details. Tekton 101 \u00b6 Tekton provides open-source components to help you standardize your CI/CD tooling and processes across vendors, languages, and deployment environments. Industry specifications around pipelines, releases, workflows, and other CI/CD components available with Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, and Knative, among others. For more information read up about it Tekton Tutorial For more information read up about it App Build Tutorial with Tekton The IBM Cloud is standardizing on using Tekton in both IBM Cloud DevOps service and IBM Cloud Pak for Applications. OpenShift 4.2 will also natively support it. This guide will focus on using Tekton when the Development tools have been installed in Redhat OpenShift, IBM Kubernetes Managed services and Red Hat Code Ready Containers to give you choice for you Continuous Integration cloud native development tool set. Note This guide will help you set up the templates with Tekton and requires that you have installed Tekton with Red Hat Code Ready Containers or have installed open source Tekton into the The IBM Kubernetes Cluster. Common App Tasks \u00b6 The following gives a description of each Task that is commonly used in a Pipeline . The Optional stages can be deleted or ignored if the tool support it is not installed. These stages represent a typical production pipeline flow for a Cloud Native application. Setup clones the code into the pipeline Build runs the build commands for the code Test validates the unit tests for the code Publish pacts ( optional ) publishes any pact contracts that have been defined Verify pact ( optional ) verifies the pact contracts Sonar scan ( optional ) runs a sonar code scan of the source code and publishes the results to SonarQube Build image Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check Validates the Health Endpoint of the deployed application Package Helm Chart ( optional ) Stores the tagged version of the Helm chart into Artifactory Trigger CD Pipeline ( optional ) This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test namespace Install Tekton \u00b6 Tekton can be installed in both RedHat Openshift and IBM Kubernetes manage service and RedHat Code Ready Containers. To install the necessary components follow the steps below. Install Cloud-Native Toolkit CLI on your managed OpenShift,CRC or IKS development cluster on the IBM Cloud. This will help configure the tools and secrets and configMap to make working with IBM Cloud so much easier. OpenShift 4.x Install on OpenShift 4.x \u00b6 If you have installed the IBM Garage for Cloud Developer Tools into your cluster this will automatically install the operator for you. Install Tekton on OpenShift 4 including CodeReady Containers (CRC) Install via operator hub Administrator perspective view, click Operator Hub search for OpenShift Pipelines and install operator Kubernetes Install Tekton on IBM Kubernetes Managed Service \u00b6 Install Tekton from Open Source Tekton Pipelines Install Tekton Dashboard follow the instructions in the README.md Add Ingress endpoint for the Tekton Dashboard add a host name that uses the IKS cluster subdomain apiVersion : extensions/v1beta1 kind : Ingress metadata : name : tekton-dashboard namespace : tekton-pipelines spec : rules : - host : tekton-dashboard.showcase-dev-iks-cluster.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : tekton-dashboard servicePort : 9097 Install Tekton Triggers Setup Tekton \u00b6 Install Tekton pipelines and tasks into the dev namespace following the instructions in the repository ibm-garage-tekton-tasks Install the Tasks kubectl create -f ibm-garage-tekton-tasks/tasks/ -n dev Install the Pipelines kubectl create -f ibm-garage-tekton-tasks/pipelines/ -n dev Configure namespace for development \u00b6 Install the Tekton CLI tkn Create a new namespace (ie dev-{initials} ) and copy all config and secrets igc namespace -n { new-namespace } Set your new-namespace the current namespace context oc project { new-namespace } The template Pipelines provided support for Java or Node.js based apps. You can configure your own custom Tasks for other runtimes. There are a number of default Tasks to get you started they are detailed above. To create an application use one of the provided templates these templates work seamlessly with the Tasks and Pipelines provided. Register the App with Tekton \u00b6 With Tekton enabled and your default Tasks and Pipelines installed into the dev namespace. You can now configure your applications to be built, packaged, tested and deployed to your OpenShift or Kubernetes development cluster. Connect to the pipeline. (See the IGC CLI for details about how the pipeline command works.) igc pipeline -n dev- { initials } --tekton Verify the pipeline \u00b6 To validate your pipeline have been correctly configured, and has triggered a PipelineRun use the following Tekton dashboards or tkn CLI to validate it ran correctly without errors. OpenShift 4.x Review you Pipeline in the OpenShift 4.x Console Review your Tasks Review your Steps Open source Tekton Dashboard If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline Tekton CLI If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline tkn pipelinerun list Review Pipeline details tkn pipelinerun describe { pipeline-name } Running Application \u00b6 Once the Tekton pipeline has successfully completed you can validate your app has been successfully deployed. Open the OpenShift Console and select the {new-namespace} project and click on Workloads Get the hostname for the application from ingress kubectl get ingress --all-namespace You can use the the igc command to get the name of the deployed application igc ingress -n { new-namespace } Use the application URL to open it your browser for testing Once you become familiar with deploying code into OpenShift using Tekton , read up about how you can manage code deployment with Continuous Delivery with ArgoCD and Artifactory Artifact Management with Artifactory Continuous Delivery with ArgoCD","title":"OpenShift Pipelines/Tekton"},{"location":"reference/tools/tekton/#tekton","text":"Use Tekton to automate your continuous integration process","title":"Tekton"},{"location":"reference/tools/tekton/#overview","text":"Continuous integration is a software development technique where software is built regularly by a team in an automated fashion. Tekton is a new emerging CI tool that has been built to support Kubernetes and OpenShift from the ground up.","title":"Overview"},{"location":"reference/tools/tekton/#what-is-tekton","text":"Tekton is a powerful yet flexible Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems. It lets you build, test, and deploy across multiple cloud providers or on-premises systems by abstracting away the underlying implementation details.","title":"What is Tekton"},{"location":"reference/tools/tekton/#tekton-101","text":"Tekton provides open-source components to help you standardize your CI/CD tooling and processes across vendors, languages, and deployment environments. Industry specifications around pipelines, releases, workflows, and other CI/CD components available with Tekton will work well with existing CI/CD tools such as Jenkins, Jenkins X, Skaffold, and Knative, among others. For more information read up about it Tekton Tutorial For more information read up about it App Build Tutorial with Tekton The IBM Cloud is standardizing on using Tekton in both IBM Cloud DevOps service and IBM Cloud Pak for Applications. OpenShift 4.2 will also natively support it. This guide will focus on using Tekton when the Development tools have been installed in Redhat OpenShift, IBM Kubernetes Managed services and Red Hat Code Ready Containers to give you choice for you Continuous Integration cloud native development tool set. Note This guide will help you set up the templates with Tekton and requires that you have installed Tekton with Red Hat Code Ready Containers or have installed open source Tekton into the The IBM Kubernetes Cluster.","title":"Tekton 101"},{"location":"reference/tools/tekton/#common-app-tasks","text":"The following gives a description of each Task that is commonly used in a Pipeline . The Optional stages can be deleted or ignored if the tool support it is not installed. These stages represent a typical production pipeline flow for a Cloud Native application. Setup clones the code into the pipeline Build runs the build commands for the code Test validates the unit tests for the code Publish pacts ( optional ) publishes any pact contracts that have been defined Verify pact ( optional ) verifies the pact contracts Sonar scan ( optional ) runs a sonar code scan of the source code and publishes the results to SonarQube Build image Builds the code into a Docker images and stores it in the IBM Cloud Image registry Deploy to DEV env Deploys the Docker image tagged version to dev namespace using Helm Chart Health Check Validates the Health Endpoint of the deployed application Package Helm Chart ( optional ) Stores the tagged version of the Helm chart into Artifactory Trigger CD Pipeline ( optional ) This is a GitOps stage that will update the build number in designated git repo and trigger ArgoCD for deployment to test namespace","title":"Common App Tasks"},{"location":"reference/tools/tekton/#install-tekton","text":"Tekton can be installed in both RedHat Openshift and IBM Kubernetes manage service and RedHat Code Ready Containers. To install the necessary components follow the steps below. Install Cloud-Native Toolkit CLI on your managed OpenShift,CRC or IKS development cluster on the IBM Cloud. This will help configure the tools and secrets and configMap to make working with IBM Cloud so much easier. OpenShift 4.x","title":"Install Tekton"},{"location":"reference/tools/tekton/#install-on-openshift-4x","text":"If you have installed the IBM Garage for Cloud Developer Tools into your cluster this will automatically install the operator for you. Install Tekton on OpenShift 4 including CodeReady Containers (CRC) Install via operator hub Administrator perspective view, click Operator Hub search for OpenShift Pipelines and install operator Kubernetes","title":"Install on OpenShift 4.x"},{"location":"reference/tools/tekton/#install-tekton-on-ibm-kubernetes-managed-service","text":"Install Tekton from Open Source Tekton Pipelines Install Tekton Dashboard follow the instructions in the README.md Add Ingress endpoint for the Tekton Dashboard add a host name that uses the IKS cluster subdomain apiVersion : extensions/v1beta1 kind : Ingress metadata : name : tekton-dashboard namespace : tekton-pipelines spec : rules : - host : tekton-dashboard.showcase-dev-iks-cluster.us-south.containers.appdomain.cloud http : paths : - backend : serviceName : tekton-dashboard servicePort : 9097 Install Tekton Triggers","title":"Install Tekton on IBM Kubernetes Managed Service"},{"location":"reference/tools/tekton/#setup-tekton","text":"Install Tekton pipelines and tasks into the dev namespace following the instructions in the repository ibm-garage-tekton-tasks Install the Tasks kubectl create -f ibm-garage-tekton-tasks/tasks/ -n dev Install the Pipelines kubectl create -f ibm-garage-tekton-tasks/pipelines/ -n dev","title":"Setup Tekton"},{"location":"reference/tools/tekton/#configure-namespace-for-development","text":"Install the Tekton CLI tkn Create a new namespace (ie dev-{initials} ) and copy all config and secrets igc namespace -n { new-namespace } Set your new-namespace the current namespace context oc project { new-namespace } The template Pipelines provided support for Java or Node.js based apps. You can configure your own custom Tasks for other runtimes. There are a number of default Tasks to get you started they are detailed above. To create an application use one of the provided templates these templates work seamlessly with the Tasks and Pipelines provided.","title":"Configure namespace for development"},{"location":"reference/tools/tekton/#register-the-app-with-tekton","text":"With Tekton enabled and your default Tasks and Pipelines installed into the dev namespace. You can now configure your applications to be built, packaged, tested and deployed to your OpenShift or Kubernetes development cluster. Connect to the pipeline. (See the IGC CLI for details about how the pipeline command works.) igc pipeline -n dev- { initials } --tekton","title":"Register the App with Tekton"},{"location":"reference/tools/tekton/#verify-the-pipeline","text":"To validate your pipeline have been correctly configured, and has triggered a PipelineRun use the following Tekton dashboards or tkn CLI to validate it ran correctly without errors. OpenShift 4.x Review you Pipeline in the OpenShift 4.x Console Review your Tasks Review your Steps Open source Tekton Dashboard If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline Tekton CLI If you are running Tekton with IBM Cloud Pak for Applications or Knative with Kubernetes managed service your dashboard view will look similar to below. Review your Pipeline tkn pipelinerun list Review Pipeline details tkn pipelinerun describe { pipeline-name }","title":"Verify the pipeline"},{"location":"reference/tools/tekton/#running-application","text":"Once the Tekton pipeline has successfully completed you can validate your app has been successfully deployed. Open the OpenShift Console and select the {new-namespace} project and click on Workloads Get the hostname for the application from ingress kubectl get ingress --all-namespace You can use the the igc command to get the name of the deployed application igc ingress -n { new-namespace } Use the application URL to open it your browser for testing Once you become familiar with deploying code into OpenShift using Tekton , read up about how you can manage code deployment with Continuous Delivery with ArgoCD and Artifactory Artifact Management with Artifactory Continuous Delivery with ArgoCD","title":"Running Application"},{"location":"reference/tools/tools/","text":"Tools used by the Toolkit \u00b6 This section provides details of the set of 3rd party tools used in a default install of the Toolkit. Tool Description ArgoCD Continuous Deliver tool Artifactory Artifact repository IBM Container Registry ( IBM Cloud only ) Container registry IBM Key Protect ( IBM Cloud only ) Secrets manager Jenkins Pipeline tool, now replaced by Tekton as the default pipeline tool OpenShift Pipelines (Tekton) Cloud-Native pipeline tool PACT Contract testing tool SonarQube Static analysis tool covering many programming languages","title":"Toolkit components"},{"location":"reference/tools/tools/#tools-used-by-the-toolkit","text":"This section provides details of the set of 3rd party tools used in a default install of the Toolkit. Tool Description ArgoCD Continuous Deliver tool Artifactory Artifact repository IBM Container Registry ( IBM Cloud only ) Container registry IBM Key Protect ( IBM Cloud only ) Secrets manager Jenkins Pipeline tool, now replaced by Tekton as the default pipeline tool OpenShift Pipelines (Tekton) Cloud-Native pipeline tool PACT Contract testing tool SonarQube Static analysis tool covering many programming languages","title":"Tools used by the Toolkit"},{"location":"resources/resources/","text":"Additional Resources \u00b6 Use the links below to find additional resources and learning content around Cloud-Native development, RedHat OpenShift and the IBM Cloud. Alternate Cloud-Native Toolkit learning options \u00b6 Workshop : Assets to run a workshop with local git server Additional Cloud-Native and OpenShift learning \u00b6 Cloud Native Bootcamp : https://cloudnative101.dev Learning Journey : https://ibm-gsi-ecosystem.github.io/ibm-gsi-cloudnative-journey/ IBM Open Labs - Red Hat OpenShift on IBM Cloud : https://developer.ibm.com/openlabs/openshift IBM Garage Methodology : https://www.ibm.com/garage/method/ Useful IBM Cloud resources \u00b6 IBM Cloud roles access control cluster fast-switching tool cloud shell Additional resources \u00b6 Cloud-Native Toolkit YouTube Channel Linking to this site \u00b6 As the site is under continuous revisions, you may want to provide links that are guaranteed to be available. The following links will always be maintained: Site home : https://cloudnativetoolkit.dev Install : https://cloudnativetoolkit.dev/install Developer setup: https://cloudnativetoolkit.dev/setup Learn : https://cloudnativetoolkit.dev/learn workshop : https://cloudnativetoolkit.dev/workshop","title":"Additional resources"},{"location":"resources/resources/#additional-resources","text":"Use the links below to find additional resources and learning content around Cloud-Native development, RedHat OpenShift and the IBM Cloud.","title":"Additional Resources"},{"location":"resources/resources/#alternate-cloud-native-toolkit-learning-options","text":"Workshop : Assets to run a workshop with local git server","title":"Alternate Cloud-Native Toolkit learning options"},{"location":"resources/resources/#additional-cloud-native-and-openshift-learning","text":"Cloud Native Bootcamp : https://cloudnative101.dev Learning Journey : https://ibm-gsi-ecosystem.github.io/ibm-gsi-cloudnative-journey/ IBM Open Labs - Red Hat OpenShift on IBM Cloud : https://developer.ibm.com/openlabs/openshift IBM Garage Methodology : https://www.ibm.com/garage/method/","title":"Additional Cloud-Native and OpenShift learning"},{"location":"resources/resources/#useful-ibm-cloud-resources","text":"IBM Cloud roles access control cluster fast-switching tool cloud shell","title":"Useful IBM Cloud resources"},{"location":"resources/resources/#additional-resources_1","text":"Cloud-Native Toolkit YouTube Channel","title":"Additional resources"},{"location":"resources/resources/#linking-to-this-site","text":"As the site is under continuous revisions, you may want to provide links that are guaranteed to be available. The following links will always be maintained: Site home : https://cloudnativetoolkit.dev Install : https://cloudnativetoolkit.dev/install Developer setup: https://cloudnativetoolkit.dev/setup Learn : https://cloudnativetoolkit.dev/learn workshop : https://cloudnativetoolkit.dev/workshop","title":"Linking to this site"},{"location":"resources/ibm-cloud/access-control/","text":"Resource Access Management \u00b6 Managing Access to Resources in IBM Cloud IBM Cloud Identity and Access Management (IAM) enables account owners to authenticate and authorize users to access resources. IAM can be complex; these best practices make IAM easier to use. Introduction \u00b6 This tutorial teaches you best practices for using Identity and Access Management (IAM) in IBM Cloud . This video presentation describes the IAM concepts, shows a demonstration, and includes a Q&A: 01:00 -- Features and concepts 26:00 -- Demonstration 46:00 -- Q&A This article explains the concepts described in the video. You can access the presentation in the Office Hours folder in Box. The scripts described in the reference section . Requirements for access control \u00b6 Access control doesn't change the features available in IBM Cloud. It does change who can use which features. One approach to access control is to not use it. Just give everyone access to everything and trust that each user will only do what they should. This is a bit like a hotel where the room doors don't have locks, but each guest is trusted to only enter their own room. Another approach is to limit which users can do what. Some users can enter almost every room (e.g. hotel management), some can rent rooms (e.g. the paying guest), and some can enter the rooms that have been rented for them (e.g. the paying guest's family). This article is about how to apply these limits in IBM Cloud. Here are two common scenarios for employing these limits: Account owner adds users to their account : An account owner must pay for all resources created in their account. How can they control which users can create resources in their account, and which can only use existing resources without creating new ones? Some resources can be expensive, so the owner will want to limit who can create them. Multiple teams developing apps : Each development team can have their own development environment. Each developer on a team should have access to their team's environment but not the other environments. They may be allowed to change aspects of their own environment (such as deploying apps), but should not be able to change other teams' environments. IBM Cloud Identity and Access Management (IAM) \u00b6 Access to resources in IBM Cloud is controlled using IBM Cloud Identity and Access Management (IAM). Identity and access management (IAM) is a general approach many software systems use to authenticate clients and authorize them to use functionality. What is IBM Cloud Identity and Access Management? introduces IBM Cloud's implementation of IAM, which includes this diagram of the IAM components and their relationships: The most fundamental component type is a resource. To organize an account that will be shared by numerous users in various roles, two key aspects are: Create a good set of resource groups to organize the resources Create a good set of access groups to assign access to the resources The documentation explains some strategies: Best practices for setting up your account What is a resource? Best practices for organizing resources in a resource group Best practices for assigning access The IAM structure enables a lot of flexibility for very fine-grained access control. However, this broad range of capabilities can be complex to learn and is often more than many administrators need. This article will focus on the most important IAM components and best practices for how to use those. IAM \u2013- Main components \u00b6 This is a much simpler diagram of the main component types in IAM: In words, this diagram says, \"An access group uses policies to access the resources in a resource group.\" Here are the four main component types (three shown in the diagram) and their purposes: Resource : Anything running in IBM Cloud that can be created from the cloud catalog. Examples include: A Kubernetes or OpenShift cluster, a service instance, even a service itself. Resource Group : Resources that go together, typically to support an app. A common example is: A cluster and its service instances. Access Group : Users who perform the same tasks; the same concept as a role in role-based access control (RBAC). For example: Developers on a team developing an app. Policy : Permissions (bundled as Roles) for users to access resources. Example: Administrator and Manager of the Kubernetes Service (aka IKS). This perspective -- focusing on the main components instead of all of them -- is the beginning of explaining best practices for using IAM. This does not focus on everything IAM can do or on what users must do or can't do, but focuses on recommending what users should do to use IAM successfully. What is a resource group? \u00b6 Let's delve into what a resource group is. A resource group is a group of resources! (Duh!!) But does that mean it's a good idea to grab a bunch of arbitrary resources and put them together in a group, will that really help? No. So how do resource groups help? It's important to design resource groups decently because their flexibility is somewhat limited. When you create a resource, you need to specify what resource group to create it in. This means that you must create a resource group before you can create the resources that go in the group. Also, once you create a resource and specify its group, its group can't change. You can't move an existing resource from one group to another. So take some time to think about what resource groups you need and what resources to create in each one. You can always add more resource groups and resources, but once they're created, you can't move them around. Resources in IBM Cloud \u00b6 This diagram illustrates one perspective for how to view IBM Cloud: It's just a bunch of stuff running in the cloud. That stuff is resources, primarily clusters and the service instances bound to each one. This model is correct but not sufficient. IBM Cloud is a multi-tenant environment, so different tenants own and use different resources. IBM Cloud needs to keep straight which resources are owned and used by which tenants. Resources in IBM Cloud accounts \u00b6 This diagram illustrates the first level of resource organization in IBM Cloud: accounts. Each tenant has their own account. Each resource is created in exactly one account. Accounts organize resources in IBM Cloud in two key respects: Billing : An account owner is billed for the resources running in their account. (And is not billed for the resources running in other accounts.) Access : An account owner can access all of the resources in their account. Also, they can invite other users to their account and grant those users access to the account's resources. Meanwhile, the account owner and other users in an account cannot access any of the resources in any other account (unless they're invited to other accounts). This model is again correct but not sufficient. Not all users in an account should have access to all resources in an account. Resources in IBM Cloud resource groups \u00b6 A sufficiently large enterprise has multiple development teams developing multiple applications, often doing so in multiple development environments (such as one environment per team). A developer on a team should be able to access and perhaps even change the resources in their team's environment, but not other teams' environments. The account managers will probably also need one or more environments for deploying the production versions of applications, and perhaps other environments used for testing, demonstrations, and other uses. In each case, some users will need access to certain environments but should not have access to other environments. This diagram illustrates how resource groups can be used to organize resources by environment. Each resource group contains the resources for an environment, whether that environment is single tenant for a single development team or multi-tenant for the teams that share an environment. A basic environment typically consists of a single Kubernetes or OpenShift cluster and the service instances bound to that cluster. This is everything needed to host an application, whether it's a monolith or a set of microservices, or a set of applications being developed by the same team. To plan the resource groups that you'll need, plan the development teams and the environments that they'll need, plus other environments you may need for production, testing, demonstrations, etc. Today, you may only know some of those needs. Create a new resource group for each environment you know about today, ensuring to give each one a unique name. Then as you create resources for an environment, put them in the environment's resource group. In the future, as you discover the need for other environments, create more resource groups for those and create each environment's resources in its corresponding resource group. The software development lifecycle (SDLC) -- with stages like dev , test , and staging -- typically does not need separate environments, although production typically should have its own environment to isolate it from the rest of development. The stages other than production can typically be performed in the same environment to better utilize its resources. Managing access to resources \u00b6 Once we have our resources organized into resource groups, one group per environment, we then need to organize how users access these resources. This is accomplished in IAM with a combination of access groups that organize users and policies that organize the roles with permissions for accessing resources. Organize users into access groups \u00b6 An access group is a collection of users. Just like a resource group isn't a collection of random resources, an access group isn't a collection of random users. Rather, the users in an access group all do the same thing and thus need the same permissions. In role-based access control (RBAC) , users are grouped into roles and permissions are assigned to roles so that all of the users in a role get the same permissions. An access group effectively serves the same purpose as an RBAC role. Users who will perform the same tasks and therefore need the same permissions should be grouped into the same access group. An added dimension, as we'll see, is that permissions specify not only what actions the users can perform but also what resources the actions can be performed on. This diagram illustrates how a resource group is a collection of resources (as explained above, the resources for an environment) and how an access group is a collection of users: Access groups are more flexible than resource groups. Whereas a resource can only belong to a single resource group that cannot be changed, a user can belong to multiple access groups and can be added and removed whenever needed. Notice that access groups and resource groups are orthogonal, which is to say that they vary independently and are not directly related to each other. Access groups and resource groups are related, but there is no direct connection. (We'll see the indirect relationship below.) Often there are multiple access groups per resource group. The set of users in an access group has nothing to do with the set of resources in a resource group. Group members can be added to and removed from each independently. Policies connect access groups to resources \u00b6 A policy associates an access group with a resource group. Each policy is a collection of roles and each role is a collection of permissions. A policy determines the permissions that users in an access group have to perform on the resources in a resource group. This diagram illustrates how a policy associates an access group with a resource group and how the policy contains a collection of roles: A policy does not necessarily provide access to all of the resources in a resource group, but rather provides access to all resources of a particular resource type in a resource group, a collection that adjusts as resources are added to or removed from the resource group. The permissions and therefore the roles available in a policy depend on the resource type that the policy accesses. The example in the diagram shows three policies for three very common resource types: Kubernetes : Clusters, either Kubernetes or OpenShift . (The IAM roles for OpenShift clusters are the same as for Kubernetes clusters, i.e. both cluster types are the same resource type called Kubernetes Service .) Registry : The IBM Cloud Container Registry , a global multi-tenant singleton built into IBM Cloud that is its own resource type with its own set of roles. Services : Service instances created from the IBM Cloud catalog (that are IAM enabled), including services which may be added to or removed from the catalog in the future. The diagram shows each policy with Administrator and Manager roles for their respective resource type. Administrator grants permissions to use a service including creating and deleting service instances, whereas Manager grants permission to manipulate a service instance (in ways that are service-specific). The set of roles for a resource type is defined in IAM and cannot be changed by the user. Different resource types have different roles. Each role encapsulates a set of related permissions. For example, one of the permissions granted to users with the Administrator role on the Kubernetes Service resource type is containers-kubernetes.cluster.create , which enables the users to create clusters. Other roles grant cluster.read , cluster.operate , and cluster.update , but none of those can create and delete a cluster, they can only use an existing cluster. IAM -- Best Practices \u00b6 The discussion above does not attempt to describe everything that IAM can do, but rather how to use IAM well to accomplish most tasks with low complexity. These guidelines follow a set of best practices. Some of the main ones are: Assign resources to resource groups other than default : An account initially has a single resource group: default . Avoid using it. Rather: Think of development teams developing apps. Create a resource group per team for the team's environment. Assign policies to access groups, not users : Don\u2019t assign the same policy to one user after another, it\u2019ll become a maintenance nightmare. Rather, use role-based access control: Design roles, grant them permissions, and assign users to roles. Assign access to resource groups, not accounts or resources : Don\u2019t create multiple policies with the same access to different resources. Rather: Assign access to a resource group, to grant access to all resources in the resource group but not other resource groups. Best practices for assigning access has similar tips and some additional explanation. Remember, the full component diagram for IAM is quite complex. The main set of components that you should use most of the time is much simpler. Access Groups for Cloud-Native Toolkit \u00b6 As part of the Cloud Native Toolkit, we create a set of three access groups. This approach is pretty good for any account with multiple environments. This diagram illustrates the three access groups and the relative scope of permissions they provide: The three access groups are: Account Manager : Acts like an account owner to configure an account. Includes permissions to create infrastructure components including VLANs, as well as IAM components including resource groups, access groups, etc. Optionally creates clusters for the environment administrator. Environment Administrator : Owns an environment: Creates it, adds and removes services, etc. Includes permissions to create clusters, to create services, and to set RBAC within the cluster. Environment User : Uses an environment: Deploys apps, runs CI/CD pipelines, etc. No permissions to create and delete clusters or services. Does include permissions to read from and write to existing services, but not to change their configuration. An account only needs one account manager access group, but needs a pair of environment administrator and environment user access groups per environment. This enables each environment to be administered and used by a different group of users, effectively confining each group of users to their particular environment. This is the minimum needed. An account could have multiple management access groups for different aspects of account administration, and likewise an environment could have multiple administrator or user access groups for different levels of access to the environment. Access Group Example \u00b6 The previous section explained how we typically use access groups for three roles: account managers, environment administrators, and environment users. This diagram illustrates an example of the access groups for these three roles for an account that contains two development environments: The diagram shows several components: Two development environments : Two development teams, each has its own development environment. Each development environment is organized as a resource group, in this example named team1 and team2 . The first has a Kubernetes cluster while the second has an OpenShift cluster. One account manager access group : The account managers control all resource groups. They act as system administrators for the account. Two environment administrator access groups : Each resource group has its own group of environment administrators who act as site reliability engineers (SREs) of the resources in the resource group. Two environment user access groups : Each resource group has its own group of environment users who use the resources in the resource group. They typically have a development or data science role such as full-stack developer. These few access groups are a pretty good start for a typical account and may be all you'll need. You can always customize the policies in these access groups and/or add more access groups for even finer-grained access management. Configuration Process \u00b6 Plan Installation and Configure Account document the process to set up an account to control access to development environments. This video presentation shows how to perform this process: 3:38 -- Plan Installation 13:16 -- Configure Account 27:44 -- Demo the access groups","title":"Access control"},{"location":"resources/ibm-cloud/access-control/#resource-access-management","text":"Managing Access to Resources in IBM Cloud IBM Cloud Identity and Access Management (IAM) enables account owners to authenticate and authorize users to access resources. IAM can be complex; these best practices make IAM easier to use.","title":"Resource Access Management"},{"location":"resources/ibm-cloud/access-control/#introduction","text":"This tutorial teaches you best practices for using Identity and Access Management (IAM) in IBM Cloud . This video presentation describes the IAM concepts, shows a demonstration, and includes a Q&A: 01:00 -- Features and concepts 26:00 -- Demonstration 46:00 -- Q&A This article explains the concepts described in the video. You can access the presentation in the Office Hours folder in Box. The scripts described in the reference section .","title":"Introduction"},{"location":"resources/ibm-cloud/access-control/#requirements-for-access-control","text":"Access control doesn't change the features available in IBM Cloud. It does change who can use which features. One approach to access control is to not use it. Just give everyone access to everything and trust that each user will only do what they should. This is a bit like a hotel where the room doors don't have locks, but each guest is trusted to only enter their own room. Another approach is to limit which users can do what. Some users can enter almost every room (e.g. hotel management), some can rent rooms (e.g. the paying guest), and some can enter the rooms that have been rented for them (e.g. the paying guest's family). This article is about how to apply these limits in IBM Cloud. Here are two common scenarios for employing these limits: Account owner adds users to their account : An account owner must pay for all resources created in their account. How can they control which users can create resources in their account, and which can only use existing resources without creating new ones? Some resources can be expensive, so the owner will want to limit who can create them. Multiple teams developing apps : Each development team can have their own development environment. Each developer on a team should have access to their team's environment but not the other environments. They may be allowed to change aspects of their own environment (such as deploying apps), but should not be able to change other teams' environments.","title":"Requirements for access control"},{"location":"resources/ibm-cloud/access-control/#ibm-cloud-identity-and-access-management-iam","text":"Access to resources in IBM Cloud is controlled using IBM Cloud Identity and Access Management (IAM). Identity and access management (IAM) is a general approach many software systems use to authenticate clients and authorize them to use functionality. What is IBM Cloud Identity and Access Management? introduces IBM Cloud's implementation of IAM, which includes this diagram of the IAM components and their relationships: The most fundamental component type is a resource. To organize an account that will be shared by numerous users in various roles, two key aspects are: Create a good set of resource groups to organize the resources Create a good set of access groups to assign access to the resources The documentation explains some strategies: Best practices for setting up your account What is a resource? Best practices for organizing resources in a resource group Best practices for assigning access The IAM structure enables a lot of flexibility for very fine-grained access control. However, this broad range of capabilities can be complex to learn and is often more than many administrators need. This article will focus on the most important IAM components and best practices for how to use those.","title":"IBM Cloud Identity and Access Management (IAM)"},{"location":"resources/ibm-cloud/access-control/#iam-main-components","text":"This is a much simpler diagram of the main component types in IAM: In words, this diagram says, \"An access group uses policies to access the resources in a resource group.\" Here are the four main component types (three shown in the diagram) and their purposes: Resource : Anything running in IBM Cloud that can be created from the cloud catalog. Examples include: A Kubernetes or OpenShift cluster, a service instance, even a service itself. Resource Group : Resources that go together, typically to support an app. A common example is: A cluster and its service instances. Access Group : Users who perform the same tasks; the same concept as a role in role-based access control (RBAC). For example: Developers on a team developing an app. Policy : Permissions (bundled as Roles) for users to access resources. Example: Administrator and Manager of the Kubernetes Service (aka IKS). This perspective -- focusing on the main components instead of all of them -- is the beginning of explaining best practices for using IAM. This does not focus on everything IAM can do or on what users must do or can't do, but focuses on recommending what users should do to use IAM successfully.","title":"IAM \u2013- Main components"},{"location":"resources/ibm-cloud/access-control/#what-is-a-resource-group","text":"Let's delve into what a resource group is. A resource group is a group of resources! (Duh!!) But does that mean it's a good idea to grab a bunch of arbitrary resources and put them together in a group, will that really help? No. So how do resource groups help? It's important to design resource groups decently because their flexibility is somewhat limited. When you create a resource, you need to specify what resource group to create it in. This means that you must create a resource group before you can create the resources that go in the group. Also, once you create a resource and specify its group, its group can't change. You can't move an existing resource from one group to another. So take some time to think about what resource groups you need and what resources to create in each one. You can always add more resource groups and resources, but once they're created, you can't move them around.","title":"What is a resource group?"},{"location":"resources/ibm-cloud/access-control/#resources-in-ibm-cloud","text":"This diagram illustrates one perspective for how to view IBM Cloud: It's just a bunch of stuff running in the cloud. That stuff is resources, primarily clusters and the service instances bound to each one. This model is correct but not sufficient. IBM Cloud is a multi-tenant environment, so different tenants own and use different resources. IBM Cloud needs to keep straight which resources are owned and used by which tenants.","title":"Resources in IBM Cloud"},{"location":"resources/ibm-cloud/access-control/#resources-in-ibm-cloud-accounts","text":"This diagram illustrates the first level of resource organization in IBM Cloud: accounts. Each tenant has their own account. Each resource is created in exactly one account. Accounts organize resources in IBM Cloud in two key respects: Billing : An account owner is billed for the resources running in their account. (And is not billed for the resources running in other accounts.) Access : An account owner can access all of the resources in their account. Also, they can invite other users to their account and grant those users access to the account's resources. Meanwhile, the account owner and other users in an account cannot access any of the resources in any other account (unless they're invited to other accounts). This model is again correct but not sufficient. Not all users in an account should have access to all resources in an account.","title":"Resources in IBM Cloud accounts"},{"location":"resources/ibm-cloud/access-control/#resources-in-ibm-cloud-resource-groups","text":"A sufficiently large enterprise has multiple development teams developing multiple applications, often doing so in multiple development environments (such as one environment per team). A developer on a team should be able to access and perhaps even change the resources in their team's environment, but not other teams' environments. The account managers will probably also need one or more environments for deploying the production versions of applications, and perhaps other environments used for testing, demonstrations, and other uses. In each case, some users will need access to certain environments but should not have access to other environments. This diagram illustrates how resource groups can be used to organize resources by environment. Each resource group contains the resources for an environment, whether that environment is single tenant for a single development team or multi-tenant for the teams that share an environment. A basic environment typically consists of a single Kubernetes or OpenShift cluster and the service instances bound to that cluster. This is everything needed to host an application, whether it's a monolith or a set of microservices, or a set of applications being developed by the same team. To plan the resource groups that you'll need, plan the development teams and the environments that they'll need, plus other environments you may need for production, testing, demonstrations, etc. Today, you may only know some of those needs. Create a new resource group for each environment you know about today, ensuring to give each one a unique name. Then as you create resources for an environment, put them in the environment's resource group. In the future, as you discover the need for other environments, create more resource groups for those and create each environment's resources in its corresponding resource group. The software development lifecycle (SDLC) -- with stages like dev , test , and staging -- typically does not need separate environments, although production typically should have its own environment to isolate it from the rest of development. The stages other than production can typically be performed in the same environment to better utilize its resources.","title":"Resources in IBM Cloud resource groups"},{"location":"resources/ibm-cloud/access-control/#managing-access-to-resources","text":"Once we have our resources organized into resource groups, one group per environment, we then need to organize how users access these resources. This is accomplished in IAM with a combination of access groups that organize users and policies that organize the roles with permissions for accessing resources.","title":"Managing access to resources"},{"location":"resources/ibm-cloud/access-control/#organize-users-into-access-groups","text":"An access group is a collection of users. Just like a resource group isn't a collection of random resources, an access group isn't a collection of random users. Rather, the users in an access group all do the same thing and thus need the same permissions. In role-based access control (RBAC) , users are grouped into roles and permissions are assigned to roles so that all of the users in a role get the same permissions. An access group effectively serves the same purpose as an RBAC role. Users who will perform the same tasks and therefore need the same permissions should be grouped into the same access group. An added dimension, as we'll see, is that permissions specify not only what actions the users can perform but also what resources the actions can be performed on. This diagram illustrates how a resource group is a collection of resources (as explained above, the resources for an environment) and how an access group is a collection of users: Access groups are more flexible than resource groups. Whereas a resource can only belong to a single resource group that cannot be changed, a user can belong to multiple access groups and can be added and removed whenever needed. Notice that access groups and resource groups are orthogonal, which is to say that they vary independently and are not directly related to each other. Access groups and resource groups are related, but there is no direct connection. (We'll see the indirect relationship below.) Often there are multiple access groups per resource group. The set of users in an access group has nothing to do with the set of resources in a resource group. Group members can be added to and removed from each independently.","title":"Organize users into access groups"},{"location":"resources/ibm-cloud/access-control/#policies-connect-access-groups-to-resources","text":"A policy associates an access group with a resource group. Each policy is a collection of roles and each role is a collection of permissions. A policy determines the permissions that users in an access group have to perform on the resources in a resource group. This diagram illustrates how a policy associates an access group with a resource group and how the policy contains a collection of roles: A policy does not necessarily provide access to all of the resources in a resource group, but rather provides access to all resources of a particular resource type in a resource group, a collection that adjusts as resources are added to or removed from the resource group. The permissions and therefore the roles available in a policy depend on the resource type that the policy accesses. The example in the diagram shows three policies for three very common resource types: Kubernetes : Clusters, either Kubernetes or OpenShift . (The IAM roles for OpenShift clusters are the same as for Kubernetes clusters, i.e. both cluster types are the same resource type called Kubernetes Service .) Registry : The IBM Cloud Container Registry , a global multi-tenant singleton built into IBM Cloud that is its own resource type with its own set of roles. Services : Service instances created from the IBM Cloud catalog (that are IAM enabled), including services which may be added to or removed from the catalog in the future. The diagram shows each policy with Administrator and Manager roles for their respective resource type. Administrator grants permissions to use a service including creating and deleting service instances, whereas Manager grants permission to manipulate a service instance (in ways that are service-specific). The set of roles for a resource type is defined in IAM and cannot be changed by the user. Different resource types have different roles. Each role encapsulates a set of related permissions. For example, one of the permissions granted to users with the Administrator role on the Kubernetes Service resource type is containers-kubernetes.cluster.create , which enables the users to create clusters. Other roles grant cluster.read , cluster.operate , and cluster.update , but none of those can create and delete a cluster, they can only use an existing cluster.","title":"Policies connect access groups to resources"},{"location":"resources/ibm-cloud/access-control/#iam-best-practices","text":"The discussion above does not attempt to describe everything that IAM can do, but rather how to use IAM well to accomplish most tasks with low complexity. These guidelines follow a set of best practices. Some of the main ones are: Assign resources to resource groups other than default : An account initially has a single resource group: default . Avoid using it. Rather: Think of development teams developing apps. Create a resource group per team for the team's environment. Assign policies to access groups, not users : Don\u2019t assign the same policy to one user after another, it\u2019ll become a maintenance nightmare. Rather, use role-based access control: Design roles, grant them permissions, and assign users to roles. Assign access to resource groups, not accounts or resources : Don\u2019t create multiple policies with the same access to different resources. Rather: Assign access to a resource group, to grant access to all resources in the resource group but not other resource groups. Best practices for assigning access has similar tips and some additional explanation. Remember, the full component diagram for IAM is quite complex. The main set of components that you should use most of the time is much simpler.","title":"IAM -- Best Practices"},{"location":"resources/ibm-cloud/access-control/#access-groups-for-cloud-native-toolkit","text":"As part of the Cloud Native Toolkit, we create a set of three access groups. This approach is pretty good for any account with multiple environments. This diagram illustrates the three access groups and the relative scope of permissions they provide: The three access groups are: Account Manager : Acts like an account owner to configure an account. Includes permissions to create infrastructure components including VLANs, as well as IAM components including resource groups, access groups, etc. Optionally creates clusters for the environment administrator. Environment Administrator : Owns an environment: Creates it, adds and removes services, etc. Includes permissions to create clusters, to create services, and to set RBAC within the cluster. Environment User : Uses an environment: Deploys apps, runs CI/CD pipelines, etc. No permissions to create and delete clusters or services. Does include permissions to read from and write to existing services, but not to change their configuration. An account only needs one account manager access group, but needs a pair of environment administrator and environment user access groups per environment. This enables each environment to be administered and used by a different group of users, effectively confining each group of users to their particular environment. This is the minimum needed. An account could have multiple management access groups for different aspects of account administration, and likewise an environment could have multiple administrator or user access groups for different levels of access to the environment.","title":"Access Groups for Cloud-Native Toolkit"},{"location":"resources/ibm-cloud/access-control/#access-group-example","text":"The previous section explained how we typically use access groups for three roles: account managers, environment administrators, and environment users. This diagram illustrates an example of the access groups for these three roles for an account that contains two development environments: The diagram shows several components: Two development environments : Two development teams, each has its own development environment. Each development environment is organized as a resource group, in this example named team1 and team2 . The first has a Kubernetes cluster while the second has an OpenShift cluster. One account manager access group : The account managers control all resource groups. They act as system administrators for the account. Two environment administrator access groups : Each resource group has its own group of environment administrators who act as site reliability engineers (SREs) of the resources in the resource group. Two environment user access groups : Each resource group has its own group of environment users who use the resources in the resource group. They typically have a development or data science role such as full-stack developer. These few access groups are a pretty good start for a typical account and may be all you'll need. You can always customize the policies in these access groups and/or add more access groups for even finer-grained access management.","title":"Access Group Example"},{"location":"resources/ibm-cloud/access-control/#configuration-process","text":"Plan Installation and Configure Account document the process to set up an account to control access to development environments. This video presentation shows how to perform this process: 3:38 -- Plan Installation 13:16 -- Configure Account 27:44 -- Demo the access groups","title":"Configuration Process"},{"location":"resources/ibm-cloud/cloud-shell/","text":"Introduction to IBM Cloud Shell \u00b6 IBM Cloud Shell is a free service that gives you complete control of your cloud resources, applications, and infrastructure, from any web browser. It's instantly accessible from your IBM Cloud account - no other installation is needed. Features of IBM Cloud Shell include: Pre-configured environment: IBM Cloud Shell provides a curated, cloud-based workspace with dozens of preinstalled tools and programming languages. It's automatically authenticated with your IBM Cloud account so you can start to develop immediately. File upload/download: use this feature to import files to IBM Cloud Shell or pull-down data to your local machine. Multiple sessions: use up to five shell sessions at a time to maximize your productivity. Mirror workflows on your local machine, or view logs on one session while editing a file in another. Accessing the Cloud Shell \u00b6 Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window. Set up the shell environment \u00b6 We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.3.5/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli kube-ps1 has been installed to display the current Kubernetes context and namespace in the prompt. It can be turned on and off with the following commands: kubeon - turns kube-ps1 on for the current session kubeon -g - turns kube-ps1 on globally kubeoff - turns kube-ps1 off for the current session kubeoff -g - turns kube-ps1 off globally Your shell configuration has been updated. Run the following to apply the changes to the current terminal: source ~/.zshrc Follow the instruction given at the end of the output to enable the changes in the current terminal session. You can check the shell was installed correctly by checking the oc sync version: oc sync --version","title":"Cloud shell"},{"location":"resources/ibm-cloud/cloud-shell/#introduction-to-ibm-cloud-shell","text":"IBM Cloud Shell is a free service that gives you complete control of your cloud resources, applications, and infrastructure, from any web browser. It's instantly accessible from your IBM Cloud account - no other installation is needed. Features of IBM Cloud Shell include: Pre-configured environment: IBM Cloud Shell provides a curated, cloud-based workspace with dozens of preinstalled tools and programming languages. It's automatically authenticated with your IBM Cloud account so you can start to develop immediately. File upload/download: use this feature to import files to IBM Cloud Shell or pull-down data to your local machine. Multiple sessions: use up to five shell sessions at a time to maximize your productivity. Mirror workflows on your local machine, or view logs on one session while editing a file in another.","title":"Introduction to IBM Cloud Shell"},{"location":"resources/ibm-cloud/cloud-shell/#accessing-the-cloud-shell","text":"Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window.","title":"Accessing the Cloud Shell"},{"location":"resources/ibm-cloud/cloud-shell/#set-up-the-shell-environment","text":"We have provided a simplified installer that will install tools and configure the shell environment. The installer will first check if the required tool is available in the path. If not, the missing tool(s) will be installed into the bin/ folder of the current user's home directory and the PATH variable will be updated in the .bashrc or .zshrc file to include that directory. The following tools are included in the shell installer: IBM Cloud cli (ibmcloud) ArgoCD cli (argocd) Tekton cli (tkn) IBM Cloud fast switching (icc) kube-ps1 prompt OpenShift cli (oc) Kubernetes cli (kubectl) JSON cli (jq) IBM Garage Cloud CLI (igc) Set up the shell environment by running: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc If successful, you should see something like the following: Downloading scripts: https://github.com/cloud-native-toolkit/cloud-shell-commands/releases/download/0.3.5/assets.tar.gz ** Installing argocd cli ** Installing tkn cli ** Installing kube-ps1 ** Installing icc ** Installing Cloud-Native Toolkit cli kube-ps1 has been installed to display the current Kubernetes context and namespace in the prompt. It can be turned on and off with the following commands: kubeon - turns kube-ps1 on for the current session kubeon -g - turns kube-ps1 on globally kubeoff - turns kube-ps1 off for the current session kubeoff -g - turns kube-ps1 off globally Your shell configuration has been updated. Run the following to apply the changes to the current terminal: source ~/.zshrc Follow the instruction given at the end of the output to enable the changes in the current terminal session. You can check the shell was installed correctly by checking the oc sync version: oc sync --version","title":"Set up the shell environment"},{"location":"resources/ibm-cloud/crw/","text":"Install CodeReady Workspaces on IBM Cloud cluster \u00b6 Add the cloud hosted development integrated development environment, CodeReady Workspaces (CRW) to your environment Installing CodeReady Workspaces \u00b6 CodeReady Workspaces can be easily added to your development experience using the Red Hat Operator Hub. See Chapter 3. Installing CodeReady Workspaces from Operator Hub . CodeReady Workspaces is a developer workspace server and cloud IDE. Workspaces are defined as project code files and all of their dependencies necessary to edit, build, run, and debug them. Each workspace has its own private IDE hosted within it. The IDE is accessible through a browser. The browser downloads the IDE as a single-page web application. CodeReady Workspaces will enable a 100% developer experience to be delivered from a user's browser. This is perfect for running enablement learning from constrained developer laptops, or for site reliability engineers (SREs) to make quick change to a microservice. CodeReady Workspaces Installation Pre-requisite \u00b6 Do this first: Provision the OpenShift Cluster Ensure the logged in user has the Administrator privileges Ensure you have created a new Project to manage the \"codeready workspace operator & cluster\" Setting up the CRW Operator & Cluster: Navigate to Operator Hub and search of the Red Hat CodeReady workspace. Click on the operator and select the appropriate workspace to install the CRW operator. Navigate to the installed operator to view the CRW operator. Now click the link visible as part of the operator to create/view the CheCluster part of the workspace Create the CheCluster button, to navigate to the YAML Configuration page (displays all the parameters). You need to change the following parameter mentioned below, As part of the storage section, please add the following parameters, depending on your cluster type: VPC postgresPVCStorageClassName : ibmc-vpc-block-10iops-tier workspacePVCStorageClassName : ibmc-vpc-block-10iops-tier Classic postgresPVCStorageClassName : ibmc-block-gold workspacePVCStorageClassName : ibmc-block-gold Post definition the yaml section for storage will look as below VPC storage : postgresPVCStorageClassName : ibmc-vpc-block-10iops-tier pvcStrategy : per-workspace pvcClaimSize : 1Gi preCreateSubPaths : true workspacePVCStorageClassName : ibmc-vpc-block-10iops-tier Classic storage : postgresPVCStorageClassName : ibmc-block-gold pvcStrategy : per-workspace pvcClaimSize : 1Gi preCreateSubPaths : true workspacePVCStorageClassName : ibmc-block-gold Now create, the cluster after doing the above changes. The cluster will take few minutes to get created as its resources such as Postgres DB, Keycloak Auth, CRW workspace will get created. Once Cluster is created navigate to the overview tab of the CheCluster in the CRW operator. You will be able to see the below : URL of the CodeReady Workspaces URL URL of the Red Hat SSO Admin Console URL oAuth SSO Enabled. This should be enabled by default, if not please slide the button to enable and confirm TLS Would be disabled. Please slide the button to enable https connectivity to the CRW workspace and confirm You have now completed the provisioning of the Code Ready Workspaces operator into your development cluster and will enable it to be used by the developers that plan to use this development cluster","title":"CodeReady Workspaces"},{"location":"resources/ibm-cloud/crw/#install-codeready-workspaces-on-ibm-cloud-cluster","text":"Add the cloud hosted development integrated development environment, CodeReady Workspaces (CRW) to your environment","title":"Install CodeReady Workspaces on IBM Cloud cluster"},{"location":"resources/ibm-cloud/crw/#installing-codeready-workspaces","text":"CodeReady Workspaces can be easily added to your development experience using the Red Hat Operator Hub. See Chapter 3. Installing CodeReady Workspaces from Operator Hub . CodeReady Workspaces is a developer workspace server and cloud IDE. Workspaces are defined as project code files and all of their dependencies necessary to edit, build, run, and debug them. Each workspace has its own private IDE hosted within it. The IDE is accessible through a browser. The browser downloads the IDE as a single-page web application. CodeReady Workspaces will enable a 100% developer experience to be delivered from a user's browser. This is perfect for running enablement learning from constrained developer laptops, or for site reliability engineers (SREs) to make quick change to a microservice.","title":"Installing CodeReady Workspaces"},{"location":"resources/ibm-cloud/crw/#codeready-workspaces-installation-pre-requisite","text":"Do this first: Provision the OpenShift Cluster Ensure the logged in user has the Administrator privileges Ensure you have created a new Project to manage the \"codeready workspace operator & cluster\" Setting up the CRW Operator & Cluster: Navigate to Operator Hub and search of the Red Hat CodeReady workspace. Click on the operator and select the appropriate workspace to install the CRW operator. Navigate to the installed operator to view the CRW operator. Now click the link visible as part of the operator to create/view the CheCluster part of the workspace Create the CheCluster button, to navigate to the YAML Configuration page (displays all the parameters). You need to change the following parameter mentioned below, As part of the storage section, please add the following parameters, depending on your cluster type: VPC postgresPVCStorageClassName : ibmc-vpc-block-10iops-tier workspacePVCStorageClassName : ibmc-vpc-block-10iops-tier Classic postgresPVCStorageClassName : ibmc-block-gold workspacePVCStorageClassName : ibmc-block-gold Post definition the yaml section for storage will look as below VPC storage : postgresPVCStorageClassName : ibmc-vpc-block-10iops-tier pvcStrategy : per-workspace pvcClaimSize : 1Gi preCreateSubPaths : true workspacePVCStorageClassName : ibmc-vpc-block-10iops-tier Classic storage : postgresPVCStorageClassName : ibmc-block-gold pvcStrategy : per-workspace pvcClaimSize : 1Gi preCreateSubPaths : true workspacePVCStorageClassName : ibmc-block-gold Now create, the cluster after doing the above changes. The cluster will take few minutes to get created as its resources such as Postgres DB, Keycloak Auth, CRW workspace will get created. Once Cluster is created navigate to the overview tab of the CheCluster in the CRW operator. You will be able to see the below : URL of the CodeReady Workspaces URL URL of the Red Hat SSO Admin Console URL oAuth SSO Enabled. This should be enabled by default, if not please slide the button to enable and confirm TLS Would be disabled. Please slide the button to enable https connectivity to the CRW workspace and confirm You have now completed the provisioning of the Code Ready Workspaces operator into your development cluster and will enable it to be used by the developers that plan to use this development cluster","title":"CodeReady Workspaces Installation Pre-requisite"},{"location":"resources/ibm-cloud/ibm-cloud-roles/","text":"IBM Cloud account management \u00b6 The IBM Cloud environment is provided with a number of powerful tools to manage user access and resource provisioning but little is configured for you out of the box. This guide gives an approach to managing the account in a sensible way that can easily be extended or re-configured based upon the requirements of a given situation. This approach to managing the account is organized around four key roles: Account owner(s) Account managers Environment administrators Environment users This diagram shows the relationship of these access groups to a pair of development environments: Account owner(s) \u00b6 The account owner(s) is the user who owns the account or users who have been granted super-user access to the account at the same level as the account owner. An account owner must create the access group for account managers. The account owner will: Create an ACCT-MGR-IAM-ADMIN access group using the acp-mgr script Add a functional ID, configured using the acp-iaas script, with API keys for the account managers Account managers \u00b6 The account managers are an account owner or other users with account management permissions As described in Configure Account , the account managers can set up the resource groups and access groups needed to install and use the environments. For each environment, the account managers will: Create a resource group Create an access group named <resource_group>-ADMIN using the script acp-admin Create an access group named <resource_group>-USER using the script acp-user Environment administrators \u00b6 The environment administrators are users in the account with permissions to create services in the environment's resource group. In this case, the \"environment\" is scoped to the resource group . Environment administrators are granted broad access to create, manage, and destroy services and resources within a given resource group . Environment users \u00b6 The environment users are users in the account with permissions to use existing services in the environment's resource group (e.g. developers, data scientists, etc.). They are consumers of the services and resources that have been provisioned in order to build and deploy business applications. More information can be found on the IBM Cloud documentation","title":"IBM Cloud roles"},{"location":"resources/ibm-cloud/ibm-cloud-roles/#ibm-cloud-account-management","text":"The IBM Cloud environment is provided with a number of powerful tools to manage user access and resource provisioning but little is configured for you out of the box. This guide gives an approach to managing the account in a sensible way that can easily be extended or re-configured based upon the requirements of a given situation. This approach to managing the account is organized around four key roles: Account owner(s) Account managers Environment administrators Environment users This diagram shows the relationship of these access groups to a pair of development environments:","title":"IBM Cloud account management"},{"location":"resources/ibm-cloud/ibm-cloud-roles/#account-owners","text":"The account owner(s) is the user who owns the account or users who have been granted super-user access to the account at the same level as the account owner. An account owner must create the access group for account managers. The account owner will: Create an ACCT-MGR-IAM-ADMIN access group using the acp-mgr script Add a functional ID, configured using the acp-iaas script, with API keys for the account managers","title":"Account owner(s)"},{"location":"resources/ibm-cloud/ibm-cloud-roles/#account-managers","text":"The account managers are an account owner or other users with account management permissions As described in Configure Account , the account managers can set up the resource groups and access groups needed to install and use the environments. For each environment, the account managers will: Create a resource group Create an access group named <resource_group>-ADMIN using the script acp-admin Create an access group named <resource_group>-USER using the script acp-user","title":"Account managers"},{"location":"resources/ibm-cloud/ibm-cloud-roles/#environment-administrators","text":"The environment administrators are users in the account with permissions to create services in the environment's resource group. In this case, the \"environment\" is scoped to the resource group . Environment administrators are granted broad access to create, manage, and destroy services and resources within a given resource group .","title":"Environment administrators"},{"location":"resources/ibm-cloud/ibm-cloud-roles/#environment-users","text":"The environment users are users in the account with permissions to use existing services in the environment's resource group (e.g. developers, data scientists, etc.). They are consumers of the services and resources that have been provisioned in order to build and deploy business applications. More information can be found on the IBM Cloud documentation","title":"Environment users"},{"location":"resources/ibm-cloud/icc/","text":"IBM Cloud cluster fast-switching (icc) tool \u00b6 Easily log in to an IBM Cloud account and cluster The ICC command-line tool makes it easy to log into an IBM Cloud account and a cluster in that account. It is the equivalent of running this for Kubernetes: ibmcloud login <a whole bunch of parameters> ibmcloud ks cluster <some more parameters> or this for OpenShift: ibmcloud login <a whole bunch of parameters> oc login <some more parameters> If you have multiple clusters, especially in multiple accounts, ICC makes it easy to switch your command line between clusters. All ICC requires is your API key for each account. Install ICC \u00b6 To install ICC, perform Developer Tools Setup to install all of the CLIs including ICC. Install yq \u00b6 ICC (currently) requires yq , and it has to be v3 (v4 doesn't have the commands ICC needs). Check to see if yq is currently installed and if it's version 3: $ which yq /usr/local/bin/yq $ yq -V yq version 3 .3.2 If you don't have yq installed, follow either of these instructions: Installing yq in Installing Command-Line Tools > For macOS Download the latest v3 build Install yq v3 using Homebrew brew install yq@3 And follow the instructions to add the install directory to your path. ICC help \u00b6 Once ICC (and yq) is installed, you can view help for how to use it: icc --help Usage: icc [ nickname ] [ --account { ACCOUNT_FILTER }] [ --resourceGroup { RG_FILTER }] [ --generate [{ ACCOUNT_FILTER }]] [ --nickname [{ cluster name }]] Logs into an IBM Cloud cluster Modes: cluster login - icc { nickname } print config - icc [ --account { account name }] [ --resourceGroup { resourceGroupName }] generate config - icc --generate [{ account name }] update nickname - icc --nickname [{ cluster name }] Args: nickname - the cluster nickname --account, -a - filter to print the config for the provided account --resourceGroup, -g - filter to print the config for the provided resource group --generate - flag to generate the config from the account, optionally restricted to { account name } . The --generate process appends to the existing configuration --nickname - update the nickname for a cluster, optionally passing the cluster name after the flag --help, -h - display help Configure ICC \u00b6 Once ICC is installed, before you can use it, you need to set it up: Get your API key for your IBM Cloud account. If you don't have one, then create an API key . Remember, your API key is uniquely for you , so keep it secret. Run icc --add-account to add you API key to ICC icc --add-account Please provide the nickname for the IBM Cloud account: my-account Provide the IBM Cloud API Key for the my-account account: <hidden> Account information added for my-account Next steps: - Run '/Users/bwoolf/bin/icc --generate' to generate the cluster config from the account This creates the files ~/ibmcloud-account.yaml and ~/ibmcloud.yaml . The first file contains your API keys, so keep it secret. Get the cluster metadata for the account by running icc --generate icc --generate Generate config for accounts: my-account Logging in to account: my-account Processing provider: classic Processing provider: vpc-gen2 Processing provider: satellite Config file generated/updated: /Users/bwoolf/ibmcloud.yaml Next steps: - Run '/Users/bwoolf/bin/icc' to list the clusters - Run '/Users/bwoolf/bin/icc {nickname}' to log into a cluster - Run '/Users/bwoolf/bin/icc --nickname' to update the nickname for a cluster !!!Note If you have multiple accounts, run icc --add-account multiple times to add the different API Keys. Also, icc --generate can be run multiple times to refresh the list of available clusters. Any nicknames that have been added will be preserved. List the available clusters with icc icc my-cluster - my-account/my-resource-group/my-cluster Optionally, give a short nickname for your cluster by running icc --nickname icc --nickname my-cluster Provide the new nickname for my-cluster: my-nickname Nickname for my-cluster updated to my-nickname in /Users/bwoolf/ibmcloud.yaml icc my-nickname - my-account/my-resource-group/my-cluster Use ICC \u00b6 You can now log in to a cluster by running icc [cluster name|cluster nickname] icc my-cluster Logging into ibmcloud: us-south/my-resource-group Logging into OpenShift cluster my-cluster with server url https://x123-z.us-south.containers.cloud.ibm.com:31047","title":"ICC tool"},{"location":"resources/ibm-cloud/icc/#ibm-cloud-cluster-fast-switching-icc-tool","text":"Easily log in to an IBM Cloud account and cluster The ICC command-line tool makes it easy to log into an IBM Cloud account and a cluster in that account. It is the equivalent of running this for Kubernetes: ibmcloud login <a whole bunch of parameters> ibmcloud ks cluster <some more parameters> or this for OpenShift: ibmcloud login <a whole bunch of parameters> oc login <some more parameters> If you have multiple clusters, especially in multiple accounts, ICC makes it easy to switch your command line between clusters. All ICC requires is your API key for each account.","title":"IBM Cloud cluster fast-switching (icc) tool"},{"location":"resources/ibm-cloud/icc/#install-icc","text":"To install ICC, perform Developer Tools Setup to install all of the CLIs including ICC.","title":"Install ICC"},{"location":"resources/ibm-cloud/icc/#install-yq","text":"ICC (currently) requires yq , and it has to be v3 (v4 doesn't have the commands ICC needs). Check to see if yq is currently installed and if it's version 3: $ which yq /usr/local/bin/yq $ yq -V yq version 3 .3.2 If you don't have yq installed, follow either of these instructions: Installing yq in Installing Command-Line Tools > For macOS Download the latest v3 build Install yq v3 using Homebrew brew install yq@3 And follow the instructions to add the install directory to your path.","title":"Install yq"},{"location":"resources/ibm-cloud/icc/#icc-help","text":"Once ICC (and yq) is installed, you can view help for how to use it: icc --help Usage: icc [ nickname ] [ --account { ACCOUNT_FILTER }] [ --resourceGroup { RG_FILTER }] [ --generate [{ ACCOUNT_FILTER }]] [ --nickname [{ cluster name }]] Logs into an IBM Cloud cluster Modes: cluster login - icc { nickname } print config - icc [ --account { account name }] [ --resourceGroup { resourceGroupName }] generate config - icc --generate [{ account name }] update nickname - icc --nickname [{ cluster name }] Args: nickname - the cluster nickname --account, -a - filter to print the config for the provided account --resourceGroup, -g - filter to print the config for the provided resource group --generate - flag to generate the config from the account, optionally restricted to { account name } . The --generate process appends to the existing configuration --nickname - update the nickname for a cluster, optionally passing the cluster name after the flag --help, -h - display help","title":"ICC help"},{"location":"resources/ibm-cloud/icc/#configure-icc","text":"Once ICC is installed, before you can use it, you need to set it up: Get your API key for your IBM Cloud account. If you don't have one, then create an API key . Remember, your API key is uniquely for you , so keep it secret. Run icc --add-account to add you API key to ICC icc --add-account Please provide the nickname for the IBM Cloud account: my-account Provide the IBM Cloud API Key for the my-account account: <hidden> Account information added for my-account Next steps: - Run '/Users/bwoolf/bin/icc --generate' to generate the cluster config from the account This creates the files ~/ibmcloud-account.yaml and ~/ibmcloud.yaml . The first file contains your API keys, so keep it secret. Get the cluster metadata for the account by running icc --generate icc --generate Generate config for accounts: my-account Logging in to account: my-account Processing provider: classic Processing provider: vpc-gen2 Processing provider: satellite Config file generated/updated: /Users/bwoolf/ibmcloud.yaml Next steps: - Run '/Users/bwoolf/bin/icc' to list the clusters - Run '/Users/bwoolf/bin/icc {nickname}' to log into a cluster - Run '/Users/bwoolf/bin/icc --nickname' to update the nickname for a cluster !!!Note If you have multiple accounts, run icc --add-account multiple times to add the different API Keys. Also, icc --generate can be run multiple times to refresh the list of available clusters. Any nicknames that have been added will be preserved. List the available clusters with icc icc my-cluster - my-account/my-resource-group/my-cluster Optionally, give a short nickname for your cluster by running icc --nickname icc --nickname my-cluster Provide the new nickname for my-cluster: my-nickname Nickname for my-cluster updated to my-nickname in /Users/bwoolf/ibmcloud.yaml icc my-nickname - my-account/my-resource-group/my-cluster","title":"Configure ICC"},{"location":"resources/ibm-cloud/icc/#use-icc","text":"You can now log in to a cluster by running icc [cluster name|cluster nickname] icc my-cluster Logging into ibmcloud: us-south/my-resource-group Logging into OpenShift cluster my-cluster with server url https://x123-z.us-south.containers.cloud.ibm.com:31047","title":"Use ICC"},{"location":"resources/workshop/ai/","text":"Deploy an Artificial Intelligence Microservice \u00b6 Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project as prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork application template git repo Open Developer Dashboard from the OpenShift Console Select Starter Kits Select One in our case Artificial Intelligence Microservice Click Fork Login into GIT Sever using the provided username and password (ie userdemo and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following. Note: We are including username/password in git url for simplicity of this lab. You would NOT want to do this in your development environment. GIT_REPO = ai-model-object-detector GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Clone the git repository and change directory cd $HOME git clone $GIT_URL app cd app Create a Tekton pipeline for the application oc pipeline --tekton Use down/up arrow and select ibm-general Enter n and hit Enter to disable image scanning Hit Enter to enable Dockerfile linting Hit Enter to select default health endpoint / Open the url to see the pipeline running in the OpenShift Console Verify that Pipeline Run completed successfully On the OpenShift web console select Pipelines At the top of the page select your development project/namespace created above (ex. project01-dev ) The app pipeline last run status should be Succeeded Review the Pipeline Tasks/Stages. Click on the last run Click on the Test task and view the logs Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link The gitops step of the pipeline triggers Argo CD to deploy the app to QA. Select Developer perspective, select project $TOOLKIT_PROJECT-qa and then select Topology from the Console and verify the application running Open the application route url and try out the application using the swagger UI or append /app to the URL to load Web UI for the Application. You can download the this sample picture to test the app Make a change to the application in the git repository and see the pipeline running again from the Console. Lets change the Machine Learning being used from ssd_mobilenet_v1 to faster_rcnn_resnet101 git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" sed -i 's/ssd_mobilenet_v1/faster_rcnn_resnet101/' Dockerfile git add . git commit -m \"update model\" git push -u origin master Verify that change in Git Server and Git WebHook Open Git Dev from Console Link Navigate to user app git repository Review the recent commit Review the webhook recent delivery Verify that a new Pipeline starts successfully Verify that the App manifests are being updated in the gitops repo in the git account toolkit under the qa directory. Open Git Ops from Console Link Select toolkit/gitops git repository Congratulations you finished this lab, continue with lab Promote an Application using CD with GitOps and ArgoCD","title":"Artificial Intelligence"},{"location":"resources/workshop/ai/#deploy-an-artificial-intelligence-microservice","text":"Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project as prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork application template git repo Open Developer Dashboard from the OpenShift Console Select Starter Kits Select One in our case Artificial Intelligence Microservice Click Fork Login into GIT Sever using the provided username and password (ie userdemo and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following. Note: We are including username/password in git url for simplicity of this lab. You would NOT want to do this in your development environment. GIT_REPO = ai-model-object-detector GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Clone the git repository and change directory cd $HOME git clone $GIT_URL app cd app Create a Tekton pipeline for the application oc pipeline --tekton Use down/up arrow and select ibm-general Enter n and hit Enter to disable image scanning Hit Enter to enable Dockerfile linting Hit Enter to select default health endpoint / Open the url to see the pipeline running in the OpenShift Console Verify that Pipeline Run completed successfully On the OpenShift web console select Pipelines At the top of the page select your development project/namespace created above (ex. project01-dev ) The app pipeline last run status should be Succeeded Review the Pipeline Tasks/Stages. Click on the last run Click on the Test task and view the logs Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link The gitops step of the pipeline triggers Argo CD to deploy the app to QA. Select Developer perspective, select project $TOOLKIT_PROJECT-qa and then select Topology from the Console and verify the application running Open the application route url and try out the application using the swagger UI or append /app to the URL to load Web UI for the Application. You can download the this sample picture to test the app Make a change to the application in the git repository and see the pipeline running again from the Console. Lets change the Machine Learning being used from ssd_mobilenet_v1 to faster_rcnn_resnet101 git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" sed -i 's/ssd_mobilenet_v1/faster_rcnn_resnet101/' Dockerfile git add . git commit -m \"update model\" git push -u origin master Verify that change in Git Server and Git WebHook Open Git Dev from Console Link Navigate to user app git repository Review the recent commit Review the webhook recent delivery Verify that a new Pipeline starts successfully Verify that the App manifests are being updated in the gitops repo in the git account toolkit under the qa directory. Open Git Ops from Console Link Select toolkit/gitops git repository Congratulations you finished this lab, continue with lab Promote an Application using CD with GitOps and ArgoCD","title":"Deploy an Artificial Intelligence Microservice"},{"location":"resources/workshop/appmod/","text":"Application Modernization DevOps, Monolith to Container \u00b6 This section will cover: Application Modernization DevOps, Monolith to Container Deploy the modernized Customer Order Services application in a WebSphere Liberty container to a Red Hat OpenShift cluster The DevOps process is composed of Continuos Integration (CI) with OpenShift Pipelines (Tekton) and Continuos Deployment (CD) with GitOps engine ArgoCD Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell An user with cluster-admin (ie kubeadmin) needs to deploy a DB2 instance to be shared by all the users oc new-project db2 oc create -n db2 serviceaccount mysvcacct oc adm policy add-scc-to-user privileged system:serviceaccount:db2:mysvcacct oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-dc.yaml\" oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-service.yaml\" (Optional) Analyze the application using the following guide Modernizing runtimes with Liberty Download Transformation Advisor The results of a Data Collector is already provided provided download AppSrv01.zip Upload the data collection into Transformation Advisor Review the CustomerOrderServicesApp.ear analysis The migration path files have been deployed to git for this lab. Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If you are using an IBM Cloud cluster login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Liberty AppMod (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = appmod-liberty-toolkit GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton \\ ${ GIT_URL } #master \\ --pipeline ibm-appmod-liberty \\ -p scan-image = false \\ -p health-endpoint = / \\ -p java-bin-path = CustomerOrderServicesApp/target Notice above that the Toolkit pipeline CLI plugin accepts pipeline names and parameters The endpoint to check that the external access is possible is / The Java bin path is not located based on the root of the git repository and instead in CustomerOrderServicesApp/target Every application would have custom values you can pass them to the pipeline as parameters. Verity that the Pipeline started using the URL printed by the command This is a good moment to take a break as the pipelines will take a few minutes. Verify that Pipeline Run completed successfully Review the Pipeline Tasks/Stages Test Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link Review the Application in GitOps git repository. The pipeline step gitops is pushing the application manifest into the GitOps git repository Open Git Ops from Console link Navigate to project1/qa/project1/appmod-liberty-toolkit Review the Helm Chart for the Application Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-qa-websphere-liberty (ie project1-qa-websphere-liberty) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT}/appmod-liberty-toolkit (ie project1/qa/project1/appmod-liberty-toolkit) Cluster: in-cluster Namespace: ${TOOLKIT_PROJECT}-qa (ie. project1-qa) Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: ${TOOLKIT_PROJECT}-websphere-liberty (ie project1-websphere-liberty) Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Click on the route url from the appmod-liberty-toolkit deployment, or the link on the circle. Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Now the Websphere application is ready, the development teams can make changes to git repository for the application, while the gitops git repository is owned by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"Application modernization"},{"location":"resources/workshop/appmod/#application-modernization-devops-monolith-to-container","text":"This section will cover: Application Modernization DevOps, Monolith to Container Deploy the modernized Customer Order Services application in a WebSphere Liberty container to a Red Hat OpenShift cluster The DevOps process is composed of Continuos Integration (CI) with OpenShift Pipelines (Tekton) and Continuos Deployment (CD) with GitOps engine ArgoCD Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell An user with cluster-admin (ie kubeadmin) needs to deploy a DB2 instance to be shared by all the users oc new-project db2 oc create -n db2 serviceaccount mysvcacct oc adm policy add-scc-to-user privileged system:serviceaccount:db2:mysvcacct oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-dc.yaml\" oc apply -n db2 -f \"http:// $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/appmod-liberty-toolkit/raw/master/db2/db2-service.yaml\" (Optional) Analyze the application using the following guide Modernizing runtimes with Liberty Download Transformation Advisor The results of a Data Collector is already provided provided download AppSrv01.zip Upload the data collection into Transformation Advisor Review the CustomerOrderServicesApp.ear analysis The migration path files have been deployed to git for this lab. Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable replace user1 with assigned usernames TOOLKIT_USERNAME = user1 (Skip if using KubeAdmin or IBM Cloud) Login into OpenShift using oc If you are using an IBM Cloud cluster login with your IBM account email and IAM API Key or Token, if using a cluster that was configured with the workshop scripts outside IBM Cloud then use user1 or respective assigned username, and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable replace project1 or projectx based on user id assigned TOOLKIT_PROJECT = project1 Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Liberty AppMod (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = appmod-liberty-toolkit GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton \\ ${ GIT_URL } #master \\ --pipeline ibm-appmod-liberty \\ -p scan-image = false \\ -p health-endpoint = / \\ -p java-bin-path = CustomerOrderServicesApp/target Notice above that the Toolkit pipeline CLI plugin accepts pipeline names and parameters The endpoint to check that the external access is possible is / The Java bin path is not located based on the root of the git repository and instead in CustomerOrderServicesApp/target Every application would have custom values you can pass them to the pipeline as parameters. Verity that the Pipeline started using the URL printed by the command This is a good moment to take a break as the pipelines will take a few minutes. Verify that Pipeline Run completed successfully Review the Pipeline Tasks/Stages Test Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link Review the Application in GitOps git repository. The pipeline step gitops is pushing the application manifest into the GitOps git repository Open Git Ops from Console link Navigate to project1/qa/project1/appmod-liberty-toolkit Review the Helm Chart for the Application Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-qa-websphere-liberty (ie project1-qa-websphere-liberty) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT}/appmod-liberty-toolkit (ie project1/qa/project1/appmod-liberty-toolkit) Cluster: in-cluster Namespace: ${TOOLKIT_PROJECT}-qa (ie. project1-qa) Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: ${TOOLKIT_PROJECT}-websphere-liberty (ie project1-websphere-liberty) Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Click on the route url from the appmod-liberty-toolkit deployment, or the link on the circle. Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Now the Websphere application is ready, the development teams can make changes to git repository for the application, while the gitops git repository is owned by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"Application Modernization DevOps, Monolith to Container"},{"location":"resources/workshop/cd/","text":"Promote an Application using CD with GitOps and ArgoCD \u00b6 Promote an Application using CD with GitOps and ArgoCD Prerequisites Complete lab Deploy an Application using CI Pipelines with Tekton . Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Verify Application is deployed in QA Get ArgoCD's admin password by running oc credentials Select ArgoCD from the Console Link and login using admin and the password you just retrieved Filter Applications by name ${TOOLKIT_PROJECT}-qa (ie project01-qa) Select the application master-qa-${TOOLKIT_PROJECT}-app (ie master-qa-project01-app) Verify Application is running in the QA namespace corresponding to your username ${TOOLKIT_PROJECT}-qa Select Developer perspective, select project ${TOOLKIT_PROJECT}-qa and then select Topology from the Console and see the application running Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL cd gitops Review the qa and staging directory in the git repository ls -l qa/ ls -l staging/ Promote the application from QA to STAGING by copying the app manifest files using git git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cp -a qa/ ${ TOOLKIT_PROJECT } / staging/ ${ TOOLKIT_PROJECT } / git add . git commit -m \"Promote Application from QA to STAGING environment for $TOOLKIT_PROJECT \" git push -u origin master Verify Application is deployed in STAGING Get ArgoCD's admin password by running oc credentials Select ArgoCD from the Console Link and login using admin and the password you just retrieved Filter Applications by namespace ${TOOLKIT_PROJECT}-staging (ie project01-staging) It might take a couple minutes for the application to show up Select the application master-staging-${TOOLKIT_PROJECT}-app (ie master-staging-project01-app) Click Refresh Verify Application is running in the STAGING namespace corresponding to your username ${TOOLKIT_PROJECT} Select Developer perspective, select project ${TOOLKIT_PROJECT}-staging and then select Topology from the Console and see the application running Propose a change for the Application in STAGING Update the replica count and create a new git branch in remote repo cat > staging/ ${ TOOLKIT_PROJECT } /app/values.yaml <<EOF global: {} app: replicaCount: 2 EOF git diff git add . git checkout -b ${ TOOLKIT_PROJECT } -pr1 git commit -m \"Update Application in ${ TOOLKIT_PROJECT } -staging namespace\" git push -u origin ${ TOOLKIT_PROJECT } -pr1 Open Git Ops from Console Link Select toolkit/gitops git repository Create a Pull Request Select Pull Request Click New Pull Request Select from compare dropdown the branch ${TOOLKIT_PROJECT}-pr1 Enter a title like Update replica count for app in namespace $TOOLKIT_PROJECT Enter a Comment like We need more instances business is growing Yay! click Create Pull Request Review the PR follow the change management process established by your team. Click Merge Pull Request Click Delete Branch Review that application scales out Review in ArgoCD UI, notice there are now two pods, it takes about 4 minutes to sync, you can click Refresh Review in OpenShift Console, click the Deployment circle details shows 2 Pods. Congratulations you finished this activity, continue with the lab Deploy a 3 tier Microservice using React, Node.js, and Java","title":"Continuous delivery"},{"location":"resources/workshop/cd/#promote-an-application-using-cd-with-gitops-and-argocd","text":"Promote an Application using CD with GitOps and ArgoCD Prerequisites Complete lab Deploy an Application using CI Pipelines with Tekton . Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Verify Application is deployed in QA Get ArgoCD's admin password by running oc credentials Select ArgoCD from the Console Link and login using admin and the password you just retrieved Filter Applications by name ${TOOLKIT_PROJECT}-qa (ie project01-qa) Select the application master-qa-${TOOLKIT_PROJECT}-app (ie master-qa-project01-app) Verify Application is running in the QA namespace corresponding to your username ${TOOLKIT_PROJECT}-qa Select Developer perspective, select project ${TOOLKIT_PROJECT}-qa and then select Topology from the Console and see the application running Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL cd gitops Review the qa and staging directory in the git repository ls -l qa/ ls -l staging/ Promote the application from QA to STAGING by copying the app manifest files using git git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cp -a qa/ ${ TOOLKIT_PROJECT } / staging/ ${ TOOLKIT_PROJECT } / git add . git commit -m \"Promote Application from QA to STAGING environment for $TOOLKIT_PROJECT \" git push -u origin master Verify Application is deployed in STAGING Get ArgoCD's admin password by running oc credentials Select ArgoCD from the Console Link and login using admin and the password you just retrieved Filter Applications by namespace ${TOOLKIT_PROJECT}-staging (ie project01-staging) It might take a couple minutes for the application to show up Select the application master-staging-${TOOLKIT_PROJECT}-app (ie master-staging-project01-app) Click Refresh Verify Application is running in the STAGING namespace corresponding to your username ${TOOLKIT_PROJECT} Select Developer perspective, select project ${TOOLKIT_PROJECT}-staging and then select Topology from the Console and see the application running Propose a change for the Application in STAGING Update the replica count and create a new git branch in remote repo cat > staging/ ${ TOOLKIT_PROJECT } /app/values.yaml <<EOF global: {} app: replicaCount: 2 EOF git diff git add . git checkout -b ${ TOOLKIT_PROJECT } -pr1 git commit -m \"Update Application in ${ TOOLKIT_PROJECT } -staging namespace\" git push -u origin ${ TOOLKIT_PROJECT } -pr1 Open Git Ops from Console Link Select toolkit/gitops git repository Create a Pull Request Select Pull Request Click New Pull Request Select from compare dropdown the branch ${TOOLKIT_PROJECT}-pr1 Enter a title like Update replica count for app in namespace $TOOLKIT_PROJECT Enter a Comment like We need more instances business is growing Yay! click Create Pull Request Review the PR follow the change management process established by your team. Click Merge Pull Request Click Delete Branch Review that application scales out Review in ArgoCD UI, notice there are now two pods, it takes about 4 minutes to sync, you can click Refresh Review in OpenShift Console, click the Deployment circle details shows 2 Pods. Congratulations you finished this activity, continue with the lab Deploy a 3 tier Microservice using React, Node.js, and Java","title":"Promote an Application using CD with GitOps and ArgoCD"},{"location":"resources/workshop/ci/","text":"Deploy an Application using CI Pipelines with Tekton \u00b6 Prerequisites The instructor should Setup Workshop Environment The student should Setup the CLI locally or Setup the Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project as prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork application template git repo Open Developer Dashboard from the OpenShift Console Select Starter Kits Select One in our case Go Gin Microservice Click Fork Login into GIT Sever using the provided username and password (ie userdemo and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = go-gin GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Clone the git repository and change directory cd $HOME git clone $GIT_URL app cd app Create a pipeline for the application oc pipeline --tekton Use down/up arrow and select ibm-golang Hit Enter to enable image scanning Open the url to see the pipeline running in the OpenShift Console Verify that Pipeline Run completed successfully On the OpenShift web console select Pipelines At the top of the page select your development project/namespace created above (ex. project01-dev ) The app pipeline last run status should be Succeeded Review the Pipeline Tasks/Stages. Click on the last run Click on the Test task and view the logs Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link The gitops step of the pipeline triggers Argo CD to deploy the app to QA. Select Developer perspective, select project $TOOLKIT_PROJECT-qa and then select Topology from the Console and verify the application running Open the application route url and try out the application using the swagger UI Make a change to the application in the git repository and see the pipeline running again from the Console. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" echo \"A change to trigger a new PipelineRun $( date ) \" >> README.md git add . git commit -m \"update readme\" git push -u origin master Verify that change in Git Server and Git WebHook Open Git Dev from Console Link Navigate to user app git repository Review the recent commit Review the webhook recent delivery Verify that a new Pipeline starts successfully Verify that the App manifests are being updated in the gitops repo in the git account toolkit under the qa directory. Open Git Ops from Console Link Select toolkit/gitops git repository Congratulations you finished this lab, continue with lab Promote an Application using CD with GitOps and ArgoCD","title":"Continuous integration"},{"location":"resources/workshop/ci/#deploy-an-application-using-ci-pipelines-with-tekton","text":"Prerequisites The instructor should Setup Workshop Environment The student should Setup the CLI locally or Setup the Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project as prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork application template git repo Open Developer Dashboard from the OpenShift Console Select Starter Kits Select One in our case Go Gin Microservice Click Fork Login into GIT Sever using the provided username and password (ie userdemo and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = go-gin GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Clone the git repository and change directory cd $HOME git clone $GIT_URL app cd app Create a pipeline for the application oc pipeline --tekton Use down/up arrow and select ibm-golang Hit Enter to enable image scanning Open the url to see the pipeline running in the OpenShift Console Verify that Pipeline Run completed successfully On the OpenShift web console select Pipelines At the top of the page select your development project/namespace created above (ex. project01-dev ) The app pipeline last run status should be Succeeded Review the Pipeline Tasks/Stages. Click on the last run Click on the Test task and view the logs Open SonarQube from Console Link Open Registry from Console Link Open Artifactory from Console Link The gitops step of the pipeline triggers Argo CD to deploy the app to QA. Select Developer perspective, select project $TOOLKIT_PROJECT-qa and then select Topology from the Console and verify the application running Open the application route url and try out the application using the swagger UI Make a change to the application in the git repository and see the pipeline running again from the Console. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" echo \"A change to trigger a new PipelineRun $( date ) \" >> README.md git add . git commit -m \"update readme\" git push -u origin master Verify that change in Git Server and Git WebHook Open Git Dev from Console Link Navigate to user app git repository Review the recent commit Review the webhook recent delivery Verify that a new Pipeline starts successfully Verify that the App manifests are being updated in the gitops repo in the git account toolkit under the qa directory. Open Git Ops from Console Link Select toolkit/gitops git repository Congratulations you finished this lab, continue with lab Promote an Application using CD with GitOps and ArgoCD","title":"Deploy an Application using CI Pipelines with Tekton"},{"location":"resources/workshop/inventory/","text":"Deploy a 3 tier Microservice using React, Node.js, and Java \u00b6 Deploy a 3 tier Microservice using React, Node.js, and Java Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory Service (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-svc-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application TypeScript Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory BFF (TypeScript) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-bff-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application React Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory UI (React) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-ui-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL gitops-inventory cd gitops-inventory Review the qa directory in the git repository, the directory might be empty if the 3 pipelines are not done yet. ls -l qa/ ${ TOOLKIT_PROJECT } / Review the qa directory in the git repository again ls -l qa/ ${ TOOLKIT_PROJECT } / You should see 3 directories inventory-management-bff-solution/ inventory-management-svc-solution/ inventory-management-ui-solution/ Note If you don't see the directories, this is a good time for a coffee break of 15 minutes until all 3 Pipeline Runs are done. Once the Pipeline Runs are done, try to list the directories again. Each directory contains their corresponding yaml manifest files (ie Helm Chart) ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-bff-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-svc-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-ui-solution Promote the application to QA using git by creating a manifest yaml (ie Helm Chart) that leverage the Cloud Native Toolkit chart argocd-config to automate the creation of multiple ArgoCD Applications. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cat > qa/ ${ TOOLKIT_PROJECT } /Chart.yaml <<EOF apiVersion: v2 version: 1.0.0 name: project-config-helm description: Chart to configure ArgoCD with the inventory application dependencies: - name: argocd-config version: 0.16.0 repository: https://ibm-garage-cloud.github.io/toolkit-charts EOF cat > qa/ ${ TOOLKIT_PROJECT } /values.yaml <<EOF global: {} argocd-config: repoUrl: \"http://gogs.tools:3000/toolkit/gitops.git\" project: inventory-qa applicationTargets: - targetRevision: master createNamespace: true targetNamespace: ${TOOLKIT_PROJECT}-qa applications: - name: qa-${TOOLKIT_PROJECT}-inventory-svc path: qa/${TOOLKIT_PROJECT}/inventory-management-svc-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-bff path: qa/${TOOLKIT_PROJECT}/inventory-management-bff-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-ui path: qa/${TOOLKIT_PROJECT}/inventory-management-ui-solution type: helm EOF cat qa/ ${ TOOLKIT_PROJECT } /values.yaml git add . git commit -m \"Add inventory application to gitops for project ${ TOOLKIT_PROJECT } \" git push -u origin master Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-inventory (ie project1-inventory) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT} (ie qa/project1) Cluster: in-cluster Namespace: tools Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: inventory-management-svc-solution Review Application: inventory-management-bff-solution Review Application: inventory-management-ui-solution Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Open the Application from the JavaScript UI and make sure the stocks show up in the browser. Click on the route url on from the ui deployment, or the link on the circle. Now the Microservices application is ready for the development teams, in some cases each team will own and work with the git repository for the microservices, while the gitops git repository is own by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"3-tier application"},{"location":"resources/workshop/inventory/#deploy-a-3-tier-microservice-using-react-nodejs-and-java","text":"Deploy a 3 tier Microservice using React, Node.js, and Java Prerequisites The instructor should Setup Workshop Environment The student should Setup CLI and Terminal Shell Instructor will provide the following info: OpenShift Console URL (OCP_CONSOLE_URL) The username and password for OpenShift and Git Server (default values are user1, user2, etc.. for users and password for password). Set TOOLKIT_USERNAME environment variable. If you are participation in a workshop replace userdemo with your assigned username (ex. user01 ). TOOLKIT_USERNAME = userdemo (Skip if using KubeAdmin or IBM Cloud) Login to OpenShift using oc If using IBM Cloud cluster then login with your IBM account email and IAM API Key or Token by using the Copy Login Command If using a cluster that was configured with the workshop scripts outside IBM Cloud then use respective assigned username (ex. user01 ), and the password is password oc login $OCP_URL -u $TOOLKIT_USERNAME -p password Set TOOLKIT_PROJECT environment variable If you are participation in a workshop replace projectdemo based on your assigned username (ex. project01 ). TOOLKIT_PROJECT = projectdemo Create a project/namespace using your project prefix, and -dev and suffix oc sync $TOOLKIT_PROJECT -dev Fork Inventory Sample Application Java Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory Service (Java) Click Fork Login into GIT Sever using the provided username and password (ie user1 and password ) Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-svc-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application TypeScript Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory BFF (TypeScript) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-bff-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Fork Inventory Sample Application React Open Developer Dashboard from the OpenShift Console Select Starter Kits Select Inventory UI (React) Click Fork Click Fork Repository Setup environment variable GIT_URL for the git url using the value from previous step or as following GIT_REPO = inventory-management-ui-solution GIT_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) / ${ TOOLKIT_USERNAME } / ${ GIT_REPO } echo GIT_URL = ${ GIT_URL } Create a pipeline for the application oc pipeline --tekton ${ GIT_URL } #master -p scan-image=false Open the url to see the pipeline running in the OpenShift Console Setup environment variable GIT_OPS_URL for the git url using the value from previous step or as following GIT_OPS_URL = http:// ${ TOOLKIT_USERNAME } :password@ $( oc get route -n tools gogs --template = '{{.spec.host}}' ) /toolkit/gitops echo GIT_OPS_URL = ${ GIT_OPS_URL } Clone the git repository and change directory cd $HOME git clone $GIT_OPS_URL gitops-inventory cd gitops-inventory Review the qa directory in the git repository, the directory might be empty if the 3 pipelines are not done yet. ls -l qa/ ${ TOOLKIT_PROJECT } / Review the qa directory in the git repository again ls -l qa/ ${ TOOLKIT_PROJECT } / You should see 3 directories inventory-management-bff-solution/ inventory-management-svc-solution/ inventory-management-ui-solution/ Note If you don't see the directories, this is a good time for a coffee break of 15 minutes until all 3 Pipeline Runs are done. Once the Pipeline Runs are done, try to list the directories again. Each directory contains their corresponding yaml manifest files (ie Helm Chart) ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-bff-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-svc-solution ls -l qa/ ${ TOOLKIT_PROJECT } /inventory-management-ui-solution Promote the application to QA using git by creating a manifest yaml (ie Helm Chart) that leverage the Cloud Native Toolkit chart argocd-config to automate the creation of multiple ArgoCD Applications. git config --local user.email \" ${ TOOLKIT_USERNAME } @example.com\" git config --local user.name \" ${ TOOLKIT_USERNAME } \" cat > qa/ ${ TOOLKIT_PROJECT } /Chart.yaml <<EOF apiVersion: v2 version: 1.0.0 name: project-config-helm description: Chart to configure ArgoCD with the inventory application dependencies: - name: argocd-config version: 0.16.0 repository: https://ibm-garage-cloud.github.io/toolkit-charts EOF cat > qa/ ${ TOOLKIT_PROJECT } /values.yaml <<EOF global: {} argocd-config: repoUrl: \"http://gogs.tools:3000/toolkit/gitops.git\" project: inventory-qa applicationTargets: - targetRevision: master createNamespace: true targetNamespace: ${TOOLKIT_PROJECT}-qa applications: - name: qa-${TOOLKIT_PROJECT}-inventory-svc path: qa/${TOOLKIT_PROJECT}/inventory-management-svc-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-bff path: qa/${TOOLKIT_PROJECT}/inventory-management-bff-solution type: helm - name: qa-${TOOLKIT_PROJECT}-inventory-ui path: qa/${TOOLKIT_PROJECT}/inventory-management-ui-solution type: helm EOF cat qa/ ${ TOOLKIT_PROJECT } /values.yaml git add . git commit -m \"Add inventory application to gitops for project ${ TOOLKIT_PROJECT } \" git push -u origin master Register the Application in ArgoCD to deploy using GitOps Select ArgoCD from the Console Link and login using OpenShift login Click NEW APP Application Name: ${TOOLKIT_PROJECT}-inventory (ie project1-inventory) ArgoCD Project: default Sync Policy: Automatic (Check prune resources and self heal) Repository URL: http://gogs.tools:3000/toolkit/gitops.git Revision: HEAD Path: qa/${TOOLKIT_PROJECT} (ie qa/project1) Cluster: in-cluster Namespace: tools Click CREATE Review the Applications in ArgoCD Filter by Namespace ${TOOLKIT_PROJECT}-qa (ie project1-qa) Review Application: inventory-management-svc-solution Review Application: inventory-management-bff-solution Review Application: inventory-management-ui-solution Review the Application in OpenShift Switch to Developer perspective Select Topology from the menu Switch to project ${TOOLKIT_PROJECT}-qa (ie project1-qa) Open the Application from the JavaScript UI and make sure the stocks show up in the browser. Click on the route url on from the ui deployment, or the link on the circle. Now the Microservices application is ready for the development teams, in some cases each team will own and work with the git repository for the microservices, while the gitops git repository is own by the operations team. Congratulations you finished this activity, continue with another lab in the workshop","title":"Deploy a 3 tier Microservice using React, Node.js, and Java"},{"location":"resources/workshop/setup/","text":"Setup Workshop Environment \u00b6 Provides the steps to install the Cloud-Native Toolkit and setting up the Cloud-Native Toolkit Workshop hands on labs. 1. Create OpenShift Cluster \u00b6 Create an OpenShift Cluster for example: The 8 hours free Cluster on IBM Open Labs select lab 6 Bring Your Own Application Deploy a Cluster on IBM Cloud VPC2 using the Toolkit On other Clouds using docs from cloudnativetoolkit.dev/multi-cloud IBM internal DTE Infrastructure access via IBM VPN or IBM CSPLAB 2. Install IBM Cloud Native Toolkit \u00b6 Use one of the install options for example the Quick Install curl -sfL get.cloudnativetoolkit.dev | sh - 3. Setup IBM Cloud Native Toolkit Workshop \u00b6 Install the foundation for the workshops curl -sfL workshop.cloudnativetoolkit.dev | sh - Note The username and password for Git Admin is toolkit toolkit Usernames user01 through user15 are configured with a password of password . Username userdemo is configured with the password password . You can use this username if using the workshop environment for self study or giving a demo. If you are preparing the environment for a workshop you can remove userdemo b y running the uninstall-userdemo.sh script which is part of the workshop scripts. 4. (Optional) Customization of the IBM Cloud Native Toolkit Workshop \u00b6 You can customize the Workshop environment by cloning the workshop repo Some of the most common customizations are: You can create more than 15 users by setting a USER_COUNT environment variable. For example to configure 30 users use the command export USER_COUNT = 30 You can create more than 15 projects by setting a PROJECT_COUNT environment variable. For example to configure 30 projects use the command export PROJECT_COUNT = 30 Once you have finished configuring your customizations, login to the cluster from the cli and run the scripts/install.sh script to perform the install. You can also add additional users to the workshop clusters. Create a file with one user id per line. IMPORTANT: there needs to be newline after the last entry. For example a users.txt file with content. additionaluserID anotheruserID someuserID For Openshift clusters on IBM Cloud (ROKS) the user ids are their IBM IDs (email address) all lowercase with an uppercase IAM# prefix added to the beginning. IAM#additionaluserid@email.com IAM#anotheruserid@email.com IAM#someuserid@email.com Create a ADDITIONAL_USERS_FILE environment variable with the path and name of the file. export ADDITIONAL_USERS_FILE = users.txt The additional users will be granted the self-provisioner role, meaning they can create new Openshift Projects. If you wish to remove this permission set an environment variable ADDITIONAL_USERS_SELF_PROVISIONER with a value of N export ADDITIONAL_USERS_SELF_PROVISIONER = N Once you have finished configuring the additional users, login to the cluster from the cli and run the scripts/13-ocp-additional-users.sh script.","title":"Setup Cluster"},{"location":"resources/workshop/setup/#setup-workshop-environment","text":"Provides the steps to install the Cloud-Native Toolkit and setting up the Cloud-Native Toolkit Workshop hands on labs.","title":"Setup Workshop Environment"},{"location":"resources/workshop/setup/#1-create-openshift-cluster","text":"Create an OpenShift Cluster for example: The 8 hours free Cluster on IBM Open Labs select lab 6 Bring Your Own Application Deploy a Cluster on IBM Cloud VPC2 using the Toolkit On other Clouds using docs from cloudnativetoolkit.dev/multi-cloud IBM internal DTE Infrastructure access via IBM VPN or IBM CSPLAB","title":"1. Create OpenShift Cluster"},{"location":"resources/workshop/setup/#2-install-ibm-cloud-native-toolkit","text":"Use one of the install options for example the Quick Install curl -sfL get.cloudnativetoolkit.dev | sh -","title":"2. Install IBM Cloud Native Toolkit"},{"location":"resources/workshop/setup/#3-setup-ibm-cloud-native-toolkit-workshop","text":"Install the foundation for the workshops curl -sfL workshop.cloudnativetoolkit.dev | sh - Note The username and password for Git Admin is toolkit toolkit Usernames user01 through user15 are configured with a password of password . Username userdemo is configured with the password password . You can use this username if using the workshop environment for self study or giving a demo. If you are preparing the environment for a workshop you can remove userdemo b y running the uninstall-userdemo.sh script which is part of the workshop scripts.","title":"3. Setup IBM Cloud Native Toolkit Workshop"},{"location":"resources/workshop/setup/#4-optional-customization-of-the-ibm-cloud-native-toolkit-workshop","text":"You can customize the Workshop environment by cloning the workshop repo Some of the most common customizations are: You can create more than 15 users by setting a USER_COUNT environment variable. For example to configure 30 users use the command export USER_COUNT = 30 You can create more than 15 projects by setting a PROJECT_COUNT environment variable. For example to configure 30 projects use the command export PROJECT_COUNT = 30 Once you have finished configuring your customizations, login to the cluster from the cli and run the scripts/install.sh script to perform the install. You can also add additional users to the workshop clusters. Create a file with one user id per line. IMPORTANT: there needs to be newline after the last entry. For example a users.txt file with content. additionaluserID anotheruserID someuserID For Openshift clusters on IBM Cloud (ROKS) the user ids are their IBM IDs (email address) all lowercase with an uppercase IAM# prefix added to the beginning. IAM#additionaluserid@email.com IAM#anotheruserid@email.com IAM#someuserid@email.com Create a ADDITIONAL_USERS_FILE environment variable with the path and name of the file. export ADDITIONAL_USERS_FILE = users.txt The additional users will be granted the self-provisioner role, meaning they can create new Openshift Projects. If you wish to remove this permission set an environment variable ADDITIONAL_USERS_SELF_PROVISIONER with a value of N export ADDITIONAL_USERS_SELF_PROVISIONER = N Once you have finished configuring the additional users, login to the cluster from the cli and run the scripts/13-ocp-additional-users.sh script.","title":"4. (Optional) Customization of the IBM Cloud Native Toolkit Workshop"},{"location":"resources/workshop/terminal/","text":"Setup Terminal Shell \u00b6 You can use IBM Cloud Shell , the OpenLabs Shell or your local workstation. More details in Toolkit Dev Setup and Toolkit CLI . Run the following command on Cloud, Linux or MacOS shell: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc Be sure to follow the instructions provided to enable the changes in the current terminal session.","title":"Setup Terminal"},{"location":"resources/workshop/terminal/#setup-terminal-shell","text":"You can use IBM Cloud Shell , the OpenLabs Shell or your local workstation. More details in Toolkit Dev Setup and Toolkit CLI . Run the following command on Cloud, Linux or MacOS shell: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.bashrc || source ~/.zshrc Be sure to follow the instructions provided to enable the changes in the current terminal session.","title":"Setup Terminal Shell"},{"location":"resources/workshop/workshop/","text":"IBM Cloud Native Toolkit Workshop \u00b6 The Workshop is design to provide a quick way to try the methodology leveraging the tools that the Toolkit integrates. Agenda \u00b6 Setup Workshop Environment ( workshop admin only ) Deploy an Application using CI Pipelines with Tekton Promote an Application using CD with GitOps and ArgoCD Deploy a 3 tier Microservice using React, Node.js, and Java App Modernization with modern DevOps","title":"Overview"},{"location":"resources/workshop/workshop/#ibm-cloud-native-toolkit-workshop","text":"The Workshop is design to provide a quick way to try the methodology leveraging the tools that the Toolkit integrates.","title":"IBM Cloud Native Toolkit Workshop"},{"location":"resources/workshop/workshop/#agenda","text":"Setup Workshop Environment ( workshop admin only ) Deploy an Application using CI Pipelines with Tekton Promote an Application using CD with GitOps and ArgoCD Deploy a 3 tier Microservice using React, Node.js, and Java App Modernization with modern DevOps","title":"Agenda"},{"location":"setup/fast-start/","text":"Fast-Start Install \u00b6 This section will guide you through installing the Cloud-Native Toolkit suitable for learning how to use the toolkit. Fast-start installation does not install the base Kubernetes cluster. You need to provide the cluster before starting the toolkit installation. Choosing a Kubernetes cluster option \u00b6 The toolkit can be installed over the Kubernetes Service running on IBM Cloud or a Red Hat OpenShift cluster. However, the fast-start installer only works against a Red Hat OpenShift cluster : OpenShift on IBM Cloud Red Hat OpenShift running on IBM Cloud \u00b6 A Red Hat OpenShift cluster is the recommended production development environment. It provides enhanced developer experience and tooling over a standard Kubernetes cluster as well as additional features, such as enhanced security for production workloads. You need an active IBM Cloud account, with billing enabled, as this option will incur costs Learn on recommended production environment Incurs costs Enhanced developer experience and cluster security over standard Kubernetes No local resources needed to run cluster Kubernetes on IBM Cloud Kubernetes running on IBM Cloud \u00b6 This option provides a managed Kubernetes cluster running on the IBM Cloud. Managed Kubernetes environment Incurs costs No local resources needed to run cluster Red Hat OpenShift Red Hat OpenShift \u00b6 This option allows you to use your own installation of RedHat OpenShift as the learning environment for the Cloud-Native Toolkit. You can install OpenShift to your own hardware or use a third party cloud provider. Warning This option requires you to setup and configure an OpenShift or OKD cluster, so assumes you have the environment and skills to complete this task. If not it is recommended that you select one of the other options run on local hardware, virtualized infrastructure or cloud provider need your own OpenShift licenses the cluster must have a default storage class configured, so a persistent volume claim will be satisfied Note OKD is a community distribution of OpenShift. The fast-start installer assumes access to the RedHat Operator Hub so it can install components such as OpenShift Pipelines, so OKD is not supported. CodeReady Containers CodeReady Containers \u00b6 CodeReady Containers is a cut-down version of OpenShift to provide a local development environment on your laptop or workstation. It is freely available, but does have some limitations. Run locally on laptop or workstation no runtime costs Need 16GB memory or greater in host system No remote access from public internet services, such as github cluster access only from host system by default - no remote access to cluster over network Open Labs cluster Open Labs cluster \u00b6 Open Labs is an IBM provided online learning environment. You can access a RedHat OpenShift cluster without needing to install anything locally to your laptop or workstation. No local resources needed to run cluster No runtime costs Limited time cluster - enough time to complete the learning material, but clusters get automatically cleaned up after a few hours Obtaining your Kubernetes Cluster \u00b6 Select the option you want for your cluster, then follow the instructions. OpenShift on IBM Cloud Red Hat OpenShift running on IBM Cloud \u00b6 Sign into your IBM account Navigate to the Catalog using the link at the top of the IBM Cloud web console Search for OpenShift and select RedHat OpenShift on IBM Cloud Create a cluster. On the options page: If working in your company account check with your account admin about the OpenShift licence entitlement, and make the appropriate entitlement choice. Otherwise, leave the OCP entitlement option at the default to purchase the needed licenses Leave the Infrastructure as Classic If you have a preferred Resource Group you need to use, select it here. Check with your account admin to verify the resource group you should use if working in a company account. Otherwise, leave as Default Select your preferred Geography. It is best to select the closest geography to your location Select availability to single zone as this cluster will be used for training, so Multi-zone is not needed Select the worker zone closest to your location Enter a cluster name in the Resource details section Press Create to create your cluster Wait for the cluster to deploy - this can take several minutes, but while waiting you can install the IBM Cloud command line interface (CLI) in the next step If you don't already have the ibmcloud CLI installed on your workstation you need to install it. The instructions can be found in the IBM Cloud documentation When the OpenShift Cluster has been deployed, use the button on the IBM Cloud web console to launch the OpenShift web console If you don't already have the OpenShift command line interface (CLI) installed on your workstation, you should install it now. The installation image can be downloaded from the OpenShift web console. Select the help icon (question mark) next to your user name at the top of the web console. Select Command Line Tools from the menu, then download and install the appropriate version of the oc CLI for your workstation. Open a command terminal window on your workstation (where the ibmcloud and oc command line tooling has been installed) Login to the IBM Cloud with command ibmcloud login if you belong to a company account that has single sign on enabled, then the command is ibmcloud login --sso . If your IBM Cloud account has access to multiple accounts and you get an option to choose the account during the login process, ensure you select the account where the OpenShift cluster was deployed. If you don't see the Resource Group in the account summary presented after logging in. Use command ibmcloud target -g Default to target the correct Resource Group (this is the resource group used when deploying the cluster. The default value is Default ) Click the dropdown next to your username at the top of the OpenShift web console and select Copy Login Command . Select Display Token and copy the oc login command from the web console and paste it into the terminal on your workstation. Run the command to login to the cluster on the command line Move to the next step to install the toolkit Red Hat OpenShift Red Hat OpenShift \u00b6 For OpenShift follow the install instructions for your preferred environment Install the OpenShift or OKD cluster: Todo Add any additional post install configuration steps needed here - storage class? Once installed you need to ensure you can sign onto the OpenShift cluster using the web console (see the Web console section in the docs for details on how to access the console). Once you are signed into the console you can download and install the OpenShift Command Line Interface (CLI) tools. The CLI tools are available from the question mark icon next to you login name at the top of the OpenShift console, or from Red Hat . On your local workstation or laptop open a command prompt and sign in to your OpenShift cluster. The exact command needed is available from the OpenShift web console, select your name at the top of the screen, then in the dropdown select the Copy login command link. This will open a screen where you can select Display Token (you may be asked to authenticate again before the token is displayed) then you can copy the command line command needed to login to the cluster. CodeReady Containers CodeReady Containers \u00b6 Navigate to the Red Hat CodeReady Containers site. Select Install OpenShift on your laptop button and follow the instructions to install CodeRead Containers to your laptop. Ideally you want to increase the resources available to CodeReady containers, so use the command line options to increase memory to as much as you can spare on your laptop or workstation and expand the disk size if possible. Warning CodeReady Containers needs to adjust your laptop networking, so on some platforms it will not work alongside VPN clients needed to access corporate networks. If you need to run a VPN client, then you can install CodeReady Containers in a virtual machine on your laptop and work inside the virtual machine to access CodeReady Containers. Download and install the OpenShift Command Line Interface (CLI) tools. The CLI tools are available from the question mark icon next to you login name at the top of the OpenShift console - see section 3.3.1 in the CodeReady Containers Getting Started Guide , or from Red Hat . Once you have the CodeReady Containers CLI installed you need to login to the cluster - this is covered in the CodeReady Containers Getting Started Guide - section 3.3.2 Move onto the next section to install the Cloud-Native Toolkit Todo What to do about github access - local server? Open Labs cluster Open Labs cluster \u00b6 Navigate to the IBM Open Labs - Red Hat OpenShift on IBM Cloud Select the Bring Your Own Application - Launch Lab button Sign-in to the IBM Cloud or sign up if you don't already have an IBM account When the Lab has been launched, forward the instructions in the left panel to show the 2nd page, Quick Links and Common Commands . Here you can see the command line commands to log into your IBM cloud account and also the OpenShift cluster. Log into the IBM cloud using the command prompt on the left side of the lab browser screen. Use the ibmcloud login command. Enter your email then password when prompted then select account DTECLOUD . Press enter to skip selecting a region and choose 'N' if prompted to update the ibmcloud utility. Log into the OpenShift cluster using the oc login command shown in the left pane. You can double click the command in the right side panel to copy it across to the left hand panel in the UI. Press enter to run it. Install the Cloud-Native Toolkit Command Line Interface (CLI). Copy and paste the code blocks below into the right panel of the Labs UI to install Node.js and then the toolkit CLI: Todo should this be in the learning section for developer setup? Create the installation directory. This command will prompt you for the student password. Above the terminal window there are a set of icons. One is a key icon. Press the key icon to see a set of credentials. The student password is in the SSH section of the Service Information panel, identified by pass sudo mkdir -p /usr/local/lib/nodejs install Node.js using commands: wget https://nodejs.org/dist/v14.16.1/node-v14.16.1-linux-x64.tar.xz sudo tar -xJvf node-v14.16.1-linux-x64.tar.xz -C /usr/local/lib/nodejs cat <<EOF >> .shellrc # Nodejs export PATH=/usr/local/lib/nodejs/node-v14.16.1-linux-x64/bin:$PATH EOF . ./.shellrc install the Toolkit CLI sudo PATH = /usr/local/lib/nodejs/node-v14.16.1-linux-x64/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin /usr/local/lib/nodejs/node-v14.16.1-linux-x64/bin/npm i -g @ibmgaragecloud/cloud-native-toolkit-cli The OpenLabs have an hour default duration. This can be extended to give you more time to complete a lab using the clock icon above the terminal panel. You should extend the lab session to allow you to complete the learning. Warning The remaining time for the lab session is shown above the terminal panel. This is a countdown clock. When it reaches 00:00:00 your lab session will be automatically terminated and cleaned up without any prompts or warnings, so be sure to extend any lab session as needed before it counts down to 00:00:00 You can access the OpenShift web console using the link in the Quick Links section in the left panel of the Labs user interface. Clicking the link will open the console in a new tab. Jump to the next section and use the command in the Linux / MacOS tab to install the Cloud-Native Toolkit - run the command on the command line in the left panel of the lab screen. You must have completed the oc login command, detailed in step 4 above, before starting the toolkit install Installing the toolkit \u00b6 To install the toolkit perform the following steps: In a command or terminal window ensure you are logged onto your cluster ( oc login or kubectl login ) with an admin account with the ability to create new namespaces/projects on the cluster and setup RBAC security. Run the following command (choose your operating system): Linux / MacOS curl -sfL get.cloudnativetoolkit.dev | sh - Windows oc create -f https :// raw . githubusercontent . com / cloud-native-toolkit / ibm-garage-iteration-zero / master / install / install-ibm -toolkit . yaml sleep 5 oc wait pod -l job-name = ibm-toolkit - -for = condition = Ready -n default oc logs job / ibm-toolkit -f -n default Wait for the terraform scripts to complete the installation of the toolkit - this can take several minutes","title":"Fast Start install"},{"location":"setup/fast-start/#fast-start-install","text":"This section will guide you through installing the Cloud-Native Toolkit suitable for learning how to use the toolkit. Fast-start installation does not install the base Kubernetes cluster. You need to provide the cluster before starting the toolkit installation.","title":"Fast-Start Install"},{"location":"setup/fast-start/#choosing-a-kubernetes-cluster-option","text":"The toolkit can be installed over the Kubernetes Service running on IBM Cloud or a Red Hat OpenShift cluster. However, the fast-start installer only works against a Red Hat OpenShift cluster : OpenShift on IBM Cloud","title":"Choosing a Kubernetes cluster option"},{"location":"setup/fast-start/#obtaining-your-kubernetes-cluster","text":"Select the option you want for your cluster, then follow the instructions. OpenShift on IBM Cloud","title":"Obtaining your Kubernetes Cluster"},{"location":"setup/fast-start/#installing-the-toolkit","text":"To install the toolkit perform the following steps: In a command or terminal window ensure you are logged onto your cluster ( oc login or kubectl login ) with an admin account with the ability to create new namespaces/projects on the cluster and setup RBAC security. Run the following command (choose your operating system): Linux / MacOS curl -sfL get.cloudnativetoolkit.dev | sh - Windows oc create -f https :// raw . githubusercontent . com / cloud-native-toolkit / ibm-garage-iteration-zero / master / install / install-ibm -toolkit . yaml sleep 5 oc wait pod -l job-name = ibm-toolkit - -for = condition = Ready -n default oc logs job / ibm-toolkit -f -n default Wait for the terraform scripts to complete the installation of the toolkit - this can take several minutes","title":"Installing the toolkit"},{"location":"setup/setup-options/","text":"Installing the Cloud-Native Toolkit \u00b6 Note If you are a developer attending a training event, where the training environment is provided, you should skip this section and move onto the Cloud-Native Toolkit fast start content If you want to host a workshop for a number of developers, then you may want to use the workshop . There is no single way to setup an enterprise development infrastructure for cloud native application development. There are many different tools available to help create Cloud Native applications. You need to find the set of tools and processes that allow the IT organization to work efficiently to create and manage production-ready applications and services. This can be a time consuming processes, to evaluate available tools and determine how to integrate them into a development environment. The Cloud-Native toolkit is an opinionated set of tools to create a production-ready development environment. The toolkit also provides installation options and starter kits to get you up and running quickly. If you don't have access to a Cloud-Native Toolkit installation then you can select one of the setup options detailed below to install the toolkit before moving onto the learning. Options for installing the toolkit \u00b6 The Cloud-Native toolkit will run on a standard Kubernetes cluster or on the Red Hat OpenShift hybrid cloud platform, which is based on Kubernetes. The Cloud-Native toolkit supports many different options for installing and configuring the toolkit across a number of different Kubernetes based environment, but for the first install it is recommended to follow the fast-start setup. The fast-start installation will create an environment for you to learn about cloud native development. Once you have a better understanding of cloud native development you can then make informed decisions about how you want to do Cloud-Native Development within your production environment and customize the toolkit to meet your requirements. You will find additional installation and toolkit configuration options in the Adopting the toolkit section, but the recommended path for your first installation is the fast-start installation option. Prerequisites for installing the Cloud-Native Toolkit \u00b6 For all options to complete the Fast-start training your need a laptop or workstation with a modern Operating System (MS Windows, Mac OS or Linux) with an up to date browser. For IBM Cloud, bare metal, virtualized infrastructure or 3rd party cloud installs you also need to have a terminal for interacting with the cluster using command line tools. If using the IBM Cloud there is a web based command line environment available so you don't need to install additional tooling on your workstation or laptop. If using the Learning environment, you only need a browser, as the learning environment provides a web based command line environment. IBM Cloud Shell \u00b6 If you plan to use an IBM Cloud based cluster to complete the learning, then you can opt to use the IBM Cloud web based shell environment. To access the shell simply navigate to https://cloud.ibm.com/shell . The web based shell contains all the tool needed (curl, node.js, ibmcloud CLI and oc CLI) to complete the installation. Install prerequisite tools on your workstation or laptop \u00b6 If you are not planning to use the IBM Cloud shell or are going to use the online Learning environment, then you need to have some prerequisite software installed on your laptop or workstation. If on Mac OS or Linux then you need to have curl installed. This is usually available from the Linux package manager or installed as part of the base operating system For all operating systems Node.js needs to be installed","title":"Options for installing up Toolkit"},{"location":"setup/setup-options/#installing-the-cloud-native-toolkit","text":"Note If you are a developer attending a training event, where the training environment is provided, you should skip this section and move onto the Cloud-Native Toolkit fast start content If you want to host a workshop for a number of developers, then you may want to use the workshop . There is no single way to setup an enterprise development infrastructure for cloud native application development. There are many different tools available to help create Cloud Native applications. You need to find the set of tools and processes that allow the IT organization to work efficiently to create and manage production-ready applications and services. This can be a time consuming processes, to evaluate available tools and determine how to integrate them into a development environment. The Cloud-Native toolkit is an opinionated set of tools to create a production-ready development environment. The toolkit also provides installation options and starter kits to get you up and running quickly. If you don't have access to a Cloud-Native Toolkit installation then you can select one of the setup options detailed below to install the toolkit before moving onto the learning.","title":"Installing the Cloud-Native Toolkit"},{"location":"setup/setup-options/#options-for-installing-the-toolkit","text":"The Cloud-Native toolkit will run on a standard Kubernetes cluster or on the Red Hat OpenShift hybrid cloud platform, which is based on Kubernetes. The Cloud-Native toolkit supports many different options for installing and configuring the toolkit across a number of different Kubernetes based environment, but for the first install it is recommended to follow the fast-start setup. The fast-start installation will create an environment for you to learn about cloud native development. Once you have a better understanding of cloud native development you can then make informed decisions about how you want to do Cloud-Native Development within your production environment and customize the toolkit to meet your requirements. You will find additional installation and toolkit configuration options in the Adopting the toolkit section, but the recommended path for your first installation is the fast-start installation option.","title":"Options for installing the toolkit"},{"location":"setup/setup-options/#prerequisites-for-installing-the-cloud-native-toolkit","text":"For all options to complete the Fast-start training your need a laptop or workstation with a modern Operating System (MS Windows, Mac OS or Linux) with an up to date browser. For IBM Cloud, bare metal, virtualized infrastructure or 3rd party cloud installs you also need to have a terminal for interacting with the cluster using command line tools. If using the IBM Cloud there is a web based command line environment available so you don't need to install additional tooling on your workstation or laptop. If using the Learning environment, you only need a browser, as the learning environment provides a web based command line environment.","title":"Prerequisites for installing the Cloud-Native Toolkit"},{"location":"setup/setup-options/#ibm-cloud-shell","text":"If you plan to use an IBM Cloud based cluster to complete the learning, then you can opt to use the IBM Cloud web based shell environment. To access the shell simply navigate to https://cloud.ibm.com/shell . The web based shell contains all the tool needed (curl, node.js, ibmcloud CLI and oc CLI) to complete the installation.","title":"IBM Cloud Shell"},{"location":"setup/setup-options/#install-prerequisite-tools-on-your-workstation-or-laptop","text":"If you are not planning to use the IBM Cloud shell or are going to use the online Learning environment, then you need to have some prerequisite software installed on your laptop or workstation. If on Mac OS or Linux then you need to have curl installed. This is usually available from the Linux package manager or installed as part of the base operating system For all operating systems Node.js needs to be installed","title":"Install prerequisite tools on your workstation or laptop"}]}